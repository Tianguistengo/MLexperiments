https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
%matplotlib inline

np.random.seed(2)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from keras.optimizers import RMSprop
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau


sns.set(style='white', context='notebook', palette='deep')


---------------------------------------------

-The computer does not see the image, it sees an arry of numbers

- [red,green,blue] that's the order, so [0,1,0] it's only green.

-low numbers==darker, high numbers==brighter

"Neural networks expect the labels of classes in a dataset to be organized in a ONE-HOT ENCODER manner: 
each row in the array contains zeros in all columns, except the column corresponding to a unique label, which is set to 1.

-
# Reshape test data
test_data = test_data.reshape(10, 784)

# Evaluate the model
model.evaluate(test_data, test_labels)

-the KERNEL defines the feature that we are looking for
kernel=np.array([-1,1])

After the convolution is done (with a kernel that tells it what to look for),
the resulting array is called a ----- "FEATURE MAP"-----,
because it contains a map of the locations in the image that match the features represented by the kernel.

-The convolution of an image with a kernel 
summarizes a part of the image as the sum of the multiplication of that part of the image with the kernel.

-The output of each unit in this layer is a convolution of a kernel over the previous layer 
(if it's the first layer then its over the image input).

-A normal dense layer has one weight for each pixel in the image,
but a convolutional layer, has only one weight for each pixel IN THE KERNEL.

- the FLATTEN layer, acts as a connector between the convolution and dense layer.
The convolution layer hands the feature map and the flatten layer gives it to the dense layer(which acts as an output layer
with the softmax activation function).

-Convolutional networks for classification are constructed from a sequence of convolutional layers (for image processing) 
and fully connected (Dense) layers (for readout). 

"
-# Initialize the model object
model = Sequential()

# Add a convolutional layer
model.add(Conv2D(10, kernel_size=3, activation='relu', 
               input_shape=(img_rows,img_cols,1)))   ----> the one is for the channel( 1 in this case = B/W)
               
 # Compile the model 
model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Fit the model on a training set
model.fit(train_data, train_labels, 
          validation_split=0.2, 
          epochs=3, batch_size=10)
          
 -muy bueno para entender una nueva network 
 -------------------------->  a description of the model ---->  model.summary()
          
   - "one of the challenges in fitting neural networks is the --large number of parameters--,
   one way to mitigate this, is ---to summarize the output---
   of convolutional layers in a concise manner == ---> POOLING <--- operations

max pooling == summarize a group of pixels based on its maximal value (the brightest feature)
-----MaxPool2D---- after a conv layer
Convolution => Max pooling => Convolution => Flatten => Dense


---- there are of course tradeoffs of reducing the #s of parameters---

"As we have seen before, CNNs can have a lot of parameters. 
Pooling layers are often added between the convolutional layers of a neural network
to summarize their outputs in a condensed manner, 
and reduce the number of parameters in the next layer in the network. This can help us 
if we want to train the network more rapidly, 
----or if we don't have enough data----- to learn a very large number of parameters.

The resulting image is smaller, but ---retains the salient features in every location-----.

A pooling layer can be described as a particular kind of convolution. 
For every window in the input it finds the maximal pixel value and passes only this pixel through.

# Result placeholder
result = np.zeros((im.shape[0]//2, im.shape[1]//2))

# Pooling operation
for ii in range(result.shape[0]):
    for jj in range(result.shape[1]):
        result[ii, jj] = np.max(im[ii*2:ii*2+2, jj*2:jj*2+2])
        
        model.add(MaxPool2D(2))     -->pooling over windows of 2x2



---------full example of conv net with MaxPool2D------------

# Result placeholder
result = np.zeros((im.shape[0]//2, im.shape[1]//2))

# Pooling operation
for ii in range(result.shape[0]):
    for jj in range(result.shape[1]):
        result[ii, jj] = np.max(im[ii*2:ii*2+2, jj*2:jj*2+2])


# Add a convolutional layer
model.add(Conv2D(15, kernel_size=2, activation='relu', 
                 input_shape=(img_rows, img_cols, 1)))

# Add a pooling operation
model.add(MaxPool2D(2))

# Add another convolutional layer
model.add(Conv2D(5, kernel_size=2, activation='relu'))

# Flatten and feed to output layer
model.add(Flatten())
model.add(Dense(3, activation='softmax'))
model.summary()

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Fit to training data
model.fit(train_data,train_labels,validation_split=0.2,epochs=3,batch_size=10)

# Evaluate on test data 
model.evaluate(test_data,test_labels,batch_size=10)


---------------

How do we use the best parameters BEFORE the network starts overfitting?

from keras.callbacks import ModelCheckpoint
(stores the weights of a network at the end of each epoch of learning)
{will only overwrite the weights if the validation loss decreases}
[if the network overfits, the weights will be stored for the epoch at which validaton loss was the smallest)

the checkpoint object is stored in a list
and passes as input when fitting the model

to use the best weights learned, we need to initialize the model again
# Load the weights from file
model.load_weights('weights.hdf5')

# Predict from the first three images in the test data
model.predict(test_data[:3])




-----Plot the learning curves

import matplotlib.pyplot as plt

# Train the model and store the training object
training = model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10)

# Extract the history from the training object
history = training.history

# Plot the training loss 
plt.plot(history['loss'])
# Plot the validation loss
plt.plot(history['val_loss'])

# Show the figure
plt.show()


------> PADDING
Padding does something pretty clever to solve this: 
pad the edges with extra, “fake” pixels (usually of value 0, hence the oft-used term “zero padding”).
This way, the kernel when sliding can allow the original edge pixels to be at its center, 
while extending into the fake pixels beyond the edge, producing an output the same size as the input.

---> STRIDES
Often when running a convolution layer, you want an output with a lower size than the input.
This is commonplace in convolutional neural networks, where the size of the spatial dimensions 
are reduced when increasing the number of channels. 
One way of accomplishing this is by using a pooling layer (eg. taking the 
average/max of every 2×2 grid to reduce each spatial dimensions in half).
Yet another way to do is is to use a stride.

More modern networks, such as the ResNet architectures entirely forgo pooling layers in their internal layers, 
in favor of strided convolutions when --NEEDING TO REDUCE THEIR OUTPUT SIZES---.

-- A FILTER is a collection of kernels.
So this is where a key distinction between terms comes in handy: 
whereas in the 1 channel case, where the term filter and kernel are interchangeable, 
in the general case (3 channels), they’re actually pretty different. 
Each filter actually happens to be a collection of kernels,
--with there being one kernel for every single input channel to the layer--, and each kernel being unique.

Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. 

Each of the per-channel processed versions are then summed together to form one channel. 
The kernels of a filter each produce one version of each channel,
and the filter as a whole produces one overall output channel.

-A essential design choice of any CNN architecture is that the input sizes grow smaller and smaller from
the start to the end of the network, while the number of channels grow deeper. 
This, as mentioned earlier, is often done through strides or pooling layers. 
Locality determines what inputs from the previous layer the outputs get to see. 
The RECEPTIVE FIELD determines what area of the original input to the entire network the output gets to see.

The idea of a strided convolution is that we only process slides a fixed distance apart,
and skip the ones in the middle. From a different point of view,
we only keep outputs a fixed distance apart, and remove the rest.



-REGULARIZATION  (2 techniques)


1) DROPOUT
"
In each step of learning, we choose a random subset of the units (in one or many layers), and ignore it.
(ignore it both in the forward pass and backprograpation step of the network)

It works great because it allows us to train many different networks on different parts of the data.
(and if one part of the network becomes very sensitive to data, others will compensate for it)
(also prevents different units in the network to become overly correlated)
(if one unit learns to prefer horizontal lines, another will prefer verticals)
(the kernels will be more different from each other)

In Keras, dropout is implemented as a layer
and add it after the layer for which we want the units ignored
and specify the proportions of the units we want ignored in each learning step.

------->   model.add(Dropout(0.25))



2) Batch Normalization

Rescale the outputs
(so that it always has 0 mean and standard deviation of 1 in ever batch of training)

Added after each layer whose outputs we want normalized:

-----> model.add(BatchNormalization())


 (sometimes dropout and batch normalization do not work well TOGETHER!
 "THE DISHARMONY OF BATCH NORMALIZATION AND DROPOUT")







