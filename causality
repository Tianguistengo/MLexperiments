https://www.wired.com/story/ai-smart-cant-grasp-cause-effect/

If a robot coud reason causally, it might be able to avoid problems it hasn't been programmed to understand

The test devised by Tenenbaum is important, 
because it provides a good way to >>measure causal understanding,
albeit in a very limited setting.

The approach requires more hand-built components than many machine learning algorithms, 
and Tenenbaum cautions that it’s brittle and won’t scale well.
But it seems to suggest that a 
mix of approaches
—along with some new ideas—
will be needed to take AI forward.

-------------

https://www.wired.com/story/ai-pioneer-algorithms-understand-why/

Understanding cause and effect would make existing AI systems smarter and more efficient.
A robot that understands that dropping things causes them to break 
would not need to toss dozens of vases onto the floor 
to see what happens to them.

Bengio says the analogy extends to self driving cars. 
“Humans don't need to live through many examples of accidents to drive prudently,” he says. 
>>They can just imagine accidents, 
>>“in order to prepare mentally if it did actually happen.”

>>>>>>>>>>The question is how to give AI systems this ability.

At his research lab, Bengio is working on a version of deep learning 
capable of recognizing simple cause-and-effect relationships. 
He and colleagues recently posted a research paper outlining the approach. 
They used a dataset that maps causal relationships between real-world phenomena, 
such as smoking and lung cancer, 
>>in terms of probabilities. 
They also generated synthetic datasets of causal relationships.

The algorithm in the paper essentially forms a hypothesis 
about which variables are causally related, 
and then tests how changes to different variables fit the theory. 

The fact that smoking is not only related to cancer but actually causes it, for instance, 
should still be apparent even if cancer is correlated with other factors, such as hospital visits.

A robot might eventually use this approach to form a hypothesis 
about what happens when it drops something, 
and then confirm its hunch when it sees several things smash to the floor.

------------

A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms

https://arxiv.org/pdf/1901.10912.pdf

Abstract

We  propose  to  meta-learn  causal  structures  
>>based  on  how  fast  
>>a  learner  adapts to  new  distributions 
arising from sparse distributional changes,
e.g.  due to interventions, actions of agents and other sources of non-stationarities. 

We show that under this assumption, 
the correct causal structural choices lead to 
>>faster adaptation to modified distributions 
because the changes are concentrated in one or just a few mechanisms
when the learned knowledge is modularized appropriately.  

This leads to sparse expected gradients
and a lower effective number of degrees of freedom 
needing to be relearned while adapting to the change. 

It motivates using the speed of adaptation to a modified distribution as a meta-learning objective.

We demonstrate how this can be used to determine the cause-effect relationship between two observed variables.
The distributional changes do not need to correspond to standard interventions (clamping a variable), 
and the learner has no direct knowledge of these interventions. 

We show that causal structures can be parameterized via continuous variables and learned end-to-end. 

We then explore how these ideas could be used to also learn an encoder that would 
map low-level observed variables to unobserved causal variables l
eading to faster adaptation out-of-distribution, 
learning a representation space where one can satisfy the assumptions of independent mechanisms
and of small and sparse changes in these mechanismsdue to actions and non-stationarities.















