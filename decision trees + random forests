# to make this notebook's output stable across runs
np.random.seed(42)

CART cost function for classificaiton
It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot
find a split that will reduce impurity.

additional stopping conditions:
-min_samples_split         #the minimum number of samples a node must have before it can be split
-min_samples_leaf          #the minimum number of samples a leaf node must have
-min_weight_fraction_leaf  #same as min_samples_leaf but expressed as a fraction of the total number of weighted instances
-max_leaf_nodes            #maximum number of leaf nodes
-max_features              #maximum number of featuers that are evaluated for splitting at each node


-By default, the Gini impurity measure is used, but you can select the entropy impurity
measure instead by setting the criterion hyperparameter to "entropy".


-from sklearn.tree import export_graphviz

export_graphviz(
        tree_clf,
        out_file=image_path("iris_tree.dot"),
        feature_names=iris.feature_names[2:],
        class_names=iris.target_names,
        rounded=True,
        filled=True
    )
    
  
 -For regression tasks:  
 from sklearn.tree import DecisionTreeRegressor
 
 
 -Grid search with cross-validation (with the help of the GridSearchCV class) 
 to find good hyperparameter values for a DecisionTreeClassifier
 
 from sklearn.model_selection import GridSearchCV
 
 
 By default, GridSearchCV trains the best model found on the whole training set (you can change this by setting refit=False), 
 so we don't need to do it again. We can simply evaluate the model's accuracy:
 
 from sklearn.metrics import accuracy_score

y_pred = grid_search_cv.predict(X_test)
accuracy_score(y_test, y_pred)
 
 
 
 --random forests: "For example, you can train a group of Decision Tree classifiers, each on a different
random subset of the training set. To make predictions, you just obtain the predic‐
tions of all individual trees, then predict the class that gets the most votes (see the last
exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest,
and despite its simplicity, this is one of the most powerful Machine Learning algo‐
rithms available today." A.G.
 
 -"As we have discussed, a Random Forest 9 is an ensemble of Decision Trees, generally
trained via the bagging method (or sometimes pasting), typically with max_samples
set to the size of the training set" A.G.
 
 
