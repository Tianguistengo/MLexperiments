# to make this notebook's output stable across runs
np.random.seed(42)

CART cost function for classificaiton
It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot
find a split that will reduce impurity.

additional stopping conditions:
-min_samples_split         #the minimum number of samples a node must have before it can be split
-min_samples_leaf          #the minimum number of samples a leaf node must have
-min_weight_fraction_leaf  #same as min_samples_leaf but expressed as a fraction of the total number of weighted instances
-max_leaf_nodes            #maximum number of leaf nodes
-max_features              #maximum number of featuers that are evaluated for splitting at each node


-By default, the Gini impurity measure is used, but you can select the entropy impurity
measure instead by setting the criterion hyperparameter to "entropy".


-from sklearn.tree import export_graphviz

export_graphviz(
        tree_clf,
        out_file=image_path("iris_tree.dot"),
        feature_names=iris.feature_names[2:],
        class_names=iris.target_names,
        rounded=True,
        filled=True
    )
    
  
 -For regression tasks:  
 from sklearn.tree import DecisionTreeRegressor
 
 
 -Grid search with cross-validation (with the help of the GridSearchCV class) 
 to find good hyperparameter values for a DecisionTreeClassifier
 
 from sklearn.model_selection import GridSearchCV
 
 
 By default, GridSearchCV trains the best model found on the whole training set (you can change this by setting refit=False), 
 so we don't need to do it again. We can simply evaluate the model's accuracy:
 
 from sklearn.metrics import accuracy_score

y_pred = grid_search_cv.predict(X_test)
accuracy_score(y_test, y_pred)
 
 
 
 --random forests: "For example, you can train a group of Decision Tree classifiers, each on a different
random subset of the training set. To make predictions, you just obtain the predic‐
tions of all individual trees, then predict the class that gets the most votes (see the last
exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest,
and despite its simplicity, ***this is one of the most powerful Machine Learning algo‐
rithms available today***." A.G.
 
 -"As we have discussed, a Random Forest 9 is an ensemble of Decision Trees, generally
trained via the bagging method (or sometimes pasting), typically with max_samples
set to the size of the training set" A.G.
 
 
 
 ---How to measure how long it took the model to trian 
 
 
from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)


import time

t0 = time.time()
rnd_clf.fit(X_train, y_train)
t1 = time.time()

print("Training took {:.2f}s".format(t1 - t0))

Training took 3.98s

-----Parameters:	

n_estimators : integer, optional (default=10)

    The number of trees in the forest.

    Changed in version 0.20: The default value of n_estimators will change from 10 in version 0.20 to 100 in version 0.22.
    
    
   --the drawback to using a single decision/regression tree – it fails to include
   predictive power from multiple, overlapping regions of the feature space.  "
   
   -- random forests vs gradient descent machines
   
   
    RF uses decision trees, which are very prone to overfitting.
    In order to achieve higher accuracy, RF decides to create a large number of them based on bagging. 
    The basic idea is to resample the data over and over and for each sample train a new classifier. 
    Different classifiers overfit the data in a different way, and through voting those differences are averaged out.
    
    GBM is a boosting method, which builds on weak classifiers. 
    The idea is to add a classifier at a time, so that the next classifier is trained to improve the already trained ensemble.
    
    -->Notice that for RF each iteration the classifier is trained independently from the rest.
    
    
    --Isolation forests for anomaly detection / fraud

--------------------------------------------------------

-Classification Trees

"Sequence of IF/ELSE questions about individual features

The objective: infer class labels

In contrast to linear models, classification trees are able to capture non-linear relationships 
between features and labels

They don-t require feature scaling

Easy to interpret its results

You don't need a lot of feature preprocessing
No need to standardize or normalize features

LIMITATIONS of CARTs:
For classification they only produce orthogonal decision boundaries
Sensitive to small variations in the data set
High variance: unconstrained CARTs can overfit the data

^solutin: ensemble methods


-A node is a question or a prediction

-Root: first node
Internal node: has a parent and kids
Leaf: has no kids (e.g. a prediction)


-The decision boundary produced by logistic regression is LINEAR 
while the boundaries produced by the classification tree divide the feature space into RECTANGULAR regions.


-Criteria to measure the impurity of a node: Gini coefficient, entropy.

Most of the time, the gini index and entropy lead to the same results. The GINI INDEX is slightly faster to compute
and is the default criterion used in the DecisionTreeClassifier model of scikit-learn.

-IG=Information Gain (what every split wishes to maximize) 
/ splitting an internal node always involves maximizing information gain!

If the information gain obtained by splitting a node is NULL, the node is declared a leaf.
(except if you have a max_depth=2, then all nodes having a depth of 2 will be declared leaves,
even if their IG is not null)

-An unconstrained tree has no max depth

- min_samples_leaf is the minimum number of samples required to be at a leaf node. 
This parameter is similar to min_samples_splits, 
however, this describe the minimum number of samples at the leafs, the base of the tree.

High Variance- overfitting

High Bias- underfitting

-To reduce the complexity of a model (meaning you have high variance problems)
try reducing the maximum tree depth
or increasing the maximum samples per leaf.
 (or gather more data so that you model doesn't overfit the old one)









































