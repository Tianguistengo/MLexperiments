
Deep learning is able to account for the INTERACTIONS between features
(the more nodes in the hidden layers, the more interactions we are able to capture).

An activation function (which converts a node's input into its output)
allows the model to capture non-linearities (tanh, ReLu, etc..)

ReLu
"This function takes a single number as an input, returning 0 if the input is negative, 
and the input if the input is positive.

Here are some examples:
relu(3) = 3
relu(-3) = 0

-Deep nets make internal representations of the patterns in the data
that are useful to make predictions

They partially remove the need for feature engineering and manually creative better predictive features.

Deep learning is also called representation learning.

As data moves through hidden layers, increasingly complex patterns and representations are learned.

-The loss function aggregates all of the errors into a single score.

- a SLOPE (as in the tangent line next to a weight in a chart that we use to know where the next step
of the gradient descent should go to) == a DERIVATIVE (calculus)


-Gradient Descent

Gradient == an array of slopes

After "calculating the slopes, it's time to use those slopes to improve a model.
If you add the slopes to the weights, you will move in the right direction. 

We use prediction erros to calculate slopes



-Backpropagation

Tries to deduce the slopes to update weights efficiently (going from the OUTput to the INput)
{estimate the slope OF THE LOSS FUNCTION with respect to EACH WEIGHT in the network)


Each time you generate predictions using forward propagation, 
you update the weights using backward propagation.

3 things we multiply to get the slopes associated with any weight:
   -node value feeding into that weight
   -slope of the activation function for the node being fed into (1 for any node receiving a positive value as input)
   -slope of the loss function with respect to the output node


-RECAP (forward and backwarward propagation + gradient descent
     -Start with random weights
     -Do forward propagation to get a prediction
     -Use backpropagation to estimate the slope OF THE LOSS FUNCTION with respect to EACH WEIGHT in the network
     -Multiply that slope by a learning rate, and subtract from the current weights
     -Keep doing that until you reach the flat part 


-For computational efficiency, calculate slopes on a subset of the data (or a batch, meaning, STOCHASTIC GRADIENT DESCENT)
{gradient descent uses the whole training data for calculating each slope}
for each update of the weights
and use a DIFFERENT BATCH  to calculate the next udpate.
Once all data is used, start from the beginning again.
Each time you go through the entire data is called an 'EPOCH'


- Keras
           4 steps
                      -SPECIFY THE ARCHITECTURE
               
               We need to load the data first because we need to know how many columns there are 
               and we need this information because the number of colums is the number of nodes in the input layer
               # Save the number of columns in n_cols:
               
                       n_cols = data_matrix.shape[1]
                       
                      -COMPILE
                      
               --->Specify the optimizer
               
               ADAM is an excellent choice as yout go-to optimizer
               (ADJUSTS THE LEARNING RATE as it does gradient descent
               to ensure reasonable values throughout the weight optimization process)
               
               --->Specify the loss function
               
               mean squared error is common for regression
               
                             
                      -FIT
                      
               Apply backpropagation and gradient descent to your data to update the weights
               
               Scaling data before fitting can help the optimization process
               
               model.fit -> Remember that the first argument is the predictive features (predictors),
               and the data to be predicted (target) is the second argument.
                      
                      -PREDICT



-a DENSE layer means all the nodes in one layer connect to all the nodes in the next layer.
other layers are Dropout, Flatten, Conv2D, MaxPool2D

-a SEQUENTIAL model means a layer has only connections or weights to the layer coming directly after it
(other models have more exotic structures).

 
 - from keras.models import load_model     --> allows me to reopen/use models I created in the past
 
 ------- CLASSIFICATION
 
 -Loss function is (most commonly) 'categorical_crossentropy' instead of MSE
 (in math is LogLoss==lower score is better).
 
 -Add         metrics= ['accuracy']      to the compile step
  for easier to understand diagnostics
  
 -Ouput layer has 2 nodes instead of 1 (one for each possible outcome),
 and uses a 'softmax' activation function.
 
 from keras.utils import    to_categorical    to get 0 and 1 



You want to SAVE,  RELOAD, and PREDICT   with your model

from keras.model import load_model

model.save('model_file.h5')  h5 is short for the model format normally used 

my_model=load_model('my_model.h5')

# Calculate predictions: predictions
predictions = model.predict(data_to_predict_with)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]


-a common learning reate is 0.01

- the DYING NEURON PROBLEM
once a node starts getting negative inputs, it may continue getting only negative inputs.
It's contributing nothing to the network and the neuron is regarded as dead


-"  Vanishing Gradient Problem


The tendency for the gradients of early hidden layers of some deep neural networks to become surprisingly flat (low).
Increasingly lower gradients result in increasingly smaller changes to the weights on nodes in a deep neural network, 
leading to little or no learning. 
Models suffering from the vanishing gradient problem become difficult or impossible to train.
Long Short-Term Memory cells address this issue. (also changing your activation function)

Compare to exploding gradient problem.


-----> Model validation

neural nets are usually so large that we don't really need to do k-fold cross validation,
a single run over the database would be large enough

-Early Stopping 
(patience=  , is how many epochs can the model go without improving before we stop training)
(3 is resonable, 1 no because sometimes it keeps improving after 1 bad one)
if you have a logic for when to stop (early stopping) you can set a higher # of maximum epochs to see if it learns better
(epochs=20, in the fit step) by default, keras trains for 10 epochs

from keras.callbacks import EarlyStopping

# Compile the model
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Fit the model
model.fit(predictors, target, epochs=30, validation_split=0.3,callbacks=[early_stopping_monitor])


-validaton split (in the fit step) + define metrics= (in the compile step, 'accuracy' for classification)

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
model.fit(predictors,target,validation_split=0.3)

-now that we have cool ways of measuring performance (namely VALIDATION LOSS after doing ^)
we can experiment with 
                            -different architectures
                            -more or fewer layers
                            -layers with more or fewer nodes
                            -etc.



---> Model capacity

similar to overfitting and underfitting,
model capacity is a model's ability to capture predictive patterns in your data

more model capacity (more hidden layers, mode nodes) == more overfitting

-Start with a simple network and get the validation score,
keep adding capacity as long as the score keeps improving.


-------------advanced-----------------

-Layers are used to construct deep learning models, 
TENSORS,  are used to define the --data flow-- through the model

- if the input data has 10 columns, you define an Input layer with a shape of (10,).
 The input layer allows your model to load data.

# Load layers
from keras.layers import Input, Dense

# Input layer
input_tensor = Input(shape=(1,))

# Dense layer
output_layer = Dense(1)

# Connect the dense layer to the input_tensor
output_tensor = output_layer(input_tensor)

"Output layers are simply Dense layers! 
Output layers are used to reduce the dimension of the inputs to the dimension of the outputs. 



-In the compile step, we choose an optimize and a loss function,
for keras model, MEAN ABSOLUTE ERROR, is a good general-purpose error function 
which is a little bit less senstive to outliers, we can also use MSE (used in traditional linear regression).

-Before fitting,            model.summary()           to get a good overview.
It is also useful to plot the model before fitting it


-Once you've defined an input layer and an output layer, you can build a Keras model. 
The model object is how you tell Keras where the model starts and stops: where data comes in and where predictions come out.

# Input/dense/output layers
from keras.layers import Input, Dense
input_tensor = Input(shape=(1,))
output_tensor = Dense(1)(input_tensor)

# Build the model
from keras.models import Model
model = Model(input_tensor, output_tensor)


"The final step in creating a model is compiling it.
Now that you've created a model, you have to compile it before you can fit it to data. 
This finalizes your model, freezes all its settings, and prepares it to meet some data!

Loss function depends on the problem at hand.
Mean squared error is a common loss function and will optimize for predicting the mean, as is done in least squares regression.
Mean absolute error optimizes for the median and is used in quantile regression. 

# Compile the model
model.compile(optimizer='adam', loss='mean_absolute_error')

Now that you've compiled the model, take a look a the result of your hard work! 
You can do this by looking at the model summary, as well as its plot.

# Import the plotting function
from keras.utils import plot_model
import matplotlib.pyplot as plt

# Summarize the model
model.summary()

# Plot the model
plot_model(model, to_file='model.png')

# Display the image
data = plt.imread('model.png')
plt.imshow(data)
plt.show()

# Now fit the model
model.fit(games_tourney_train['seed_diff'], 
          games_tourney_train['score_diff'],
          epochs=1,
          batch_size=128,
          validation_split=0.1,
          verbose=True)

# Load the X variable from the test data
X_test = games_tourney_test['seed_diff']

# Load the y variable from the test data
y_test = games_tourney_test['score_diff']

# Evaluate the model on the test data
print(model.evaluate(X_test, y_test, verbose=False))

<script.py> output:
    10.06973339669147        --> good 


-----------

-CATEGORICAL EMBEDDINGS are an advance type of layer available only in deep learning libraries.
Useful for dealing with --high cardinality categorical data--, text data.

To map the interger teams IDs to a decimal rating, we will use an embedding layer.

Embeddings add another dimension to your data (apart from rows and columns) so we need to flatten it
to return to a 2D       flatten_tensor = Flatten() (embed_tensor)

"The embedding layer is a lot like a dictionary, but your model learns the values for each key.

from G glossary "An embedding is a categorical feature represented as a continuous-valued feature.
Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. 

Embedding layers maps integers to floats.



# Imports
from keras.layers import Embedding
from numpy import unique

# Count the unique number of teams
n_teams = unique(games_season['team_1']).shape[0]

# Create an embedding layer
team_lookup = Embedding(input_dim=n_teams,
                        output_dim=1,         cuz we just want 1 number to come out
                        input_length=1,       cuz it's just 1 number coming in
                        name='Team-Strength')
                        
# Imports
from keras.layers import Input, Embedding, Flatten
from keras.models import Model

# Create an input layer for the team ID
teamid_in = Input(shape=(1,))

# Lookup the input in the team strength embedding layer
strength_lookup = team_lookup(teamid_in)         ==        embedding(input layer)

# Flatten the output
strength_lookup_flat = Flatten()(strength_lookup)    ==    Flatten () (embedding layer)

# Combine the operations into a single, re-usable model
team_strength_model = Model(teamid_in, strength_lookup_flat, name='Team-Strength-Model')


" The team strength lookup has three components: an input, an embedding layer, and a flatten layer that creates the output.

If you wrap these three layers in a model with an input and output, 
you can re-use that stack of three layers at multiple places.

Note again that the weights for all three layers will be shared everywhere we use them.




----->  SHARED LAYERS

They allow you to define an operation, and then apply the exact same operation (with the exact same weights)
to different inputs.

You can also share models, not just layers.
When you have a model, you can re-use that model to share that sequence of steps
for different input layers.

# Load the input layer from keras.layers
from keras.layers import Input

# Input layer for team 1
team_in_1 = Input(shape=(1,),name='Team-1-In')

# Separate input layer for team 2
team_in_2 = Input(shape=(1,),name='Team-2-In')

"
Since we already have a team_strength_model and an input layer for each team, 
we can lookup the team inputs in the shared team strength model. The two inputs will share the same weights.

# Lookup team 1 in the team strength model
team_1_strength = team_strength_model(team_in_1)

# Lookup team 2 in the team strength model
team_2_strength = team_strength_model(team_in_2)

Now your model knows how strong each team is.




-----> MERGE LAYERS:

Merge layers allow you to defined advance, non-sequential   network topologies

- Add, Substract, and Multiply layers  need to have the same shape

-Concatenate layers, do not.

-Concatenate layers are good for combining multiple inputs into a single layer 
to predict a single output.


"Now that you've looked up how "strong" each team is, 
subtract the team strengths to determine which team is expected to win the game.

The subtract layer will combine the weights from the two layers by subtracting them.

# Import the Subtract layer from keras
from keras.layers import Subtract

# Create a subtract layer using the inputs from the previous exercise
score_diff = Subtract()([team_1_strength, team_2_strength])

"Now that you have your two inputs (team id 1 and team id 2) and output (score difference), 
you can wrap them up in a model so you can use it later for fitting to data and evaluating on new data.

from keras.models import Model

# Subtraction layer from previous exercise
score_diff = Subtract()([team_1_strength, team_2_strength])

# Create the model
model = Model([team_in_1, team_in_2], score_diff)

# Compile the model
model.compile(optimizer='adam', loss='mean_absolute_error')

Now your model is finalized and ready to fit to data.

Next step is fit, evaluate, and predict.

Fit the model to the regular season training data.

Now that you've defined a complete team strength model,
you can fit it to the basketball data! 
Since your model has two inputs now, you need to pass the input data as a list.

# Get the team_1 column from the regular season data
input_1 = games_season['team_1']

# Get the team_2 column from the regular season data
input_2 = games_season['team_2']

# Fit the model to input 1 and 2, using score diff as a target
model.fit([input_1,input_2],
          games_season['score_diff'],
          epochs=1,
          batch_size=2048,
          validation_split=0.1,
          verbose=True)

Now our model has learned a strength rating for every team.

Evaluate the model on this new dataset 'games_tourney'

# Get team_1 from the tournament data
input_1 = games_tourney['team_1']

# Get team_2 from the tournament data
input_2 = games_tourney['team_2']

# Evaluate the model using these inputs
print(model.evaluate([input_1, input_2], games_tourney['score_diff'], verbose=False))



------>  3 inputs



# Create an Input for each team
team_in_1 = Input(shape=(1,), name='Team-1-In')
team_in_2 = Input(shape=(1,), name='Team-2-In')

# Create an input for home vs away
home_in = Input(shape=(1,), name='Home-In')

# Lookup the team inputs in the team strength model
team_1_strength = team_strength_model(team_in_1)
team_2_strength = team_strength_model(team_in_2)

# Combine the team strengths with the home input using a Concatenate layer, then add a Dense layer
out = Concatenate()([team_1_strength, team_2_strength, home_in])
out = Dense(1)(out)

# Import the model class
from keras.models import Model

# Make a Model
model = Model([team_in_1, team_in_2, home_in], out)

# Compile the model
model.compile(optimizer='adam', loss='mean_absolute_error')

# Fit the model to the games_season dataset
model.fit([games_season['team_1'], games_season['team_2'], games_season['home']],
          games_season['score_diff'],
          epochs=1,
          verbose=True,
          validation_split=.10,
          batch_size=2048)

# Evaluate the model on the games_tourney dataset
print(model.evaluate([games_tourney['team_1'], games_tourney['team_2'], games_tourney['home']],
               games_tourney['score_diff'], verbose=False))



----->Summarizing and plotting models

Keras models can have non-trainable parameters that are fixed and do not change,
and trainable parameters that are learned from the data when the model is fit.

Models with more trainable parameters and more flexible (and prone to overfitting).

A model-s trainable parametres are usually in its Dense layers.

Embedding layers often add a large number of trainable parameters to a model.
(because each unique value of the embedding input gets a parameter for its output)

model.summary()



plotting: 

# Imports
import matplotlib.pyplot as plt
from keras.utils import plot_model

# Plot the model
plot_model(model, to_file='model.png')

# Display the image
data = plt.imread('model.png')
plt.imshow(data)
plt.show()




--------> Stacking models

Using the predictions of one model as an input to another model.

Used in Kaggle competitoins to yield some of the most accurate results.

"When stacking, it's important to use different datasets for each model.

If you input dataset is --purely numeric--, you can put multiple inputs in a single input layer.



You'll use the prediction from the regular season model as an input to the tournament model. 
This is a form of "model stacking."

To start, take the regular season model from the previous lesson, and predict on the tournament data.
Add this prediction to the tournament data as a new column.

# Predict
games_tourney['pred'] = model.predict([games_tourney['team_1'],
                                             games_tourney['team_2'],
                                             games_tourney['home']])

With the new column 'pred' you now have --three numeric columns-- in the tournament dataset: 
'seed_diff', 'home', and 'pred'.
Now, you will create a neural network that uses a single input layer to process all three of these numeric inputs.

# Create an input layer with 3 columns
input_tensor = Input((3,))

# Pass it to a Dense layer with 1 unit
output_tensor = Dense(1)(input_tensor)

# Create a model
model = Model(input_tensor, output_tensor)

# Compile the model
model.compile(optimizer='adam', loss='mean_absolute_error')

Now your model is ready to meet some data!

Now that you've enriched the tournament dataset and built a model to make use of the new data,
fit that model to the tournament data.

Note that this model has only one input layer that is capable of handling all 3 inputs, 
so it's inputs and outputs do not need to be a list.

# Fit the model
model.fit(games_tourney_train[['home','seed_diff', 'pred',]],
          games_tourney_train['score_diff'],
          epochs=1,
          verbose=True)

# Evaluate the model on the games_tourney_test dataset
print(model.evaluate(games_tourney_test[['home', 'seed_diff', 'prediction']],
               games_tourney_test['score_diff'], verbose=False))

<script.py> output:
    9.07315489498804   -----> good




--------------> Two-output models

Make predictions for 2 targets at once

The only difference between a 2 output model and a 1 output model is the size of the output layer

To fit a model with 2 outputs, you use a data set with 2 columns for the y variable
y = data_set [ ['column 1', 'column2'] ]
model.fit (x,y,epochs = 500)

now that the model is fit, we can take a look at what it learned:
model.get_weights()
output would be   array [ weights, weights]
                  array [bias, bias]


# Define the input
input_tensor = Input((2,))

# Define the output
output_tensor = Dense (2)(input_tensor)  ----> we can see the 2 output here

# Create a model
model = Model(input_tensor,output_tensor)

# Compile the model
model.compile(optimizer='adam',loss='mean_absolute_error')

# Fit the model
model.fit(games_tourney_train[['seed_diff', 'pred']],
  		  games_tourney_train[['score_1', 'score_2']],
  		  verbose=True,
  		  epochs= 100,
  		  batch_size=16384)
        
# Print the model's weights
print(model.get_weights())

# Print the column means of the training data
print(games_tourney_train.mean())

The input layer will have 4 weights: 2 for each input times 2 for each output.
The output layer will have 2 weights, one for each output.

Notice that both output weights are about ~72.
This is because, on average, a team will score about 72 points in the tournament.

<script.py> output:
    [array([[ 0.13067713, -0.10371894],
           [ 0.38644195, -0.35632333]], dtype=float32), 
     array([72.38115, 72.38473], dtype=float32)]
     
    season        1.998074e+03
    team_1        5.556771e+03
    team_2        5.556771e+03
    home          0.000000e+00
    seed_diff     0.000000e+00
    score_diff    0.000000e+00
    score_1       7.162128e+01
    score_2       7.162128e+01
    won           5.000000e-01
    pred         -1.625470e-14
    dtype: float64


Now that we've fit your model and inspected it's weights to make sure it makes sense,
let's evaluate it on the tournament test set to see how well it performs on new data.

# Evaluate the model on the tournament test data
print(model.evaluate(
    games_tourney_test[['seed_diff','pred']],
games_tourney_test[['score_1','score_2']], verbose=False))



---------> Single model for classification and regression



This is another example of a model with 2 outputs

Input layer --> Regression Output /Dense  -->  Classification Output (Sigmoid)  /Dense

For the classification output layer, I use the regression model prediction as input.

With 2 output models, each output needs it's own loss function.

The purpose of having 2 outputs is not just to have 2 answers, 
but for answer #2 to using the finding of answer #1 as input. 
(usually the first one is the easier which then helps the 2nd one with a more complex task)


# Create an input layer with 2 columns
input_tensor = Input((2,))

# Create the first output
output_tensor_1 = Dense(1, activation='linear', use_bias=False)(input_tensor)

# Create the second output (use the first output as input here)
output_tensor_2 = Dense(1, activation='sigmoid', use_bias=False)(output_tensor_1)

# Create a model with 2 outputs
model = Model(input_tensor, [output_tensor_1, output_tensor_2])


# Import the Adam optimizer
from keras.optimizers import Adam

# Compile the model with 2 losses and the Adam optimzer with a higher learning rate
(notice how ther are 2 loss functions and Adam is initialized without '' and with a customized learning rate

model.compile(loss=['mean_absolute_error', 'binary_crossentropy'], optimizer=Adam(0.01))

# Fit the model to the tournament training data, with 2 inputs and 2 outputs
model.fit(games_tourney_train[['seed_diff', 'pred']],
          [games_tourney_train[['score_diff']], games_tourney_train[['won']]],
          epochs=10,
          verbose=True,
          batch_size=16384)

We fitted a model that is both a classifier and a regressor!


Inspect the model (II)

"Now we should take a look at the weights for this model. 
In particular, note the last weight of the model. 
This weight converts the predicted score difference to a predicted win probability.
If you multiply the predicted score difference by the last weight of the model and then apply the sigmoid function,
you get the win probability of the game.

# Import the sigmoid function from scipy
from scipy.special import expit as sigmoid

# Weight from the model
weight = 0.14

# Print the approximate win probability predicted close game
print(sigmoid(1 * weight + 0))    cero because earlier we specified use_bias=False

# Print the approximate win probability predicted blowout game
print(sigmoid(10 * weight + 0))

So sigmoid(1 * 0.14) is 0.53, which represents a pretty close game 
and sigmoid(10 * 0.14) is 0.80, which represents a pretty likely win. 
In other words, if the model predicts a win of 1 point,
it is less sure of the win than if it predicts 10 points.   Who says neural networks are black boxes?


# Evaluate the model on new data
print(model.evaluate(games_tourney_test[['seed_diff', 'pred']],
               [games_tourney_test[['score_diff']], games_tourney_test[['won']]], verbose=False))
               
  <script.py> output:
    [9.685116468970456, 9.11585681592647, 0.5692595753503676]
first one is the sum of both loss functions, seond is regression loss, third one is classification loss
This model is both a good regressor and a good classifier!




----------->  Wrap-up

---Shared layers are incredibly useful when you want to compare 2 things---

-Share models are known as ----Siamese Networks----
(used to calculate things like document SIMILARITY, using a shared embedding layer
and a LSTM [long-short term memory]{commonly used for text analysis}, and then comparing the LSTM outputs.
Since both documents are encodded with the same embedding layer and the same LSTM layer, 
the model uses a representation of the documentas that can be used to compare them.

-You can make networks that understand both text and images simulataneously.

-SKIP CONNECTIONS , simplify the optimization of neural networks.
These make it a lot easier for the adam optimizer to find the global minimum of the network's loss function.

"In Keras, implementing a skip connection is as simple as using a Concatenate layer
to concatenate the inputs to the deep network's outputs
right before the final output layer.

output_tensor = Concatenate () ([input_tensor, hidden_tensor])


-Multiple input networks are especially useful when you want to process
different types of data within your model.

-For NUMERIC data, such as home vs away (1,0), we skip the embedding layer
and pass the input directly to the output.

-For fitting and evaluating you need to use lists  data[] and double lists[[]] for multiple inputs/outputs

-Keras 'Functional API' allows you to define shared layers while the Sequential API does not.



----Practice

-
inputs = Input (shape= (3,) )       if it's a layer it uses the 'shape' 
                                    if it's a tensor it uses just the number  Input( (3,) )
dense_layer = Dense (1)

output = dense_layer (inputs)

print(output)



-     merged_in = Substract() ([in_1,in_2])



- 
I have a model (embed_model)  with an embedding layer  

Now I want to connect this model to a new input (new_in)

new_in = Input (shape = (3,0) )

new_embeddings = embed_model (new_in)

print (new_embeddings)


-
Merge 2 input layers usind Add ()

merged_in = Add () ([ in_1, in_2])

-To calculate the slope
slope = 2 * input_data * error

-Correct order
Sequential (), add, compile, fit

-
model.evaluate( [test_1,test_2] , test_labels)

-
model = Sequential()

model.add(Dense(100,activation='relu, input_shape = (n_cols,)))
print (model.layers)

-
assign a name to this layer

input = Input(shape=(5,), name= 'Inputs')


-
input_layer = Input(shape=(2,))
output = Dense (2) (input_layer)
model = Model (input_layer, output)
print (output)



















