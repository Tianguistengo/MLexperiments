
Deep learning is able to account for the INTERACTIONS between features
(the more nodes in the hidden layers, the more interactions we are able to capture).

An activation function (which converts a node's input into its output)
allows the model to capture non-linearities (tanh, ReLu, etc..)

ReLu
"This function takes a single number as an input, returning 0 if the input is negative, 
and the input if the input is positive.

Here are some examples:
relu(3) = 3
relu(-3) = 0

-Deep nets make internal representations of the patterns in the data
that are useful to make predictions

They partially remove the need for feature engineering and manually creative better predictive features.

Deep learning is also called representation learning.

As data moves through hidden layers, increasingly complex patterns and representations are learned.

-The loss function aggregates all of the errors into a single score.

- a SLOPE (as in the tangent line next to a weight in a chart that we use to know where the next step
of the gradient descent should go to) == a DERIVATIVE (calculus)


-Gradient Descent

Gradient == an array of slopes

After "calculating the slopes, it's time to use those slopes to improve a model.
If you add the slopes to the weights, you will move in the right direction. 

We use prediction erros to calculate slopes



-Backpropagation

Tries to deduce the slopes to update weights efficiently (going from the OUTput to the INput)
{estimate the slope OF THE LOSS FUNCTION with respect to EACH WEIGHT in the network)


Each time you generate predictions using forward propagation, 
you update the weights using backward propagation.

3 things we multiply to get the slopes associated with any weight:
   -node value feeding into that weight
   -slope of the activation function for the node being fed into (1 for any node receiving a positive value as input)
   -slope of the loss function with respect to the output node


-RECAP (forward and backwarward propagation + gradient descent
     -Start with random weights
     -Do forward propagation to get a prediction
     -Use backpropagation to estimate the slope OF THE LOSS FUNCTION with respect to EACH WEIGHT in the network
     -Multiply that slope by a learning rate, and subtract from the current weights
     -Keep doing that until you reach the flat part 


-For computational efficiency, calculate slopes on a subset of the data (or a batch, meaning, STOCHASTIC GRADIENT DESCENT)
{gradient descent uses the whole training data for calculating each slope}
for each update of the weights
and use a DIFFERENT BATCH  to calculate the next udpate.
Once all data is used, start from the beginning again.
Each time you go through the entire data is called an 'EPOCH'


- Keras
           4 steps
                      -SPECIFY THE ARCHITECTURE
               
               We need to load the data first because we need to know how many columns there are 
               and we need this information because the number of colums is the number of nodes in the input layer
               # Save the number of columns in n_cols:
               
                       n_cols = data_matrix.shape[1]
                       
                      -COMPILE
                      
               --->Specify the optimizer
               
               ADAM is an excellent choice as yout go-to optimizer
               (ADJUSTS THE LEARNING RATE as it does gradient descent
               to ensure reasonable values throughout the weight optimization process)
               
               --->Specify the loss function
               
               mean squared error is common for regression
               
                             
                      -FIT
                      
               Apply backpropagation and gradient descent to your data to update the weights
               
               Scaling data before fitting can help the optimization process
               
               model.fit -> Remember that the first argument is the predictive features (predictors),
               and the data to be predicted (target) is the second argument.
                      
                      -PREDICT



-a DENSE layer means all the nodes in one layer connect to all the nodes in the next layer.
other layers are Dropout, Flatten, Conv2D, MaxPool2D

-a SEQUENTIAL model means a layer has only connections or weights to the layer coming directly after it
(other models have more exotic structures).

 
 
 ------- CLASSIFICATION
 
 -Loss function is (most commonly) 'categorical_crossentropy' instead of MSE
 (in math is LogLoss==lower score is better).
 
 -Add         metrics= ['accuracy']      to the compile step
  for easier to understand diagnostics
  
 -Ouput layer has 2 nodes instead of 1 (one for each possible outcome),
 and uses a 'softmax' activation function.
 
 from keras.utils import    to_categorical    to get 0 and 1 



You want to SAVE,  RELOAD, and PREDICT   with your model

from keras.model import load_model

model.save('model_file.h5')  h5 is short for the model format normally used 

my_model=load_model('my_model.h5')

# Calculate predictions: predictions
predictions = model.predict(data_to_predict_with)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]


-a common learning reate is 0.01

- the DYING NEURON PROBLEM
once a node starts getting negative inputs, it may continue getting only negative inputs.
It's contributing nothing to the network and the neuron is regarded as dead


-"  Vanishing Gradient Problem


The tendency for the gradients of early hidden layers of some deep neural networks to become surprisingly flat (low).
Increasingly lower gradients result in increasingly smaller changes to the weights on nodes in a deep neural network, 
leading to little or no learning. 
Models suffering from the vanishing gradient problem become difficult or impossible to train.
Long Short-Term Memory cells address this issue. (also changing your activation function)

Compare to exploding gradient problem.


-----> Model validation

neural nets are usually so large that we don't really need to do k-fold cross validation,
a single run over the database would be large enough

-Early Stopping 
(patience=  , is how many epochs can the model go without improving before we stop training)
(3 is resonable, 1 no because sometimes it keeps improving after 1 bad one)
if you have a logic for when to stop (early stopping) you can set a higher # of maximum epochs to see if it learns better
(epochs=20, in the fit step) by default, keras trains for 10 epochs

from keras.callbacks import EarlyStopping

# Compile the model
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Fit the model
model.fit(predictors, target, epochs=30, validation_split=0.3,callbacks=[early_stopping_monitor])


-validaton split (in the fit step) + define metrics= (in the compile step, 'accuracy' for classification)

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
model.fit(predictors,target,validation_split=0.3)

-now that we have cool ways of measuring performance (namely VALIDATION LOSS after doing ^)
we can experiment with 
                            -different architectures
                            -more or fewer layers
                            -layers with more or fewer nodes
                            -etc.
















