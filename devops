"Today, one of the big factors that can affect the accuracy of deployed models is if the data 
being used to generate predictions differs from data used to train the model.

For example, changing economic conditions could drive new interest rates affecting home purchasing predictions. 
------This is called concept drift,------
whereby the patterns the model uses to make predictions no longer apply. 
SageMaker Model Monitor automatically detects concept drift in deployed models and provides detailed alerts 
that help identify the source of the problem.
All models trained in SageMaker automatically emit key metrics that can be collected and viewed in SageMaker Studio.
From inside SageMaker Studio you can configure data to be collected, how to view it, and when to receive alerts.



---
What is a data lake?

A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.




-- devops sagemaker ML
https://aws.amazon.com/devops/what-is-devops/?nc1=f_cc
https://aws.amazon.com/sagemaker/



----------->   Pytorch

https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec

What makes it really luring is it’s ---> dynamic computation graph paradigm. <---
Don’t worry if the last line doesn’t make sense to you now.
But take my word that it makes --> debugging neural networks way easier. <--

>>>>PyTorch is easier to learn and lighter to work with, and hence,
is relatively better for passion projects and building rapid prototypes.

TensorFlow has adopted PyTorch innovations and PyTorch has adopted TensorFlow innovations. 
>>>>Notably, now both languages can run in a >>> dynamic eager execution mode >>>>or a static graph mode.
Both frameworks are open source, but PyTorch is Facebook's baby and TensorFlow is Google's baby.

In the recent NerulIPS conference,
PyTorch was in 166 papers and TensorFlow was in 74. 
PyTorch went from being in fewer papers than TensorFlow in 2018 to more than doubling TensorFlow’s number in 2019.

TensorFlow is still mentioned in many more >>>job listings that PyTorch, but the gap is closing.
PyTorch has taken the lead in usage in >>>>research papers at top conferences 
and almost closed the gap in Google search results. 
TensorFlow remains three times more common in usage according to the most recent Stack Overflow Developer Survey.

" On 1st October 2019, the first stable version of TF2.0 was released.
The new features include tight integration with Keras,
Eager Execution as default (finally 😅), 
functions and not sessions, 
Multi-GPU support and much more. 
>>>Another beautiful feature is tf.function decorator, 
>>>that converts a block of code into optimized graphs thus providing faster execution.

(pro tf)Comparing tensorflow to pytorch is like comparing a car to an engine block.
It tricks you into just comparing the engine parts and not even considering the stuff around it.

At their core, PyTorch and Tensorflow are >>>auto-differentiation frameworks. 
That is, they >>allow one to take the derivative of some function.
However, there are many ways to enable auto-differentiation, 
and the particular implementation that most modern ML frameworks choose is called “reverse-mode auto-differentiation”,
more commonly known as “backpropagation”. 
This implementation turns out to be extremely efficient for taking the derivative of neural networks.

However, things change for computing higher order derivatives (Hessian/Hessian Vector Products).




The reason why we use --> Numpy <-- is because it’s much faster than Python lists at doing matrix ops.
Why? Because it does most of the heavy lifting in C.

But, in case of training deep neural networks, NumPy arrays simply don’t cut it.
I’m too lazy to do the actual calculations here (google for “FLOPS in one iteration of ResNet to get an idea), 
but code utilising NumPy arrays alone would take months to train some of the state of the art networks.

This is where --Tensors-- come into play.
PyTorch provides us with a data structure called a Tensor, 
which is very similar to NumPy’s ndarray. 
But unlike the latter, tensors can tap into the resources of a GPU to significantly speed up matrix operations.



(  The N-dimensional array (ndarray)  )

An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size. 
The number of dimensions and items in an array is defined by its shape, which is a tuple of N non-negative integers 
that specify the sizes of each dimension. 
The type of items in the array is specified by a separate data-type object (dtype),
one of which is associated with each ndarray.

As with other container objects in Python, 
the contents of an ndarray can be accessed and modified by indexing or slicing the array 
(using, for example, N integers), and via the methods and attributes of the ndarray.

Different ndarrays can share the same data, 
so that changes made in one ndarray may be visible in another. 
That is, an ndarray can be a “view” to another ndarray, 
and the data it is referring to is taken care of by the “base” ndarray.
ndarrays can also be views to memory owned by Python strings or objects implementing the buffer or array interfaces.



(( The chain rule  )) 
The chain rule is conceptually a divide and conquer strategy  that breaks complicated expressions into SUBEXPRESSIONS
whose DERIVATIVES are easier to compute. 
Its power derives from the fact that we can process each simple subexpression in ISOLATION
yet still combine the INTERMEDIATE RESULTS to get the correct overall result.

The chain rule comes into play when we need the derivative of an expression composed of nested subexpressions. 





BUILDING BLOCK 2: COMPUTATION GRAPHS

 When a neural network is trained, we need to compute gradients of the loss function,
 with respect to EVERY weight and bias,
 and then update these weights using gradient descent.

With neural networks hitting billions of weights, doing the above step efficiently can make or break
the feasibility of training.
Building Block #2.1: Computation Graphs

Computation graphs lie at the heart of the way modern deep learning networks work, and PyTorch is no exception. 

>>>>>  The computation graph is simply a data structure that allows you to efficiently apply the chain rule
to compute gradients for all of your parameters.<<


BUILDING BLOCK #3 : Variables and Autograd

PyTorch accomplishes what we described above using the Autograd package.

Now, there are basically three important things to understand about how Autograd works.

#3.1 : Variable

The Variable, just like a Tensor is a CLASS that is used to hold data.
It differs, however, in the way it’s meant to be used.
--> Variables are specifically tailored to hold values which change during training of a neural network, <---
i.e. the learnable paramaters of our network.
Tensors on the other hand are used to store values that are not to be learned. 
For example, a Tensor maybe used to store the values of the loss generated by each example.

A Variable class wraps a tensor. 
You can access this tensor by calling .data attribute of a Variable.

The Variable also stores the gradient of a scalar quantity (say, loss) with respect to the parameter it holds. 
This gradient can be accessed by calling the .grad attribute.
This is basically the gradient computed up to this particular node,
and the gradient of the every subsequent node, can be computed by multiplying the edge weight 
with the gradient computed at the node just before it.


-
Tensors 

PyTorch Tensors Explained - Neural Network Programming - deeplizard

"
 PyTorch tensors are instances of the >>>>  torch.Tensor Python class. 
 We can create a torch.Tensor object >> using the class constructor like so:

> t = torch.Tensor()
> type(t)
torch.Tensor

This creates an empty tensor (tensor with no data), but we'll get to adding data in just a moment.

Tensor attributes:

First, let’s look at a few tensor attributes. Every torch.Tensor has these attributes:

  >  torch.dtype
  >  torch.device
  >  torch.layout

Looking at our Tensor t, we can see the following default attribute values:

> print(t.dtype)
> print(t.device)
> print(t.layout)
torch.float32
cpu
torch.strided



Tensors have a >>>>torch.dtype

The dtype, which is torch.float32 in our case, 
specifies the type of the data that is contained within the tensor. 
Tensors contain uniform (of the same type) numerical data with one of these types: 


Data type 	            dtype 	        CPU tensor 	       GPU tensor
32-bit floating point 	torch.float32 	torch.FloatTensor 	torch.cuda.FloatTensor
64-bit floating point 	torch.float64 	torch.DoubleTensor torch.cuda.DoubleTensor
16-bit floating point 	torch.float16 	torch.HalfTensor 	 torch.cuda.HalfTensor
8-bit integer (unsigned) torch.uint8 	torch.ByteTensor 	 torch.cuda.ByteTensor
8-bit integer (signed)  torch.int8 	  torch.CharTensor 	 torch.cuda.CharTensor
16-bit integer (signed) 	torch.int16 	torch.ShortTensor 	torch.cuda.ShortTensor
32-bit integer (signed) 	torch.int32 	torch.IntTensor 	  torch.cuda.IntTensor
64-bit integer (signed) 	torch.int64 	torch.LongTensor  	torch.cuda.LongTensor


 >>> Tensors have a torch.device

The device, cpu in our case, specifies the device (CPU or GPU) 
where the tensor's data is >> allocated.
This determines where tensor computations for the given tensor will be performed.

PyTorch supports the use of multiple devices, and they are specified using an index like so:

> device = torch.device('cuda:0')
> device
device(type='cuda', index=0)


If we have a device like the above, we can create a tensor on the device
by passing the device to the tensor’s constructor. 

One thing to keep in mind about using multiple devices
> is that tensor operations between tensors must happen between tensors that exists on the same device.

Using multiple devices is typically something we will do as we become more advanced users



>>  Tensors have a torch.layout

The layout, strided in our case, specifies >> how the tensor is stored in memory.
To learn more about stride check here.


As neural network programmers, we need to be aware of the following:

 >>   Tensors contain data of a uniform type (dtype).
 >>   Tensor computations between tensors depend on the .dtype and the .device 
 (tensos have to have same dtype and be on same device).

Let’s look now at the common ways of creating tensors using data in PyTorch.
Creating tensors using data

These are the primary ways of >>>  creating tensor objects (instances of the torch.Tensor class),
with data (array-like) in PyTorch:
(four ways to create a torch.Tensor object:)

    torch.Tensor(data)
    torch.tensor(data)
    torch.as_tensor(data)
    torch.from_numpy(data)

Let’s look at each of these.
>> They all accept some form of data 
and
give us an instance of the torch.Tensor class.

torch.Tensor() and torch.tensor() --copy-- their input data 
while torch.as_tensor() and torch.from_numpy() --share-- their input data in memory with the original input object. 

Share Data 	Copy Data
torch.as_tensor() 	torch.tensor()
torch.from_numpy() 	torch.Tensor()

This sharing just means that the --actual data in memory-- exists in a single place.
As a result, any changes that occur in the underlying data >> will be reflected in both objects,
the torch.Tensor and the numpy.ndarray.

Sharing data is more efficient and uses less memory than copying data 
because the data is not written to two locations in memory.

If we have a torch.Tensor and we want to convert it to a numpy.ndarray, we do it like so:

> print(o3.numpy())
> print(o4.numpy())
[0 2 3]
[0 2 3]

 This establishes that torch.as_tensor() and torch.from_numpy() both >>share memory with their input data.
 However, which one should we use, and how are they different?

The torch.from_numpy() function only accepts numpy.ndarrays, 
while the torch.as_tensor() function accepts a wide variety of array-like objects 
including other PyTorch tensors.

For this reason, torch.as_tensor() is the winning choice in the memory sharing game.

Best options for creating tensors in PyTorch
Given all of these details, these two are the best options:

    torch.tensor()
    torch.as_tensor()

The torch.tensor() call is the sort of go-to call, 
while torch.as_tensor() should be employed when tuning our code for performance. 

 Some things to keep in mind about memory sharing (it works where it can):

    Since >numpy.ndarray objects are allocated on the CPU,
    the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used.
    
    The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists.
    
    The as_tensor() call requires developer knowledge of the sharing feature.
    This is necessary so we don’t inadvertently make an unwanted change 
    in the underlying data without realizing the change impacts multiple objects.
    
    The as_tensor() performance improvement will be greater
    if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. 
    However, if there is just a single load operation, 
    there shouldn’t be much impact from a performance perspective.


Wrapping up

At this point, we should now have a better understanding of
the  > PyTorch tensor creation options. 

We’ve learned about factory functions 
and we’ve seen how memory sharing vs copying can impact performance and program behavior. 





 Now, let’s create our tensors with each of these options 1-4, and have a look at what we get:

> o1 = torch.Tensor(data)
> o2 = torch.tensor(data)
> o3 = torch.as_tensor(data)
> o4 = torch.from_numpy(data)

> print(o1)
> print(o2)
> print(o3)
> print(o4)
tensor([1., 2., 3.])
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)

All of the options (o1, o2, o3, o4) 
appear to have produced the same tensors except for the first one.
The first option (o1) has dots after the number indicating that the numbers are floats,
while the next three options have a type of int32. 

 Uppercase/lowercase: torch.Tensor() vs torch.tensor()

Notice how the first option torch.Tensor() has an uppercase T 
while the second option torch.tensor() has a lowercase t. 
What’s up with this difference?

The first option with the uppercase T is the --constructor-- of the torch.Tensor class,
and the second option is what we call a --factory function--
that constructs torch.Tensor objects 
and returns them to the caller. 

You can think of the torch.tensor() function
as a factory that builds tensors given some parameter inputs.

Factory functions are a software design pattern >>for creating objects. 

But which way is better between these two? 
The answer is that ----> it’s fine to use either one. 
However, the factory function torch.tensor() has better documentation and more configuration options, 
so it gets the winning spot at the moment. 


torch.Tensor() and torch.tensor() >  --copy-- their input data 
while torch.as_tensor() and torch.from_numpy() >  --share-- their input data in memory with the original input object. 


  > Creation options   WITHOUT DATA

Here are some other creation options that are available.

>We have the torch.eye() function
which returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
The name eye() is connected to the idea of an identity matrix ,
which is a square matrix with ones on the main diagonal and zeros everywhere else.

 print(torch.eye(2))
tensor([
    [1., 0.],
    [0., 1.]
])


>We have the torch.zeros() function 
that creates a tensor of zeros with the shape of specified shape argument. 

 This task is a breeze if we are using numpy.ndarrays,
 so congratulations if you are already familiar with NumPy. 

print(torch.zeros([2,2]))
tensor([
    [0., 0.],
    [0., 0.]
])

Similarly, we have the torch.ones() function that creates a tensor of ones.

> print(torch.ones([2,2]))
tensor([
    [1., 1.],
    [1., 1.]
])

 We also have the torch.rand() function that creates a tensor with a shape of the specified argument whose values are random.

> print(torch.rand([2,2]))
tensor([
    [0.0465, 0.4557],
    [0.6596, 0.0941]
])

This is a small subset of the available creation functions that don’t require data. Check with the PyTorch documentation for the full list. 



In PyTorch, we transfer a tensor t to a GPU with:
t.cuda()




PyTorch tensors as we have seen are ---instances--- of the > torch.Tensor  PyTorch class. 
The difference between the abstract concept of a tensor and a PyTorch tensor 
is that PyTorch tensors give us a concrete implementation 
that we can work with in code.



 GPU can be slower than CPU

We said that we can selectively run our computations on the GPU or the CPU,
but why not just run every computation on the GPU?
Isn’t a GPU faster than a CPU?

>>> The answer is that a GPU is only faster for particular (specialized) tasks. 
One issue that we can run into is bottlenecks that slow our performance. 

For example, moving data from the CPU to the GPU is costly, 
so in this case, the overall performance might be slower >>  if the computation task is a simple one. 

Remember, the GPU works well for >> tasks that can be broken into many smaller tasks,
and if a compute task is already small, we won’t have much to gain by moving the task to the GPU.





















