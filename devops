"Today, one of the big factors that can affect the accuracy of deployed models is if the data 
being used to generate predictions differs from data used to train the model.

For example, changing economic conditions could drive new interest rates affecting home purchasing predictions. 
------This is called concept drift,------
whereby the patterns the model uses to make predictions no longer apply. 
SageMaker Model Monitor automatically detects concept drift in deployed models and provides detailed alerts 
that help identify the source of the problem.
All models trained in SageMaker automatically emit key metrics that can be collected and viewed in SageMaker Studio.
From inside SageMaker Studio you can configure data to be collected, how to view it, and when to receive alerts.


----------->   Pytorch

https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec

What makes it really luring is it’s ---> dynamic computation graph paradigm. <---
Don’t worry if the last line doesn’t make sense to you now.
But take my word that it makes --> debugging neural networks way easier. <--

>>>>PyTorch is easier to learn and lighter to work with, and hence,
is relatively better for passion projects and building rapid prototypes.

TensorFlow has adopted PyTorch innovations and PyTorch has adopted TensorFlow innovations. 
>>>>Notably, now both languages can run in a >>> dynamic eager execution mode >>>>or a static graph mode.
Both frameworks are open source, but PyTorch is Facebook's baby and TensorFlow is Google's baby.

In the recent NerulIPS conference,
PyTorch was in 166 papers and TensorFlow was in 74. 
PyTorch went from being in fewer papers than TensorFlow in 2018 to more than doubling TensorFlow’s number in 2019.

TensorFlow is still mentioned in many more >>>job listings that PyTorch, but the gap is closing.
PyTorch has taken the lead in usage in >>>>research papers at top conferences 
and almost closed the gap in Google search results. 
TensorFlow remains three times more common in usage according to the most recent Stack Overflow Developer Survey.

On 1st October 2019, the first stable version of TF2.0 was released.
The new features include tight integration with Keras,
Eager Execution as default (finally 😅), 
functions and not sessions, 
Multi-GPU support and much more. 
>>>Another beautiful feature is tf.function decorator, 
>>>that converts a block of code into optimized graphs thus providing faster execution.

(pro tf)Comparing tensorflow to pytorch is like comparing a car to an engine block.
It tricks you into just comparing the engine parts and not even considering the stuff around it.

At their core, PyTorch and Tensorflow are >>>auto-differentiation frameworks. 
That is, they >>allow one to take the derivative of some function.
However, there are many ways to enable auto-differentiation, 
and the particular implementation that most modern ML frameworks choose is called “reverse-mode auto-differentiation”,
more commonly known as “backpropagation”. 
This implementation turns out to be extremely efficient for taking the derivative of neural networks.

However, things change for computing higher order derivatives (Hessian/Hessian Vector Products).




The reason why we use --> Numpy <-- is because it’s much faster than Python lists at doing matrix ops.
Why? Because it does most of the heavy lifting in C.

But, in case of training deep neural networks, NumPy arrays simply don’t cut it.
I’m too lazy to do the actual calculations here (google for “FLOPS in one iteration of ResNet to get an idea), 
but code utilising NumPy arrays alone would take months to train some of the state of the art networks.

This is where --Tensors-- come into play.
PyTorch provides us with a data structure called a Tensor, 
which is very similar to NumPy’s ndarray. 
But unlike the latter, tensors can tap into the resources of a GPU to significantly speed up matrix operations.



(  The N-dimensional array (ndarray)  )

An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size. 
The number of dimensions and items in an array is defined by its shape, which is a tuple of N non-negative integers 
that specify the sizes of each dimension. 
The type of items in the array is specified by a separate data-type object (dtype),
one of which is associated with each ndarray.

As with other container objects in Python, 
the contents of an ndarray can be accessed and modified by indexing or slicing the array 
(using, for example, N integers), and via the methods and attributes of the ndarray.

Different ndarrays can share the same data, 
so that changes made in one ndarray may be visible in another. 
That is, an ndarray can be a “view” to another ndarray, 
and the data it is referring to is taken care of by the “base” ndarray.
ndarrays can also be views to memory owned by Python strings or objects implementing the buffer or array interfaces.



(( The chain rule  )) 
The chain rule is conceptually a divide and conquer strategy  that breaks complicated expressions into SUBEXPRESSIONS
whose DERIVATIVES are easier to compute. 
Its power derives from the fact that we can process each simple subexpression in ISOLATION
yet still combine the INTERMEDIATE RESULTS to get the correct overall result.

The chain rule comes into play when we need the derivative of an expression composed of nested subexpressions. 





BUILDING BLOCK 2: COMPUTATION GRAPHS

 When a neural network is trained, we need to compute gradients of the loss function,
 with respect to EVERY weight and bias,
 and then update these weights using gradient descent.

With neural networks hitting billions of weights, doing the above step efficiently can make or break
the feasibility of training.
Building Block #2.1: Computation Graphs

Computation graphs lie at the heart of the way modern deep learning networks work, and PyTorch is no exception. 

>>>>>  The computation graph is simply a data structure that allows you to efficiently apply the chain rule
to compute gradients for all of your parameters.<<


BUILDING BLOCK #3 : Variables and Autograd

PyTorch accomplishes what we described above using the Autograd package.

Now, there are basically three important things to understand about how Autograd works.

#3.1 : Variable

The Variable, just like a Tensor is a CLASS that is used to hold data.
It differs, however, in the way it’s meant to be used.
--> Variables are specifically tailored to hold values which change during training of a neural network, <---
i.e. the learnable paramaters of our network.
Tensors on the other hand are used to store values that are not to be learned. 
For example, a Tensor maybe used to store the values of the loss generated by each example.

A Variable class wraps a tensor. 
You can access this tensor by calling .data attribute of a Variable.

The Variable also stores the gradient of a scalar quantity (say, loss) with respect to the parameter it holds. 
This gradient can be accessed by calling the .grad attribute.
This is basically the gradient computed up to this particular node,
and the gradient of the every subsequent node, can be computed by multiplying the edge weight 
with the gradient computed at the node just before it.







