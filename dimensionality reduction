
A.G.  The following Python code uses NumPy’s svd() function to obtain all the principal
components of the training set, then extracts the first two PCs:

X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]

--Scikit-Learn’s PCA class implements PCA using SVD decomposition.
The following code applies PCA to reduce the dimensionality of the dataset
down to two dimensions (note that it automatically takes care of centering the data):

from sklearn.decomposition import PCA

pca = PCA(n_components = 2)
X2D = pca.fit_transform(X)

----The following code compresses the MNIST dataset down to 154 dimensions

pca = PCA(n_components = 154)
X_reduced = pca.fit_transform(X_train)


2 approaches: 

--projection 

a much lower-dimensional SUBSPACE of the high-dimensional space
Notice that all training instances lie close to a plane: this is a lower-dimensional (2D)
subspace of the high-dimensional (3D) space.

-- Manifold Learning

The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D
shape that can be bent and twisted in a higher-dimensional space.





3 dimensionality reduction techniques:



---- Principal Component Analysis (PCA)

PCA identifies the axis that accounts for the largest amount of variance in the train‐
ing set.


-- --Kernel PCA

The following code uses Scikit-Learn’s KernelPCA class to perform kPCA
with an RBF kernel :

from sklearn.decomposition import KernelPCA

rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)

++the followingcode creates a two-step pipeline (first reducing dimensionality to two dimensions
using kPCA, then applying Logistic Regression for classification).
Then it uses GridSearchCV to find the best kernel and gamma value for kPCA in order to get the best
classification accuracy at the end of the pipeline:

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

clf = Pipeline([
("kpca", KernelPCA(n_components=2)),
("log_reg", LogisticRegression())
])

param_grid = [{
"kpca__gamma": np.linspace(0.03, 0.05, 10),
"kpca__kernel": ["rbf", "sigmoid"]
}]

grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)

The best kernel and hyperparameters are then available through the best_params_
variable:
>>> print(grid_search.best_params_)
{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}

-- LLE (Locally Linear Embedding)


A.G. "

It is common for performance to drop slightly when reducing dimensionality, 
because we do lose some useful signal in the process. However, the performance drop is rather severe in this case. 
So PCA really did not help: it slowed down training and reduced performance.   :(   "
