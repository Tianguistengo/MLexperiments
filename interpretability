https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html

dif between interpretability and explainability

Interpretability 
is about the extent to which a cause and effect can be observed within a system. 
Or, to put it another way, it is the extent to which you are able to predict what is going to happen, 
given a change in input or algorithmic parameters. 
It’s being able to look at an algorithm and go yep, I can see what’s happening here.

Explainability, 
meanwhile, is the extent to which the internal mechanics of a machine or deep learning system 
can be explained in human terms. 

It’s easy to miss the subtle difference with interpretability, but consider it like this: 
interpretability is about being able to discern the mechanics without necessarily knowing why. 
Explainability is being able to quite literally explain what is happening.

Techniques and methods for improving machine learning interpretability

While questions of transparency and ethics may feel abstract for the data scientist on the ground, there are, in fact, 
a number of practical things that can be done to improve an algorithm’s interpretability and explainability.

>Algorithmic generalization
The first is to improve generalization. This sounds simple, but it isn’t that easy.
When you think most machine learning engineering is applying algorithms in a very specific way 
to uncover a certain desired outcome, the model itself can feel like a secondary element - it’s simply a means to an end.
However, by shifting this attitude to consider the overall health of the algorithm, and the data on which it is running,
you can begin to set a solid foundation for improved interpretability.

-Pay attention to feature importance
This should be obvious, but it’s easily missed. Looking closely at the way the various features of your algorithm
have been set is a practical way to actually engage with a diverse range of questions, from business alignment to ethics.
Debate and discussion over how each feature should be set might be a little time-consuming,
but having that tacit awareness that different features have been set in a certain way is nevertheless an important step 
in moving towards interpretability and explainability.

-LIME: Local Interpretable Model-Agnostic Explanations
While the techniques above offer practical steps that data scientists can take,
LIME is an actual method developed by researchers to gain greater transparency on what’s happening inside an algorithm.
The researchers explain that LIME can explain “the predictions of any classifier in an interpretable and faithful manner, 
>>by learning an interpretable model locally around the prediction.”

What this means in practice is that the LIME model develops an approximation of the model 
by testing it out to see what happens when certain aspects within the model are changed. 
>>>Essentially it’s about trying to recreate the output from the same input through a process of experimentation.

-DeepLIFT (Deep Learning Important Features)
DeepLIFT is a useful model in the particularly tricky area of deep learning.
It works through a form of backpropagation:
it takes the output, then attempts to pull it apart
by ‘reading’ the various neurons that have gone into developing that original output.

Essentially, it’s a way of digging back into the feature selection inside of the algorithm (as the name indicates).

-Layer-wise relevance propagation
Layer-wise relevance propagation is similar to DeepLIFT, 
in that it works backwards from the output, identifying the most relevant neurons within the neural network until you return to the input (say, for example, an image). If you want to learn more about the mathematics behind the concept, this post by Dan Shiebler is a great place to begin.
Adding complexity to tackle complexity: can it improve transparency?

The central problem with both explainability and interpretability is that you’re adding an additional step in the development process. Indeed, you’re probably adding multiple steps. From one perspective, this looks like you’re trying to tackle complexity with even greater complexity.

-----------

"using deep networks for scientific discovery in physiological signals"

p.16
Building efficient interpretable systems 
is one of the major obstacles to deployment of machine learning models in healthcare (Tonekaboni et al., 2019). 
Some studies have attempted to interpret the decisions made by their black-box models. 
For 2D imaging data, Rajpurkar et al. (2017) >> inspect class activation mappings of their derived model
to gain trust in its predictions. 
For 1D ECG data, Goodfellow et al. (2018b) >> inspect neural activations 
and find correlates between decision rules of experts and deep networks.


-------------
Maithra Raghu and Eric Schmid


7. Interpretability Model Inspection and Representation Analysis

Many standard applications of deep learning (and machine learning more broadly) focus on 
>prediction — learning to output specific target values given an input.

Scientific applications, on the other hand, are often focused on 
>understanding — identifying underlying mechanisms giving rise to observed patterns in the data.
the ultimate goal remains to understand what attributes give rise to these observations.

To answer these kinds of questions, we can turn to interpretability techniques. 

Interpretability methods are sometimes equated to a fully understandable, step-by-step explanation of the model’s decision process. 
Such detailed insights can often be intractable, especially for complex deep neural network models. 

Instead, research in interpretability focuses on a much broader suite of techniques that can provide insights 
ranging from (rough) >>feature attributions — determining what input features matter the most, 
to >>model inspection — determining what causes certain neurons in the network to fire. 
In fact, these two examples also provide a rough split in the type of interpretability method.
