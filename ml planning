
https://machinelearningmastery.com/plan-run-machine-learning-experiments-systematically/

a simple approach to plan and manage your machine learning experiments.

With this approach, you will be able to:

    Stay on top of the most important questions and findings in your project.
    Keep track of what experiments you have completed and would like to run.
    Zoom in on the data preparations, models, and model configurations that give the best performance.
    
Only experiments that are planned are set-up and run 
and their order ensures that the most important experiments are run first.

You will be surprised at how much such a simple approach can free up your time
and get you thinking deeply about your project.
    
These are just an example from the last project I worked on. I recommend adapting these to your own needs.

    Sub-Project: A subproject may be a group of ideas you are exploring, a technique, a data preparation, and so on.
    Context: The context may be the specific objective such as beating a baseline, tuning, a diagnostic, and so on.
    Setup: The setup is the fixed configuration of the experiment.
    Name: The name is the unique identifier, perhaps the filename of the script.
    Parameter: The parameter is the thing being varied or looked at in the experiment.
    Values: The value is the value or values of the parameter that are being explored in the experiment.
    Status: The status is the status of the experiment, such as planned, running, or done.
    Skill: The skill is the North Star metric that really matters on the project, like accuracy or error.
    Question: The question is the motivating question the experiment seeks to address.
    Finding: The finding is the one line summary of the outcome of the experiment, the answer to the question.

I cannot say how much time this approach has saved me. 
And the number of >assumptions 
that it proved wrong in the pursuit of getting top results.



-------------

https://www.oreilly.com/radar/the-unreasonable-importance-of-data-preparation/



why data preparation is particularly important for reanalyzing data,

>and why you should stay focused on the question you hope to answer.

Your models are only as good as the data you feed them. 
This is the      >>> garbage in, garbage out principle
flawed data going in leads to flawed results, algorithms, and business decisions.

example self driving cars-
if such an algorithm is trained in an environment with cars driven by humans, 
how can you expect it to perform well on roads with other self-driving cars?

Beyond the autonomous driving example described, the “garbage in” side of the equation can take many forms
for example, 
incorrectly entered data, 
poorly packaged data, 
not representative data
and data collected incorrectly,


 drawing attention to thinking carefully about what you hope to get out of the data, 
 what question you hope to answer, 
 what biases may exist, a
 nd what you need to correct before jumping in with an analysis.
 With the right mindset, you can get a lot out of analyzing existing data

Data can be low-quality if:

    It doesn’t fit your question or its collection wasn’t carefully considered;
    
    It’s erroneous (it may say “cicago” for a location),
    inconsistent (it may say “cicago” in one place and “Chicago” in another),
    or missing;
    
    It’s good data but --packaged-- in an atrocious way—
    e.g., it’s stored across a range of siloed databases in an organization;
    
    It requires human labeling to be useful 
    (such as manually labeling emails as “spam” or “not” for a spam detection algorithm).

>Automating data preparation 
won’t necessarily remove such bias, 
but it will make it systematic, discoverable, auditable, unit-testable, and correctable. 

The third point above speaks more generally to the need for 
>automation around all parts of the data science workflow.

Model results will then be less reliant on individuals making hundreds of micro-decisions.
An added benefit is that the work will be reproducible and robust.

For the increasing number of real-time algorithms in production, 
humans need to be taken out of the loop at runtime as much as possible 
(and perhaps be kept in the loop more as algorithmic managers).

it’s important to recognize that we’ve done pretty well at democratizing
data collection and gathering, modeling, and data reporting, 
but what remains stubbornly difficult is the whole process of >>preparing the data.


---------------------


https://neptune.ai/blog/experiment-management


    So I was developing a machine learning model with my team 
    and within a few weeks of extensive experimentation, we got promising results…

    …unfortunately, we couldn’t tell exactly what performed best
    because we didn’t track feature versions, didn’t record the parameters, 
    and used different environments to run our models…

    …after a few weeks, we weren’t even sure what we have actually tried so we needed to rerun pretty much everything”


Experiment management in the context of machine learning is a process of tracking experiment metadata like:

    code versions
    data versions
    hyperparameters
    environment
    metrics


Tracking hyperparameters

Every machine learning model or pipeline needs hyperparameters.
Those could be learning rate, number of trees or a missing value imputation method.
Failing to keep track of hyperparameters can result in weeks of wasted time looking for them or retraining models.

The good thing is, keeping track of hyperparameters can be really simple. 
Let’s start with the way people tend to define them and then we’ll proceed to hyperparameter tracking:

Config files
Typically a .yaml file that contains all the information that your script needs to run. For example:

Command line + argparse
You simply pass your parameters to your script as arguments:

Parameters dictionary in main.py
You put all of your parameters in a dictionary inside your script:

Magic numbers all over the place
Whenever you need to pass a parameter you simply pass a value of that parameter.

We all do that sometimes but it is not a great idea especially if someone will need to take over your work.

Ok, so I do like .yaml configs and passing arguments from the command line (option 1 and 2),
but anything other than magic numbers is fine.
>>What is important is that you log those parameters for every experiment.

If you decide to pass all parameters as the script arguments make sure to log them somewhere. 
It is easy to forget, so using an experiment management tool that does this automatically can save you here. 

There is nothing so painful as to have a perfect script on perfect data version producing perfect metrics 
only to discover that you don’t remember what are the hyperparameters that were passed as arguments

Note:
A bonus of having your hyperparameters abstracted away entirely (option 1 and 2)
is that you implicitly turn your training and evaluation scripts into an objective function 
that you can optimize automatically:
That means you can use readily available libraries 
and run hyperparameter optimization algorithms 
with virtually no additional work! 



Data versioning

In real-life projects, data is changing over time. Some typical situations include:

    new images are added,
    labels are improved,
    mislabeled/wrong data is removed,
    new data tables are discovered,
    new features are engineered and processed,
    validation and testing datasets change to reflect the production environment.

For the vast majority of use cases whenever new data comes in,
you can save it in a new location and log this location and a hash of the data. 

Even if the data is very large, for example when dealing with images, 
you can create a smaller metadata file with image paths and labels and track changes of that file.

A wise man once told me:

    “Storage is cheap, training a model for 2 weeks on an 8-GPU node is not.”

And if you think about it, logging this information doesn’t have to be rocket science.

exp.set_property('data_path', 'DATASET_PATH')
exp.set_property('data_version', md5_hash('DATASET_PATH'))

You can calculate hash yourself,
use a simple data versioning extension 
or outsource hashing to a full-blown data versioning tool like DVC.


----------

https://mlpowered.com/posts/how-to-deliver-on-machine-learning-projects/

Tips

Depending on your diagnosis, here are a number of common sets of solutions.

If you need to tune the optimizer to better fit the data:

    For numerical optimizers, try adjusting the learning rate or momentum settings. 
 Starting with a small momentum (0.5) is usually easiest to get working.
 
    Try different initialization strategies, or start from a pre-trained model.
    
    Try a type of model that is easier to tune. 
 In deep learning, residual networks or networks with batch normalization may be simpler to train.

If the model is unable to fit the training data well:

    Use a larger or more expressive model class. 
 For example, when using decision trees you could make the trees deeper.
    
    Check the examples the model gets wrong on the training set for labeling errors, missing fields, etc.
 Investing time in training data cleanup can improve results significantly.

If the model isn’t generalizing to the development set:

    Add more training data. 
 Note that it may be important to focus on adding training examples similar to classes of errors seen in the development set.
   
   Augment your data with novel samples generated from real training examples. 
For example, if you notice your tree detector consistently performs poorly on foggy images, 
play with OpenCV to add an augmentation step that makes your images look a bit foggy.

    Search over a wider or finer-grained range of hyper-parameters 
 to ensure you find the model that performs best on the development set.
 
    Try a different form of regularization 
 (such as weight decay, dropout for neural networks, or pruning for decision trees).
 
    Try a different model class. 
 A different type of model can alter how well you fit your data and how well it generalizes,
 so it is difficult to know when this will work. 
 One advantage of deep learning is that there is a wide range of 
 “building block” neural network components you can easily try out. 
 If you’re using a traditional model (such as a decision tree or gaussian mixture model),
 switching model classes is much more involved. 
 If the assumptions embodied by the new model are more correct,
 the change might help — but it may be better to try easier things first.


-----

Implement
Build only what you need to build, and do it fast

You know what to try, and you’ve made it simple for yourself, now it’s just a matter of implementing… “just”.

The goal of this phase is to prototype rapidly so that you can measure the results,
learn from them, 
and go back around the cycle quickly. 

For that reason, we recommend focusing on building just what’s needed for your current experiment.

Note that while your goal is to learn quickly rather than polish everything, 
your work still needs to be correct 
and so you should check that the code works as expected -frequently.


------

When you code up a new model, start from a similar existing implementation.

Many research papers now have freely available code 
— so try to get code before reimplementing an idea from a paper as there are often undocumented details.
This will save you hours/days of work.
If possible, for any problem, we recommend going through these successive steps:

    Find an implementation of a model solving a similar problem.
    Reproduce the implementation locally in the conditions of the existing model (same dataset and hyperparameters).
    Slowly tweak the implementation of the model and the data pipeline to match your needs.
    Rewrite any parts needed.

Write tests to check whether your gradients, tensor values, and input data and labels  -->> are properly formatted. 

Do this as you are setting up your model initially, 
that way if you catch an error once, you will never have to deal with it again.



-----

Measure 

Print out your test results 
and any other metrics (e.g., production constraints)
you need to decide whether you’re ready to ship.

If your performance improved somewhat, you might be on the right track. 
In this case, now might be a good time to clean up any components you know are working well
and make sure that the experiment can be reproduced by others in your team.

On the other hand, if performance became worse or didn’t improve enough,
you’ll need to decide 
whether to try again (going back to the analysis phase)
or to abandon your current idea. 

These decisions are easier to make
>>>> if each cycle of the ML Loop is relatively cheap: 
you haven’t put too much energy into making your code perfect, 
and another attempt won’t take too long 
— — so you can decide what to do based on the risk and value of the idea
instead of sunk cost.


















