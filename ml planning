
https://machinelearningmastery.com/plan-run-machine-learning-experiments-systematically/

a simple approach to plan and manage your machine learning experiments.

With this approach, you will be able to:

    Stay on top of the most important questions and findings in your project.
    Keep track of what experiments you have completed and would like to run.
    Zoom in on the data preparations, models, and model configurations that give the best performance.
    
Only experiments that are planned are set-up and run 
and their order ensures that the most important experiments are run first.

You will be surprised at how much such a simple approach can free up your time
and get you thinking deeply about your project.
    
These are just an example from the last project I worked on. I recommend adapting these to your own needs.

    Sub-Project: A subproject may be a group of ideas you are exploring, a technique, a data preparation, and so on.
    Context: The context may be the specific objective such as beating a baseline, tuning, a diagnostic, and so on.
    Setup: The setup is the fixed configuration of the experiment.
    Name: The name is the unique identifier, perhaps the filename of the script.
    Parameter: The parameter is the thing being varied or looked at in the experiment.
    Values: The value is the value or values of the parameter that are being explored in the experiment.
    Status: The status is the status of the experiment, such as planned, running, or done.
    Skill: The skill is the North Star metric that really matters on the project, like accuracy or error.
    Question: The question is the motivating question the experiment seeks to address.
    Finding: The finding is the one line summary of the outcome of the experiment, the answer to the question.

I cannot say how much time this approach has saved me. 
And the number of >assumptions 
that it proved wrong in the pursuit of getting top results.



-------------

https://www.oreilly.com/radar/the-unreasonable-importance-of-data-preparation/



why data preparation is particularly important for reanalyzing data,

>and why you should stay focused on the question you hope to answer.

Your models are only as good as the data you feed them. 
This is the      >>> garbage in, garbage out principle
flawed data going in leads to flawed results, algorithms, and business decisions.

example self driving cars-
if such an algorithm is trained in an environment with cars driven by humans, 
how can you expect it to perform well on roads with other self-driving cars?

Beyond the autonomous driving example described, the “garbage in” side of the equation can take many forms
for example, 
incorrectly entered data, 
poorly packaged data, 
not representative data
and data collected incorrectly,


 drawing attention to thinking carefully about what you hope to get out of the data, 
 what question you hope to answer, 
 what biases may exist, a
 nd what you need to correct before jumping in with an analysis.
 With the right mindset, you can get a lot out of analyzing existing data

Data can be low-quality if:

    It doesn’t fit your question or its collection wasn’t carefully considered;
    
    It’s erroneous (it may say “cicago” for a location),
    inconsistent (it may say “cicago” in one place and “Chicago” in another),
    or missing;
    
    It’s good data but --packaged-- in an atrocious way—
    e.g., it’s stored across a range of siloed databases in an organization;
    
    It requires human labeling to be useful 
    (such as manually labeling emails as “spam” or “not” for a spam detection algorithm).

>Automating data preparation 
won’t necessarily remove such bias, 
but it will make it systematic, discoverable, auditable, unit-testable, and correctable. 

The third point above speaks more generally to the need for 
>automation around all parts of the data science workflow.

Model results will then be less reliant on individuals making hundreds of micro-decisions.
An added benefit is that the work will be reproducible and robust.

For the increasing number of real-time algorithms in production, 
humans need to be taken out of the loop at runtime as much as possible 
(and perhaps be kept in the loop more as algorithmic managers).

it’s important to recognize that we’ve done pretty well at democratizing
data collection and gathering, modeling, and data reporting, 
but what remains stubbornly difficult is the whole process of >>preparing the data.


---------------------


https://neptune.ai/blog/experiment-management


    So I was developing a machine learning model with my team 
    and within a few weeks of extensive experimentation, we got promising results…

    …unfortunately, we couldn’t tell exactly what performed best
    because we didn’t track feature versions, didn’t record the parameters, 
    and used different environments to run our models…

    …after a few weeks, we weren’t even sure what we have actually tried so we needed to rerun pretty much everything”


Experiment management in the context of machine learning is a process of tracking experiment metadata like:

    code versions
    data versions
    hyperparameters
    environment
    metrics








