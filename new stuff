
- andrew ng "Better Than Backprop

End-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. 
However, recent work suggests that neither is necessary to train effective neural networks to represent complex data. 
What’s new: Sindy Löwe, Peter O’Connor, and Bastiaan Veeling propose Greedy InfoMax (GIM),
an unsupervised method for learning to extract features that trains only one layer at a time.

Key insight: The information bottleneck theory (IB) suggests that neural networks work by
concentrating information like a data-compression algorithm. In data compression,
the amount of information retained is measured in mutual information (MI) between original and compressed versions. 
IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning
as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.


    GIM uses the previous layer’s output as the next layer’s input to train each layer independently. 
    This differs from the usual backpropagation in which all layers learn at once.
    
    The researchers devised a task that teaches layers to extract features that maximize MI.
    Given a subsequence of input data that has been compressed according to the current weights, 
    the layer predicts the next element in the compressed sequence, choosing from a random selection drawn 
    from the input including the correct choice. High success demonstrates that the layer is able to compress the input.
    
    The process effectively removes redundancy between nearby regions of the input. 
    For example, a recording of a song’s chorus may repeat several times, 
    so it’s possible to represent the recording without capturing the repetitions.
    
    
    --- MLPerf
    
        Fair and useful benchmarks for measuring -training and inference- performance of ML hardware, software, and services.

 
 
 -----Black box problem
" One solution is to build machine-learning systems that show their workings: so-called glassbox—as opposed to black-box—AI. 
 Glassbox models are typically much-simplified versions of a neural network in which it is easier to track
 how different pieces of data affect the model. It's a tricky balance, however. 
 Too much transparency can lead to information overload. 

Visualize this: Another approach is to include visualizations that show a few key properties of the model
and its underlying data. However it’s not clear if they really help.

So-called interpretable or explainable AI (XAI) aims to help people understand what features 
in the data a neural network is actually learning. 

In a new study, researchers found that people who used one of two popular interpretability tools didn’t actually 
understand them but trusted them anyway. In fact, use of the tools led to incorrect assumptions
about the data and the models, and instilled a false confidence that made participants more trigger-happy
about model deployment. This phenomenon is known as “automation bias”—and it could be increasing people’s chance of
using AI that’s inaccurate or biased. This is exactly the opposite effect of what XAI is trying to do in the first place.


-  from mit review 
"Facebook has a new way to track the images used to train a model.
The technique involves making imperceptible tweaks to images to create a kind of watermark.
Every model makes its own distinct mark, and the mark doesn’t impair the image classifier’s overall accuracy. 
This allows researchers to more easily match up models with the images that were used to train them 
in large-scale systems with complicated data pipelines.

Facebook calls the technique “radioactive data” 
because it is analogous to the use of radioactive markers in medicine, which show up in the body under x-ray.
Highlighting what data has been used to train an AI model makes them more transparent.
It could help flag potential sources of bias 
or reveal when a data set was used without permission or for inappropriate purposes. " very cool!


--"" New software can now fool the AI behind Alexa and Siri.
Software called TextFooler can now trick natural-language processing (NLP) systems into misunderstanding text 
just by replacing certain words in a sentence with synonyms. 
Developed by a team at MIT, it looks for the words that are most important to an NLP classifier
and replaces them with a synonym that a human would find natural. 
For example, it changes the sentence “The characters, cast in impossibly contrived situations, 
are totally estranged from reality” to “The characters, cast in impossibly engineered circumstances,
are fully estranged from reality.”
We see no difference, but AI interprets the sentences completely differently.


--"" Medication madness. According to the FDA, serious adverse drug interactions are estimated to kill 
more than 100,000 hospitalized people in the US every year.
But avoiding such interactions during drug development is laborious and expensive. 
It involves intensive testing and clinical trials to catalogue all the proposed drug’s possible chemical interactions
with existing ones.

Now a new AI system could make this easier by predicting the interactions
between two drugs based only on their chemical structure. 
The researchers first developed a new way to represent the 3D chemical structures of drugs in a character format
that could be read by a neural network. The drug melatonin, for example, is represented by “CC(=O)NCCC1=CNc2c1cc(OC)cc2.”

They then translated a database of known drug interactions into this format
and used it to train a neural network. The resulting system predicts the probability 
that two drugs will have an adverse interaction and shows the particular parts of the molecule 
that contributed to that prediction. When the researchers tested their system on two common drug interaction data sets,
it performed better than state-of-the-art results. Read more here.


---" Quotable

It’s really snowballed.

—Vincent Cate, who lives on the island of Anguilla, 
which has become an unlikely benefactor of the AI boom because of its domain name “.ai”

