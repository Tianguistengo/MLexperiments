
- andrew ng "Better Than Backprop

End-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. 
However, recent work suggests that neither is necessary to train effective neural networks to represent complex data. 
What’s new: Sindy Löwe, Peter O’Connor, and Bastiaan Veeling propose Greedy InfoMax (GIM),
an unsupervised method for learning to extract features that trains only one layer at a time.

Key insight: The information bottleneck theory (IB) suggests that neural networks work by
concentrating information like a data-compression algorithm. In data compression,
the amount of information retained is measured in mutual information (MI) between original and compressed versions. 
IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning
as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.


    GIM uses the previous layer’s output as the next layer’s input to train each layer independently. 
    This differs from the usual backpropagation in which all layers learn at once.
    
    The researchers devised a task that teaches layers to extract features that maximize MI.
    Given a subsequence of input data that has been compressed according to the current weights, 
    the layer predicts the next element in the compressed sequence, choosing from a random selection drawn 
    from the input including the correct choice. High success demonstrates that the layer is able to compress the input.
    
    The process effectively removes redundancy between nearby regions of the input. 
    For example, a recording of a song’s chorus may repeat several times, 
    so it’s possible to represent the recording without capturing the repetitions.
    
    
    --- MLPerf
    
        Fair and useful benchmarks for measuring -training and inference- performance of ML hardware, software, and services.

 
 
 -----Black box problem
" One solution is to build machine-learning systems that show their workings: so-called glassbox—as opposed to black-box—AI. 
 Glassbox models are typically much-simplified versions of a neural network in which it is easier to track
 how different pieces of data affect the model. It's a tricky balance, however. 
 Too much transparency can lead to information overload. 

Visualize this: Another approach is to include visualizations that show a few key properties of the model
and its underlying data. However it’s not clear if they really help.

So-called interpretable or explainable AI (XAI) aims to help people understand what features 
in the data a neural network is actually learning. 

In a new study, researchers found that people who used one of two popular interpretability tools didn’t actually 
understand them but trusted them anyway. In fact, use of the tools led to incorrect assumptions
about the data and the models, and instilled a false confidence that made participants more trigger-happy
about model deployment. This phenomenon is known as “automation bias”—and it could be increasing people’s chance of
using AI that’s inaccurate or biased. This is exactly the opposite effect of what XAI is trying to do in the first place.


-  from mit review 
"Facebook has a new way to track the images used to train a model.
The technique involves making imperceptible tweaks to images to create a kind of watermark.
Every model makes its own distinct mark, and the mark doesn’t impair the image classifier’s overall accuracy. 
This allows researchers to more easily match up models with the images that were used to train them 
in large-scale systems with complicated data pipelines.

Facebook calls the technique “radioactive data” 
because it is analogous to the use of radioactive markers in medicine, which show up in the body under x-ray.
Highlighting what data has been used to train an AI model makes them more transparent.
It could help flag potential sources of bias 
or reveal when a data set was used without permission or for inappropriate purposes. " very cool!


--"" New software can now fool the AI behind Alexa and Siri.
Software called TextFooler can now trick natural-language processing (NLP) systems into misunderstanding text 
just by replacing certain words in a sentence with synonyms. 
Developed by a team at MIT, it looks for the words that are most important to an NLP classifier
and replaces them with a synonym that a human would find natural. 
For example, it changes the sentence “The characters, cast in impossibly contrived situations, 
are totally estranged from reality” to “The characters, cast in impossibly engineered circumstances,
are fully estranged from reality.”
We see no difference, but AI interprets the sentences completely differently.


--"" Medication madness. According to the FDA, serious adverse drug interactions are estimated to kill 
more than 100,000 hospitalized people in the US every year.
But avoiding such interactions during drug development is laborious and expensive. 
It involves intensive testing and clinical trials to catalogue all the proposed drug’s possible chemical interactions
with existing ones.

Now a new AI system could make this easier by predicting the interactions
between two drugs based only on their chemical structure. 
The researchers first developed a new way to represent the 3D chemical structures of drugs in a character format
that could be read by a neural network. The drug melatonin, for example, is represented by “CC(=O)NCCC1=CNc2c1cc(OC)cc2.”

They then translated a database of known drug interactions into this format
and used it to train a neural network. The resulting system predicts the probability 
that two drugs will have an adverse interaction and shows the particular parts of the molecule 
that contributed to that prediction. When the researchers tested their system on two common drug interaction data sets,
it performed better than state-of-the-art results. Read more here.


---" Quotable

It’s really snowballed.

—Vincent Cate, who lives on the island of Anguilla, 
which has become an unlikely benefactor of the AI boom because of its domain name “.ai”


---Double descent ---
occurs when a model’s performance changes in unpredictable ways
as the amount of training data or number of parameters crosses a certain threshold. 
The error falls as expected with additional data or parameters, but then rises, drops again, and may take further turns. 
Preetum Nakkiran and
collaborators at Harvard, Stanford, and Microsoft found a way to eliminate double descent in some circumstances.


-
Probabilistic Programming Languages

https://towardsdatascience.com/a-gentle-introduction-to-probabilistic-programming-languages-ba9105d9cbce

"Conceptually, probabilistic programming languages(PPLs) are domain-specific languages
that describe probabilistic models 
and the mechanics to perform inference in those models.
The magic of PPL relies on combining the inference capabilities of probabilistic methods 
with the representational power of programming languages.



--
------->Playing with GANs

from the batch


Generative adversarial networks don’t just produce pretty pictures. They can build world models, too.

What’s new: A GAN generated a fully functional replica of the classic video game Pac-Man. 
Researchers from Nvidia, MIT, the University of Toronto, and Vector Institute developed GameGAN
to celebrate the original Pac-Man's 40th anniversary. 
The company plans to release the code in a few months.

How it works: GameGAN learned to reproduce the game by watching it in action for 50,000 hours. 
During gameplay, the system synthesizes the action frame by frame using three neural networks. 

    An LSTM-style network learned how user actions change the game's state.
    For example, pressing the system's joystick equivalent upward moves the Pac-Man character forward one space.
    
    A network inspired by neural Turing machines allows the system to store information about previously generated frames. 
    In a maze game, retracing your steps should look familiar, and that would be difficult without memory.
    
    Based on the memory, updated game state, and latest user action, GameGAN's generator produces the next frame. 

Behind the news: While Nvidia is the first to use a generative adversarial network to reproduce a video game,
other researchers have used machine learning for this purpose.

    An earlier model from Georgia Tech learns approximate representations of classic titles to create new games.
    The Metacreation Lab at Simon Fraser University is working on models that generate new levels for existing games.
    Researchers from Queen Mary University trained a neural network to duplicate a video game’s underlying mechanics
    by observing pixels.

Yes, but: Compared to the original arcade game, 
Pac-Man’s GAN-driven twin requires orders of magnitude more computation to run.

Why it matters: Autonomous systems such as self-driving cars and robots are often trained in elaborate simulators. 
Nvidia hopes that GAN-based sims can save time and money.

We’re thinking: Fifty thousand hours is an awful lot of Pac-Man — or anything else! 
Simulation makes it possible to amass training data that would be virtually impossible to collect in the real world. 
It's also a crutch that leads researchers to develop algorithms 
that work well in simulated environments but are hard to generalize to real-world conditions. 

Until better small-data algorithms emerge, GAN-based simulation looks like an exciting new direction.




--
One-shot learning

https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d

 There are applications wherein we neither have enough data for each class and the total number classes is huge 
 as well as dynamically changing. 
 Thus, the cost of data collection and periodical re-training is -->  too high.
 
 


--
Siamese networks
https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d

Typically the similarity score is squished between 0 and 1 using a sigmoid function;
wherein 0 denotes no similarity and 1 denotes full similarity. 
Any number between 0 and 1 is interpreted accordingly.

Notice that this network is not learning to classify an image directly to any of the output classes.
Rather, it is learning a similarity function,
which takes two images as input and expresses how similar they are.

Oneshot
But the biggest advantage is that , let’s say in case of face recognition,
we have a new employee who has joined the organization. Now in order for the network to detect his face, 
we only require a single image of his face which will be stored in the database.
Using this as the reference image,
the network will calculate the similarity for any new instance presented to it.
Thus we say that network predicts the score in one shot.

Notice that we will train the system on one set of characters 
and then test it on a completely different set of characters
which were never used during the training. 
This is not possible in a traditional classification cycle.

Mapping the problem to binary classification task
Let’s understand how can we map this problem into a supervised learning task 
where our dataset contains pairs of (Xi, Yi) 
where ‘Xi’ is the input and ‘Yi’ is the output.

Recall that the input to our system will be a pair of images
and the output will be a similarity score between 0 and 1.

Xi = Pair of images

Yi = 1 ; if both images contain the same character

Yi = 0; if both images contain different characters

Thus we need to create pairs of images along with the target variable, as shown above, 
to be fed as input to the Siamese Network.
Note that even though characters from Sanskrit alphabet are shown above, 
but in practice we will generate pairs randomly from all the alphabets in the training data.

Intuition: The term Siamese means twins. 
The two Convolutional Neural Networks shown above are not different networks
but are two copies of the same network, hence the name Siamese Networks.
Basically they share the same parameters. 
The two input images (x1 and x2) are passed through the ConvNet 
to generate a fixed length feature vector for each (h(x1) 
and h(x2)). 
Assuming the neural network model is trained properly, we can make the following hypothesis: 
If the two input images belong to the same character, then their >>feature vectors must also be similar,
while if the two input images belong to the different characters, then their feature vectors will also be different.
Thus the element-wise absolute difference 
between the two feature vectors 
must be very different in both the above cases. 
And hence the similarity score generated by the output sigmoid layer must also be different in these two cases.
This is the central idea behind the Siamese Networks.

Notice that there is no predefined layer in Keras to compute the absolute difference between two tensors.
We do this using the Lambda layer in Keras which is used to add customized layers in Keras.
    # Add a customized layer to compute the absolute difference between the encodings
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])


The model was compiled using the adam optimizer and binary cross entropy loss function.
Learning rate was kept low as it was found that with high learning rate, the model took a lot of time to converge. 
However these parameters can well be tuned further to improve the present settings.

The model was trained for 20,000 iterations with batch size of 32.
After every 200 iterations, model validation was done using 20-way one shot learning 
and the accuracy was calculated over 250 trials.

VAlidate and test
Note that, for every pair of input images, 
our model generates a similarity score between 0 and 1. 
But just looking at the score its difficult to ascertain whether the model is really able to recognize
similar characters and distinguish dissimilar ones.
A nice way to judge the model is >>>>> N-way one shot learning.

Basically the same character is compared to 4 different characters 
out of which only one of them matches the original character. 
Let’s say by doing the above 4 comparisons we get 4 similarity scores S1, S2, S3 and S4 as shown.
Now if the model is trained properly, we expect that S1 is the maximum of all the 4 similarity scores
because the first pair of images is the only one where we have two same characters.

Thus if S1 happens to be the maximum score,
we treat this as a correct prediction otherwise we consider this as an incorrect prediction.
Repeating this procedure ‘k’ times, we can calculate the percentage of correct predictions as follows.


Base Line 1 — Nearest Neighbor Model
It is always a good practice to create simple baseline models and compare their results with the complex model 
you are trying to build.

Conclusion
This is just a first cut solution and many of the hyper parameters can be tuned >>in order to avoid over fitting.
Also more rigorous testing can be done by increasing the value of ’N’ 
in N-way testing and by increasing the number of trials.

--
tf.keras.layers.Lambda
Wraps arbitrary expressions as a Layer object.

Inherits From: Layer


tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs)
    
    in siamese 
    # Add a customized layer to compute the absolute difference between the encodings
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])
    
    
    --
    Lambda functions
    
    Python and other languages like Java, C#, and even C++ have had lambda functions added to their syntax,
    whereas languages like LISP or the ML family of languages, Haskell, OCaml, and F#, use lambdas as a core concept.

>>Python lambdas are little, anonymous functions, 
subject to a more restrictive but more concise syntax than regular Python functions.

By the end of this article, you’ll know:

    How Python lambdas came to be
    How lambdas compare with regular function objects
    How to write lambda functions
    Which functions in the Python standard library leverage lambdas
    When to use or avoid Python lambda functions
    
    History

Alonzo Church formalized lambda calculus,
a language based on pure abstraction, in the 1930s. 
Lambda functions are also referred to as lambda abstractions, 
a direct reference to the abstraction model of Alonzo Church’s original creation.

Lambda calculus can encode any computation. 
It is Turing complete, but contrary to the concept of a Turing machine, it is pure and >>does not keep any state.

Functional languages
get their origin in mathematical logic and lambda calculus,
while imperative programming languages
embrace the state-based model of computation invented by Alan Turing.
The two models of computation, lambda calculus and Turing machines, can be translated into each another.
This equivalence is known as the Church-Turing hypothesis.

Functional languages directly inherit the lambda calculus philosophy,
adopting a declarative approach of programming that emphasizes abstraction,
data transformation, composition, and purity (no state and no side effects).
Examples of functional languages include Haskell, Lisp, or Erlang.

By contrast, the Turing Machine led to imperative programming found in languages like Fortran, C, or Python.

The imperative style consists of programming with statements,
driving the flow of the program step by step with detailed instructions.
This approach promotes mutation and requires managing state.

Python is not inherently a functional language,
but it adopted some functional concepts early on. 
In January 1994, map(), filter(), reduce(), and the lambda operator were added to the language.

The following terms may be used interchangeably depending on the programming language type and culture:

    Anonymous functions
    Lambda functions
    Lambda expressions
    Lambda abstractions
    Lambda form
    Function literals

For the rest of this article after this section, you’ll mostly see the term lambda function.

Taken literally, an anonymous function is a function without a name.
In Python, an anonymous function is created with the lambda keyword. 
More loosely, it may or not be assigned a name.
Consider a two-argument anonymous function defined with lambda but not bound to a variable.
The lambda is not given a name:

>>> lambda x, y: x + y

The function above defines a lambda expression that takes two arguments and returns their sum.
--

Deep Double Descent

OpenAI

https://openai.com/blog/deep-double-descent/

We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves,
then gets worse, and then improves again with increasing model size, data size, or training time. 
This effect is often avoided through careful regularization.
While this behavior appears to be fairly universal, we don’t yet fully understand why it happens,
and view further study of this phenomenon as an important research direction.

Moreover, we show that double descent occurs not just as a function of model size,
but also as a function of the number of training epochs. 

We unify the above phenomena by defining a new complexity measure 
we call the effective model complexity 
and conjecture a generalized double descent with respect to this measure. 

Furthermore, our notion of model complexity allows us to identify certain regimes 
where increasing (even quadrupling) the number of train samples 
actually hurts test performance.



---

Life long ML
Chen and Lei

Lifelong Machine Learning or Lifelong Learning (LL) is an advanced machine learning (ML) paradigm 
that learns continuously, accumulates the knowledge learned in the past,
and uses/adapts it to help future learning and problem solving. 
In the process, the learner becomes more and more knowledgeable and better and better at learning.



__
Semi supervised learning

FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence

Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel

    Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data
    to improve a model's performance.
    In this paper, we demonstrate the power of a simple combination of two common SSL methods:
    consistency regularization and pseudo-labeling.
    
    Our algorithm, FixMatch, first generates pseudo-labels
    using the model's predictions on weakly-augmented unlabeled images.
    
    >>For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction.
    The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image.
    Despite its simplicity, we show that FixMatch achieves state-of-the-art performance 
    across a variety of standard semi-supervised learning benchmarks,
    including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class.
    
    Since FixMatch bears many similarities to existing SSL methods 
    that achieve worse performance, we carry out an extensive ablation study
    to tease apart the experimental factors that are most important to FixMatch's success.
    We make our code available at this https URL. 



---

Q Learning


Q-learning is an off policy reinforcement learning algorithm 
that seeks to find the best action to take given the current state. 

It’s considered off-policy because the q-learning function learns from actions that are outside the current policy,
like taking random actions, and therefore a policy isn’t needed. 

More specifically, q-learning seeks to learn a policy that maximizes the total reward.

The ‘q’ in q-learning stands for quality. 
Quality in this case represents how useful a given action is in gaining some future reward.

Create a q-table
When q-learning is performed we create what’s called a q-table or matrix 
that follows the shape of [state, action] and we initialize our values to zero.
We then update and store our q-values after an episode. 
This q-table becomes a reference table for our agent to select the best action based on the q-value.




--
Graph Deep Learning (GDL) 
Graph Neural Network (GNN)
https://medium.com/dair-ai/an-illustrated-guide-to-graph-neural-networks-d5564a551783

The black arrows on the edges represent the kind of relationship between the nodes.
It shows whether a relationship is mutual or one-sided.
The two different kinds of graphs are 
---directed (connection direction matters between nodes)
----- undirected (connection order doesn’t matter). 
Directed graphs can be unidirectional or bidirectional in nature.

A graph can represent many things — social media networks, molecules, etc.
Nodes can be thought of as users/products/atoms 
while the edges represent connections (following/usually-purchased-with/bonds). 
A social media graph may look like this with nodes as users and edges as connections

Each node has a set of features defining it. 
In the case of social network graphs, this could be age, gender, country of residence, political leaning, and so on. 
Each edge may connect nodes together that have similar features. (yo/ necessarily similar? nah, otras relaciones tmb)
It shows some kind of interaction or relationship between them.

Message Passing
Once the conversion of nodes and edges are completed,
the graph performs Message Passing between the nodes. 
This process is also called Neighbourhood Aggregation 
because it involves pushing messages (aka, the embeddings) 
from surrounding nodes around a given reference node, through the directed edges. (feed forward nns)

Once you perform the Neighbourhood Aggregation/Message Passing procedure a few times,
you obtain a completely new set of embeddings for each nodal recurrent unit.

Through the timesteps/rounds of Message Passing, 
the nodes know more about their own information (features) and that of neighbouring nodes. 
This creates an even more accurate representation of the entire graph.

For further processing in higher layers of a pipeline, or simply to represent the graph,
you can take all the embeddings and sum them up together
to get vector H that represents the whole graph.

((To summarise this step, we sum together the final vector representations
of all nodal recurrent units (order-invariant,of course)
use this resulting vector as inputs to other pipelines
or to simply represent the graph.))

GNNs are fairly simple to use. In fact, implementing them involved four steps.

    1.Given a graph, we first convert the nodes to recurrent units
    and the edges to feed-forward neural networks.
    2.Then we perform Neighbourhood Aggregation (Message Passing, if that sounds better) 
    for all nodes n number of times.
    3.Then we sum over the embedding vectors of all nodes 
    to get graph representation H.
    4.Feel free to pass H into higher layers
    or use it to represent the graph’s unique properties!

Why Graph Neural Networks?
Now that we know how Graph Neural Networks work, why would we want to apply/use them?
In the case of social media graphs, GNNs are great at content recommendation. 
When a user follows other users with a similar taste in political leaning (for example), 
GNNs can be used for node classification
to predict if a certain piece of content on the site 
can be sent to the news feed of said user.

When suggesting “who to follow”, systems can take into account the industry of the user 
and provide potential connections — edge classification.

There are also great resources to learn about GDL algorithms 
and different ways to capture lots of sequential and spatial aspects from graph representations.

https://github.com/rusty1s/pytorch_geometric
https://pytorch-geometric.readthedocs.io/
https://www.dgl.ai/
http://geometricdeeplearning.com/

Geometric Learning
In the last decade, Deep Learning approaches (e.g. Convolutional Neural Networks and Recurrent Neural Networks)
allowed to achieve unprecedented performance on a broad range of problems
coming from a variety of different fields (e.g. Computer Vision and Speech Recognition).

Despite the results obtained, 
research on DL techniques has mainly focused so far on data defined on Euclidean domains (i.e. grids).

Nonetheless, in a multitude of different fields, such as: Biology, Physics, Network Science,
Recommender Systems and Computer Graphics; 
one may have to deal with data defined on non-Euclidean domains (i.e. graphs and manifolds).

The adoption of Deep Learning in these particular fields has been lagging behind until very recently,
primarily since the non-Euclidean nature of data 
makes the definition of basic operations (such as convolution) rather elusive.

Geometric Deep Learning deals in this sense 
with the extension of Deep Learning techniques to graph/manifold structured data.


--
A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction

We present a novel theory of Fermat paths of light
between a known visible scene and an unknown object not in the line of sight of a transient camera. 

These light paths either obey specular reflection or are reflected by the object’s boundary,
and hence encode the shape of the hidden object.

We prove that Fermat paths correspond to discontinuitiesin the transient measurements. 
We then derive a novel constraint 
that relates the spatial derivatives of the path lengths at these discontinuities to the surface normal.

Based on this theory, we present an algorithm, called Fermat Flow, 
to es-timate the shape of the non-line-of-sight object.

Our method allows, for the first time, accurate shape recovery of com-plex objects,
ranging from diffuse to specular, that are hid-den around the corner as well as hidden behind a diffuser.




--

Pruning 

Michela Paganini, Postdoctoral Researcher at Facebook AI, 
shares her personal experience creating a core PyTorch feature: Pruning (torch.nn.utils.prune). 

State-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. 
On the contrary, biological neural networks are known to use efficient sparse connectivity. 

Identifying optimal techniques to compress models by reducing the number of parameters in them is important
in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, 
deploy lightweight models on device, and guarantee privacy with private on-device computation. 

On the research front, pruning is used to investigate the differences in learning dynamics between over-parametrized and under-parametrized networks, 
to study the role of lucky sparse subnetworks and initializations (“lottery tickets”) as a destructive neural architecture search technique, and more.




--
https://gizmodo.com/this-algorithm-might-make-facial-recognition-obsolete-1844591686

 By swapping out or distorting some of these pixels,
 the face might still be recognizable to you or me, 
 but it would register as an entirely different person to just about every popular facial recognition algo. 
 
 According to the team’s research, this “cloaking” technique managed to fool the facial recognition systems
 peddled by Microsoft, Amazon, and Google 100% of the time.

If you want to give this algo a whirl yourself, the good news is that the U. Chicago team has the Fawkes program 
freely available for download on their website. If you have a picture you want to protect from snoopers or scrapers, 
you can load them into Fawkes, which then jumbles those unseen pixels in about 40 seconds per photograph, according to the researchers. 


---

A critical analysis of self-supervision, or what we can learn from a single image
Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi

    We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels.
    We show that three different and representative methods, BiGAN, RotNet and DeepCluster,
    can learn the first few layers of a convolutional network from a single image 
    as well as using millions of images and manual labels, provided that strong data augmentation is used. 
    
    However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.





----
Efficient Nets


Image Classification with EfficientNet: -----> Better performance with computational efficiency
Anand Borad
https://medium.com/analytics-vidhya/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6

In May 2019, two engineers from Google brain team named Mingxing Tan and Quoc V. Le 
published a paper called “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”. 
The core idea of publication was about strategically scaling deep neural networks 
but it also introduced a new family of neural nets, EfficientNets.

EfficientNets, as the name suggests are very much efficient computationally
and also achieved state of art result on ImageNet dataset which is 84.4% top-1 accuracy.

So, in this article, we will discuss EfficientNets in detail but first, we will talk about the core idea introduced in the paper, 
-->model scaling.

Model scaling is about scaling the existing model in terms of model depth, model width, and less popular input image resolution
to improve the performance of the model. 
Depth wise scaling is most popular amongst all, e.g. ResNet can be scaled from Resnet18 to ResNet200. 
Here ResNet10 has 18 residual blocks and can be scaled for depth to have 200 residual blocks.

ResNet200 delivers better performance than ResNet18 and thus, manually scaling works pretty well. 
But there is one problem with traditional manual scaling method, after a certain level, scaling doesn’t improve performance.
It starts to affect adversely by degrading performance.

The scaling method introduced in paper is named -->compound scaling 
and suggests that instead of scaling only one model attribute out of depth, width, and resolution; 
-->strategically scaling all three of them together delivers better results.

Compound scaling
--->Compound scaling method uses a compound co-efficient ø to scale width, depth, and resolution together.




----

Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection
Debidatta Dwibedi, Ishan Misra, Martial Hebert
https://arxiv.org/abs/1708.01642

    A major impediment in rapidly deploying object detection models for instance detection
    is the lack of large annotated datasets.
    
    For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely.
    Each new environment with new instances requires expensive data collection and annotation. 
    
    In this paper, 
    >>we propose a simple approach to generate large annotated instance datasets with minimal effort. 
    
    Our key insight is that ensuring only patch-level realism 
    provides enough training signal for current object detector models. 
    We automatically `cut' object instances and `paste' them on random backgrounds. 
    
    A naive way to do this results in pixel artifacts which result in poor performance for trained models.
    We show how to make detectors ignore these artifacts during training 
    and generate data that gives competitive performance on real data. 
    
    Our method outperforms existing synthesis approaches
    and when combined with real images 
    improves relative performance by more than 21% on benchmark datasets.
    
    In a cross-domain setting, our synthetic data combined with just 10% real data 
    outperforms models trained on all real data. 


-----------

Frustratingly Simple Few-Shot Object Detection

https://arxiv.org/pdf/2003.06957.pdf

Abstract
Detecting rare objects from a few examples is an emerging problem.  
Prior works show meta-learning  is  a  promising  approach.  
But,  fine-tuning techniques have drawn scant attention. 

We find  that  fine-tuning  only  the  last  layer  of  existing detectors on rare classes is crucial 
to the few-shot object detection task.   

Such a simple approach  outperforms  the  meta-learning  methods
by roughly 2∼20 points on current benchmarks and sometimes even doubles the accurac yof the prior methods.  

However,  the high variance in the few samples 
often leads to the unreliability  of  existing  benchmarks.

We  revise the  evaluation  protocols  
by  sampling  multiple groups of training examples
to obtain stable comparisons  
and  build  new  benchmarks  based  on three datasets: PASCAL VOC, COCO and LVIS.

Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks.
The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.


--------

One-bit Supervision for Image Classification
https://arxiv.org/pdf/2009.06168.pdf

Abstract
This paper presents one-bit supervision, a novel setting of learning from incomplete annotations, in the scenario of image classification.
Instead of training a model upon the accurate label of each sample, 
our setting requires the model to query with a predicted label of each sample 
and learn from the answer whether the guess is correct. 

This provides one bit (yes or no) of information, and more importantly,
annotating each sample becomes much easier than finding the accurate label from many candidate classes.  

There are two keys to training a model upon one-bit supervision: 
improving the guess accuracy 
and making use of incorrect guesses.

For these purposes, we propose a multi-stage training paradigm 
which incorporates negative label suppression into an off-the-shelf semi-supervised learning algorithm. 
In three popular image classification benchmarks, our approach claims higher efficiency in utilizing the limited amount of annotations.


------

Relational inductive biases, deep learning, and graph networks   (combinatorial generalization)

https://arxiv.org/pdf/1806.01261.pdf

The  following  is  part  position  paper,  part  review,  and  part  unification. 
We  argue  that combinatorial generalization 
must be a top priority for AI to achieve human-like abilities,
and that structured representations and computations are key to realizing this objective. 

Just as biology uses nature and nurture cooperatively,
we reject the false choice between “hand-engineering”and “end-to-end” learning, 
and instead advocate for an approach which benefits from their complementary strengths. 

We explore how using relational inductive biases 
within deep learning architectures 
can facilitate learning about entities, relations, and rules for composing them.  

We present a new building block for the AI toolkit with a strong relational inductive bias
—the graph network—
which generalizes and extends various approaches for neural networks that operate on graphs,
and provides a straightforward interface 
for manipulating structured knowledge and producing structured behaviors.  

We discuss how graph networks can support relational reasoning and combinatorial generalization, 
laying the foundation for more sophisticated, interpretable,and flexible patterns of reasoning.  

As a companion to this paper,  we have also released anopen-source software library for building graph networks, 
with demonstrations of how to use them in practice.




---------------------

DNA computing

https://interestingengineering.com/what-is-dna-computing-how-does-it-work-and-why-its-such-a-big-deal

The reason this generated excitement was that DNA structures are cheap, relatively easy to produce, and scalable. 
There is no limit to the power that DNA computing can theoretically have 
since its power increases the more molecules you add to the equation 
and unlike silicon transistors which can perform a single logical operation at a time,
these DNA structures can theoretically perform as many calculations at a time as needed to solve a problem 
and do it all at once.

The problem however, is speed.
Even though it took moments for Adleman’s solution to the traveling salesman problem to be encoded into his DNA strands in the test tube,
it took days of filtering out bad solutions to find the optimal solution he was looking for
—after meticulous preparation for this single computation.

Still, the concept was a sound one and the potential for incredible gains 
in storage capacity
and computational speeds was obvious.
This kicked off two decades of research into how to create practical DNA computing a reality.


As demonstrated with Adleman’s paper (1994), 
the major advantage of DNA computing over classical computing—and even quantum computing to an extent—
>>>>>> is that it can perform countless calculations in parallel. 
This idea of parallel computing isn’t new and has been mimicked in classical computing for decades.









