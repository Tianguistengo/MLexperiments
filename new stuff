
- andrew ng "Better Than Backprop

End-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. 
However, recent work suggests that neither is necessary to train effective neural networks to represent complex data. 
What’s new: Sindy Löwe, Peter O’Connor, and Bastiaan Veeling propose Greedy InfoMax (GIM),
an unsupervised method for learning to extract features that trains only one layer at a time.

Key insight: The information bottleneck theory (IB) suggests that neural networks work by
concentrating information like a data-compression algorithm. In data compression,
the amount of information retained is measured in mutual information (MI) between original and compressed versions. 
IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning
as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.


    GIM uses the previous layer’s output as the next layer’s input to train each layer independently. 
    This differs from the usual backpropagation in which all layers learn at once.
    
    The researchers devised a task that teaches layers to extract features that maximize MI.
    Given a subsequence of input data that has been compressed according to the current weights, 
    the layer predicts the next element in the compressed sequence, choosing from a random selection drawn 
    from the input including the correct choice. High success demonstrates that the layer is able to compress the input.
    
    The process effectively removes redundancy between nearby regions of the input. 
    For example, a recording of a song’s chorus may repeat several times, 
    so it’s possible to represent the recording without capturing the repetitions.
    
    
    --- MLPerf
    
        Fair and useful benchmarks for measuring -training and inference- performance of ML hardware, software, and services.

 
 
 -----Black box problem
" One solution is to build machine-learning systems that show their workings: so-called glassbox—as opposed to black-box—AI. 
 Glassbox models are typically much-simplified versions of a neural network in which it is easier to track
 how different pieces of data affect the model. It's a tricky balance, however. 
 Too much transparency can lead to information overload. 

Visualize this: Another approach is to include visualizations that show a few key properties of the model
and its underlying data. However it’s not clear if they really help.

So-called interpretable or explainable AI (XAI) aims to help people understand what features 
in the data a neural network is actually learning. 

In a new study, researchers found that people who used one of two popular interpretability tools didn’t actually 
understand them but trusted them anyway. In fact, use of the tools led to incorrect assumptions
about the data and the models, and instilled a false confidence that made participants more trigger-happy
about model deployment. This phenomenon is known as “automation bias”—and it could be increasing people’s chance of
using AI that’s inaccurate or biased. This is exactly the opposite effect of what XAI is trying to do in the first place. 
