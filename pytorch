----------->   Pytorch

bibliography
https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec
deeplizard.com 
https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html


What makes it really luring is it’s ---> dynamic computation graph paradigm. <---
Don’t worry if the last line doesn’t make sense to you now.
But take my word that it makes --> debugging neural networks way easier. <--

>>>>PyTorch is easier to learn and lighter to work with, and hence,
is relatively better for passion projects and building rapid prototypes.

TensorFlow has adopted PyTorch innovations and PyTorch has adopted TensorFlow innovations. 
>>>>Notably, now both languages can run in a >>> dynamic eager execution mode >>>>or a static graph mode.
Both frameworks are open source, but PyTorch is Facebook's baby and TensorFlow is Google's baby.

In the recent NerulIPS conference,
PyTorch was in 166 papers and TensorFlow was in 74. 
PyTorch went from being in fewer papers than TensorFlow in 2018 to more than doubling TensorFlow’s number in 2019.

TensorFlow is still mentioned in many more >>>job listings that PyTorch, but the gap is closing.
PyTorch has taken the lead in usage in >>>>research papers at top conferences 
and almost closed the gap in Google search results. 
TensorFlow remains three times more common in usage according to the most recent Stack Overflow Developer Survey.

" On 1st October 2019, the first stable version of TF2.0 was released.
The new features include tight integration with Keras,
Eager Execution as default (finally 😅), 
functions and not sessions, 
Multi-GPU support and much more. 
>>>Another beautiful feature is tf.function decorator, 
>>>that converts a block of code into optimized graphs thus providing faster execution.

(pro tf)Comparing tensorflow to pytorch is like comparing a car to an engine block.
It tricks you into just comparing the engine parts and not even considering the stuff around it.

At their core, PyTorch and Tensorflow are >>>auto-differentiation frameworks. 
That is, they >>allow one to take the derivative of some function.
However, there are many ways to enable auto-differentiation, 
and the particular implementation that most modern ML frameworks choose is called “reverse-mode auto-differentiation”,
more commonly known as “backpropagation”. 
This implementation turns out to be extremely efficient for taking the derivative of neural networks.

However, things change for computing higher order derivatives (Hessian/Hessian Vector Products).




The reason why we use --> Numpy <-- is because it’s much faster than Python lists at doing matrix ops.
Why? Because it does most of the heavy lifting in C.

But, in case of training deep neural networks, NumPy arrays simply don’t cut it.
I’m too lazy to do the actual calculations here (google for “FLOPS in one iteration of ResNet to get an idea), 
but code utilising NumPy arrays alone would take months to train some of the state of the art networks.

This is where --Tensors-- come into play.
PyTorch provides us with a data structure called a Tensor, 
which is very similar to NumPy’s ndarray. 
But unlike the latter, tensors can tap into the resources of a GPU to significantly speed up matrix operations.



(  The N-dimensional array (ndarray)  )

An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size. 
The number of dimensions and items in an array is defined by its shape, which is a tuple of N non-negative integers 
that specify the sizes of each dimension. 
The type of items in the array is specified by a separate data-type object (dtype),
one of which is associated with each ndarray.

As with other container objects in Python, 
the contents of an ndarray can be accessed and modified by indexing or slicing the array 
(using, for example, N integers), and via the methods and attributes of the ndarray.

Different ndarrays can share the same data, 
so that changes made in one ndarray may be visible in another. 
That is, an ndarray can be a “view” to another ndarray, 
and the data it is referring to is taken care of by the “base” ndarray.
ndarrays can also be views to memory owned by Python strings or objects implementing the buffer or array interfaces.



(( The chain rule  )) 
The chain rule is conceptually a divide and conquer strategy  that breaks complicated expressions into SUBEXPRESSIONS
whose DERIVATIVES are easier to compute. 
Its power derives from the fact that we can process each simple subexpression in ISOLATION
yet still combine the INTERMEDIATE RESULTS to get the correct overall result.

The chain rule comes into play when we need the derivative of an expression composed of nested subexpressions. 





BUILDING BLOCK 2: COMPUTATION GRAPHS

 When a neural network is trained, we need to compute gradients of the loss function,
 with respect to EVERY weight and bias,
 and then update these weights using gradient descent.

With neural networks hitting billions of weights, doing the above step efficiently can make or break
the feasibility of training.
Building Block #2.1: Computation Graphs

Computation graphs lie at the heart of the way modern deep learning networks work, and PyTorch is no exception. 

>>>>>  The computation graph is simply a data structure that allows you to efficiently apply the chain rule
to compute gradients for all of your parameters.<<


BUILDING BLOCK #3 : Variables and Autograd

PyTorch accomplishes what we described above using the Autograd package.

Now, there are basically three important things to understand about how Autograd works.

#3.1 : Variable

The Variable, just like a Tensor is a CLASS that is used to hold data.
It differs, however, in the way it’s meant to be used.
--> Variables are specifically tailored to hold values which change during training of a neural network, <---
i.e. the learnable paramaters of our network.
Tensors on the other hand are used to store values that are not to be learned. 
For example, a Tensor maybe used to store the values of the loss generated by each example.

A Variable class wraps a tensor. 
You can access this tensor by calling .data attribute of a Variable.

The Variable also stores the gradient of a scalar quantity (say, loss) with respect to the parameter it holds. 
This gradient can be accessed by calling the .grad attribute.
This is basically the gradient computed up to this particular node,
and the gradient of the every subsequent node, can be computed by multiplying the edge weight 
with the gradient computed at the node just before it.


-
Tensors 

PyTorch Tensors Explained - Neural Network Programming - deeplizard

"
 PyTorch tensors are instances of the >>>>  torch.Tensor Python class. 
 We can create a torch.Tensor object >> using the class constructor like so:

> t = torch.Tensor()
> type(t)
torch.Tensor

This creates an empty tensor (tensor with no data), but we'll get to adding data in just a moment.

Tensor attributes:

First, let’s look at a few tensor attributes. Every torch.Tensor has these attributes:

  >  torch.dtype
  >  torch.device
  >  torch.layout

Looking at our Tensor t, we can see the following default attribute values:

> print(t.dtype)
> print(t.device)
> print(t.layout)
torch.float32
cpu
torch.strided



Tensors have a >>>>torch.dtype

The dtype, which is torch.float32 in our case, 
specifies the type of the data that is contained within the tensor. 
Tensors contain uniform (of the same type) numerical data with one of these types: 


Data type 	            dtype 	        CPU tensor 	       GPU tensor
32-bit floating point 	torch.float32 	torch.FloatTensor 	torch.cuda.FloatTensor
64-bit floating point 	torch.float64 	torch.DoubleTensor torch.cuda.DoubleTensor
16-bit floating point 	torch.float16 	torch.HalfTensor 	 torch.cuda.HalfTensor
8-bit integer (unsigned) torch.uint8 	torch.ByteTensor 	 torch.cuda.ByteTensor
8-bit integer (signed)  torch.int8 	  torch.CharTensor 	 torch.cuda.CharTensor
16-bit integer (signed) 	torch.int16 	torch.ShortTensor 	torch.cuda.ShortTensor
32-bit integer (signed) 	torch.int32 	torch.IntTensor 	  torch.cuda.IntTensor
64-bit integer (signed) 	torch.int64 	torch.LongTensor  	torch.cuda.LongTensor


 >>> Tensors have a torch.device

The device, cpu in our case, specifies the device (CPU or GPU) 
where the tensor's data is >> allocated.
This determines where tensor computations for the given tensor will be performed.

PyTorch supports the use of multiple devices, and they are specified using an index like so:

> device = torch.device('cuda:0')
> device
device(type='cuda', index=0)


If we have a device like the above, we can create a tensor on the device
by passing the device to the tensor’s constructor. 

One thing to keep in mind about using multiple devices
> is that tensor operations between tensors must happen between tensors that exists on the same device.

Using multiple devices is typically something we will do as we become more advanced users



>>  Tensors have a torch.layout

The layout, strided in our case, specifies >> how the tensor is stored in memory.
To learn more about stride check here.


As neural network programmers, we need to be aware of the following:

 >>   Tensors contain data of a uniform type (dtype).
 >>   Tensor computations between tensors depend on the .dtype and the .device 
 (tensors have to have same dtype and be on the same device).

Let’s look now at the common ways of creating tensors using data in PyTorch.
Creating tensors using data

These are the primary ways of >>>  creating tensor objects (instances of the torch.Tensor class),
with data (array-like) in PyTorch:
(four ways to create a torch.Tensor object:)

    torch.Tensor(data)
    torch.tensor(data)
    torch.as_tensor(data)
    torch.from_numpy(data)

Let’s look at each of these.
>> They all accept some form of data 
and
give us an instance of the torch.Tensor class.

torch.Tensor() and torch.tensor() --copy-- their input data 
while torch.as_tensor() and torch.from_numpy() --share-- their input data in memory with the original input object. 

Share Data 	          Copy Data
torch.as_tensor() 	   torch.tensor()
torch.from_numpy() 	   torch.Tensor()

This sharing just means that the --actual data in memory-- exists in a single place.
As a result, any changes that occur in the underlying data >> will be reflected in both objects,
the torch.Tensor and the numpy.ndarray.

>Sharing data is more efficient and uses less memory than copying data 
because the data is not written to two locations in memory.

If we have a torch.Tensor and we want to convert it to a numpy.ndarray, we do it like so:

> print(o3.numpy())
> print(o4.numpy())
[0 2 3]
[0 2 3]

 This establishes that torch.as_tensor() and torch.from_numpy() both >>share memory with their input data.
 However, which one should we use, and how are they different?

The torch.from_numpy() function only accepts numpy.ndarrays, 
while the torch.as_tensor() function accepts a wide variety of array-like objects 
including other PyTorch tensors.

For this reason, >>>   torch.as_tensor() is the winning choice in the memory sharing game.

Best options for creating tensors in PyTorch
Given all of these details, these two are the best options:

    torch.tensor()
    torch.as_tensor()

 ->>  torch.tensor() call is the sort of --go-to-- call, 
while torch.as_tensor() should be employed when tuning our code for performance. 


torch.tensor(d) copies d      and torch.as_tensor(d) shares memory with d.


 Some things to keep in mind about memory sharing (it works where it can):

    Since >numpy.ndarray objects are allocated on the CPU,
    the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used.
    
    The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists.
    
    The as_tensor() call requires developer knowledge of the sharing feature.
    This is necessary so we don’t inadvertently make an unwanted change 
    in the underlying data without realizing the change impacts multiple objects.
    
    The as_tensor() performance improvement will be greater
    if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. 
    However, if there is just a single load operation, 
    there shouldn’t be much impact from a performance perspective.


Wrapping up

At this point, we should now have a better understanding of
the  > PyTorch tensor creation options. 

We’ve learned about factory functions 
and we’ve seen how memory sharing vs copying can impact performance and program behavior. 


type inference 
is when the .tensor / .as_tensor / .from_numpy  guess what dtype the input data is.
Note that the dtype > can also be explicitly set for these calls by specifying the dtype as an argument:
(a configuration option that the constructor (Tensor, capital T) does not have.

> torch.tensor(data, dtype=torch.float32)
> torch.as_tensor(data, dtype=torch.float32)




 Now, let’s create our tensors with each of these options 1-4, and have a look at what we get:

> o1 = torch.Tensor(data)
> o2 = torch.tensor(data)
> o3 = torch.as_tensor(data)
> o4 = torch(data)

> print(o1)
> print(o2)
> print(o3)
> print(o4)
tensor([1., 2., 3.])
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)
tensor([1, 2, 3], dtype=torch.int32)

All of the options (o1, o2, o3, o4) 
appear to have produced the same tensors except for the first one.
The first option (o1) has dots after the number indicating that the numbers are floats,
while the next three options have a type of int32. 

 Uppercase/lowercase: torch.Tensor() vs torch.tensor()

Notice how the first option torch.Tensor() has an uppercase T 
while the second option torch.tensor() has a lowercase t. 
What’s up with this difference?

The first option with the uppercase T is the --constructor-- of the torch.Tensor class,
and the second option is what we call a --factory function--
that constructs torch.Tensor objects 
and returns them to the caller. 

You can think of the torch.tensor() function
as a factory that builds tensors given some parameter inputs.

Factory functions are a software design pattern >>for creating objects. 

But which way is better between these two? 
The answer is that ----> it’s fine to use either one. 
However, the factory function torch.tensor() has better documentation and more configuration options, 
so it gets the winning spot at the moment. 


torch.Tensor() and torch.tensor() >  --copy-- their input data 
while torch.as_tensor() and torch.from_numpy() >  --share-- their input data in memory with the original input object. 


  > Creation options   WITHOUT DATA

Here are some other creation options that are available.

>We have the torch.eye() function
which returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
The name eye() is connected to the idea of an identity matrix ,
which is a square matrix with ones on the main diagonal and zeros everywhere else.

 print(torch.eye(2))
tensor([
    [1., 0.],
    [0., 1.]
])


>We have the torch.zeros() function 
that creates a tensor of zeros with the shape of specified shape argument. 

 This task is a breeze if we are using numpy.ndarrays,
 so congratulations if you are already familiar with NumPy. 

print(torch.zeros([2,2]))
tensor([
    [0., 0.],
    [0., 0.]
])

Similarly, we have the torch.ones() function that creates a tensor of ones.

> print(torch.ones([2,2]))
tensor([
    [1., 1.],
    [1., 1.]
])

 We also have the torch.rand() function that creates a tensor with a shape of the specified argument whose values are random.

> print(torch.rand([2,2]))
tensor([
    [0.0465, 0.4557],
    [0.6596, 0.0941]
])

This is a small subset of the available creation functions that don’t require data. Check with the PyTorch documentation for the full list. 



In PyTorch, we transfer a tensor t to a GPU with:
t.cuda()




PyTorch tensors as we have seen are ---instances--- of the > torch.Tensor  PyTorch class. 
The difference between the abstract concept of a tensor and a PyTorch tensor 
is that PyTorch tensors give us a concrete implementation 
that we can work with in code.



 GPU can be slower than CPU

We said that we can selectively run our computations on the GPU or the CPU,
but why not just run every computation on the GPU?
Isn’t a GPU faster than a CPU?

>>> The answer is that a GPU is only faster for particular (specialized) tasks. 
One issue that we can run into is bottlenecks that slow our performance. 

For example, moving data from the CPU to the GPU is costly, 
so in this case, the overall performance might be slower >>  if the computation task is a simple one. 

Remember, the GPU works well for >> tasks that can be broken into many smaller tasks,
and if a compute task is already small, we won’t have much to gain by moving the task to the GPU.




""In pytorch, you are required to know the input and output sizes of each of the layers,
but this is one of the easier aspects which one can get the hang of quite quickly. 
You don’t have to deal with building an abstract computational graph 
which you can’t see inside of for debugging.

Another plus for Pytorch is the smoothness in which you can go back and forth
between Torch Tensors and Numpy arrays. 
If you need to implement something custom, then going back and forth between TF tensors and Numpy arrays can be a pain, 
requiring the developer to have a solid understanding of TensorFlow sessions.

Of course, if you never have to implement anything fancy, then Keras will do just fine
as you won’t run into any TensorFlow road blocks.
But if you do, then Pytorch will probably be a lot smoother of a ride.



pytorch
"
3) Easy to debug networks. Any time you find any problem with the network,
just use something like print("avg_pool", avg_pool.size()) in the forward pass 
to check the sizes of the layer and you will debug the network easily.

4) You can return multiple outputs from the forward layer.
This is pretty helpful in the Encoder-Decoder architecture 
where you can return both the encoder and decoder output. 
Or in the case of autoencoder where you can 
return the output of the model and the hidden layer embedding for the data.

5) Pytorch tensors work in a very similar manner to numpy arrays. 
For example, I could have used Pytorch Maxpool function to write the
maxpool layer but
max_pool, _ = torch.max(h_gru, 1) 
will also work.

6) You can set up different layers with different initialization schemes.
Something you won’t be able to do in Keras. 
For example, in the below network I have changed the initialization scheme of my LSTM layer. 
The LSTM layer has different initializations for biases, input layer weights, and hidden layer weights.



Ignite vs Lightning 

I really like PyTorch and I'm not alone. However, there is one aspect where PyTorch is not too user-friendly.
PyTorch does not have a nice high-level fit function, i.e. a fit interface like scikit-learn or keras.
That is the complaint I hear most often about PyTorch.

At a base level, lightning implements distributed GPU, multi-node, 16-bit,
auto-checkpoint and resubmit for cluster training. 
>>The learning curve with lightning is also lower since it doesn’t make unnecessary wrappings.

Further, lightning is also very thoroughly documented and well tested as is ignite.

Basically, if you want to retain 99% of the control and are ok not having to deal with your own
training loop, checkpointing, distributing, etc... then lighting is for you.
It’s designed to give academics the most control without oversimplifying
or making a whole new library someone has to learn.


Lightning

This is where lightning differs though. >> In PyTorch, you write the for loop yourself 
which means you have to remember to call the correct things in the right order 
— this leaves a lot of room for bugs.

The Lightning Trainer
The trainer is how we abstract the boilerplate code.


# The dataloaders handle shuffling, batching, etc...
mnist_train = DataLoader(mnist_train, batch_size=64)

Full Training loop in Lightning

The lightning version is EXACTLY the same except:

    The core ingredients have been organized by the LightningModule
    The training/validation loop code has been abstracted by the Trainer


Highlights

Let’s call out a few key points

    Without Lightning, the PyTorch code is allowed to be in arbitrary parts. With Lightning, this is structured.
    It is the same exact code for both except that it’s structured in Lightning. (worth saying twice lol).
    As the project grows in complexity, your code won’t because Lightning abstracts out most of it.
    You retain the flexibility of PyTorch because you have full control over the key points in training. For instance, you could have an arbitrarily complex training_step such as a seq2seq

- bonus feature
tensorboard logs (yup! you had to do nothing to get this)
{Log using 5 other alternatives to Tensorboard}


-TensorBoard 
is a tool for providing the measurements and visualizations needed during the machine learning workflow.
It enables tracking experiment metrics 
like loss and accuracy, visualizing the model graph, 
projecting embeddings to a lower dimensional space, and much more.

-
Tensor operations:


    Reshaping operations
    Element-wise operations
    Reduction operations
    Access operations

Reshaping operations--- gave us the ability to position our elements along particular axes.

Element-wise operations ----allow us to perform operations on elements BETWEEN two tensors. 

Reduction operations ------allow us to perform operations on elements WITHIN a single tensor. 

Access operations ---  the ability to ACCESS data from WITHIN the tensor. 


 Common tensor reduction operations

> t.sum()
tensor(8.)

> t.prod()
tensor(0.)

> t.mean()
tensor(.8889)

> t.std()
tensor(1.1667)


When we call the argmax() method on a tensor,
the tensor is reduced to a new tensor that contains an index value
indicating where the max value is inside the tensor.

In practice, we often use the argmax() function on a network’s output prediction tensor,
to determine which category has the highest prediction value. 
(and this is because each index of the output tensor
typically corresponds to a particular prediction category/class)


--
 To prepare our data, 
 we'll be following what is loosely known as an ETL process.

    Extract data from a data source.
    Transform data into a desirable format.
    Load data into a suitable structure.


    Extract –     Get the Fashion-MNIST image data from the source.
    Transform –   Put our data into tensor form.
    Load –        Put our data into an object to make it easily accessible.

 For these purposes, PyTorch provides us with two classes:
 
Class 	                      Description

torch.utils.data.Dataset 	   An abstract class for representing a dataset.

torch.utils.data.DataLoader 	Wraps a dataset and provides access to the underlying data. 


 PyTorch Dataset class

To get an instance of the FashionMNIST dataset using torchvision, we just create one like so:

train_set = torchvision.datasets.FashionMNIST(
    root='./data'
    ,train=True
    ,download=True
    ,transform=transforms.Compose([
        transforms.ToTensor()
    ])
)

Note that the root argument used to be './data/FashionMNIST', however, it has since changed due to torchvision updates.

We specify the following arguments:
Parameter 	Description
root 	     The location on disk where the data is located.
train     	If the dataset is the training set
download  	If the data should be downloaded.
transform 	A composition of transformations that should be performed on the dataset elements.

Since we want our images to be transformed into tensors, 
we use the built-in transforms.ToTensor() transformation, 
and since this dataset is going to be used for training, we’ll name the instance train_set.

When we run this code for the first time, the Fashion-MNIST dataset will be downloaded locally. 
Subsequent calls check for the data before downloading it. 
Thus, we don't have to worry about double downloads or repeated network calls.
PyTorch DataLoader class

To create a DataLoader wrapper for our training set, we do it like this:

train_loader = torch.utils.data.DataLoader(train_set
    ,batch_size=1000
    ,shuffle=True
)

We just pass train_set as an argument.
Now, we can leverage the loader for tasks that would otherwise be pretty complicated to implement by hand:

    batch_size (1000 in our case)
    shuffle (True in our case)
    num_workers (Default is 0 which means the main process will be used)


--
 The torchvision package, 
 gives us access to the following resources:

    Datasets (like MNIST and Fashion-MNIST)
    Models (like VGG16)
    Transforms
    Utils



--
Resizing: If you want to resize/reshape tensor, you can use torch.view:

x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)   # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())

Out:

torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])


+++++++++++      100+ Tensor operations   https://pytorch.org/docs/stable/torch.html


-
Converting a Torch Tensor to a NumPy array and vice versa is a breeze.

The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), 
and changing one will change the other.

b = a.numpy()
print(b)

and from numpy to tensor

import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)



CUDA Tensors

Tensors can be moved onto any device using the .to method.



--
Autograd

This class is an engine to calculate derivatives (Jacobian-vector product to be more precise).
It records a graph of all the operations performed on a gradient enabled tensor 
and creates an acyclic graph called the dynamic computational graph.
The leaves of this graph are input tensors and the roots are output tensors.
Gradients are calculated by tracing the graph from the root to the leaf
and multiplying every gradient in the way using the chain rule.

A Jacobian matrix is a matrix representing all the possible partial derivatives of two vectors. 
It’s the gradient of a vector with respect to another vector.

Let’s just agree, we are all bad at calculus when it comes to large neural networks.
It is impractical to calculate gradients of such large composite functions
by explicitly solving mathematical equations especially because these curves exist in a large number of dimensions
and are impossible to fathom.

This is where PyTorch’s autograd comes in. It abstracts the complicated mathematics
and helps us “magically” calculate gradients of high dimensional curves with only a few lines of code.

Gradient enabled tensors (variables) along with functions (operations)
combine to create the dynamic computational graph. 
The flow of data and the operations applied to the data are defined at runtime 
hence constructing the computational graph dynamically. 
This graph is made dynamically by the autograd class under the hood.
You don’t have to encode all possible paths before you launch the training — what you run is what you differentiate.

The autograd package provides automatic differentiation for all operations on Tensors. 
It is a define-by-run framework,
which means that your backprop is defined by how your code is run,
and that every single iteration can be different.

torch.Tensor   --->is the central class of the package. 
If you set its attribute .requires_grad as True, it starts to track all operations on it. 
When you finish your computation you can call .backward() and have all the gradients computed automatically.
The gradient for this tensor will be accumulated into .grad attribute.

To stop a tensor from tracking history, you can call .detach() 
to detach it from the computation history, and to prevent future computation from being tracked.

To prevent tracking history (and using memory),
you can also wrap the code block in with torch.no_grad():. 
This can be particularly helpful when evaluating a model 
because the model may have trainable parameters with requires_grad=True, but for which we don’t need the gradients.

There’s one more class which is very important for autograd implementation - a  --->   Function.

Tensor and Function    -->  are interconnected and build up an acyclic graph, 
that encodes a complete history of computation. 
Each tensor has a   .grad_fn    attribute that references a Function that has created the Tensor 
(except for Tensors created by the user - their grad_fn is None).

If you want to compute the derivatives, you can call .backward() on a Tensor. 
If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(),
however, if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.

import torch

Create a tensor and set requires_grad=True to track computation with it

x = torch.ones(2, 2, requires_grad=True)
print(x)

Do a tensor operation:

y = x + 2
print(y)

Out:

tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)

y was created as a result of an operation, so it has a grad_fn.

print(y.grad_fn)

Out:

<AddBackward0 object at 0x7f1d35f45ef0>


.requires_grad_( ... )
changes an existing Tensor’s requires_grad flag in-place. 
The input flag defaults to False if not given.

You can also stop autograd from tracking history on Tensors with .requires_grad=True 
either by wrapping the code block in with torch.no_grad()
Or by using .detach() to get a new Tensor with the same content but that does not require gradients:



Generally speaking, torch.autograd is an engine for computing ---->  vector-Jacobian product.



class
torch.autograd.Function


    Records operation history and defines formulas for differentiating ops.

    Every operation performed on Tensor s creates a new function object, that performs the computation,
    and records that it happened. The history is retained in the form of a DAG of functions,
    with edges denoting data dependencies (input <- output). 
    Then, when backward is called, the graph is processed in the topological ordering,
    by calling backward() methods of each Function object, and passing returned gradients on to next Function s.

    Normally, the only way users interact with functions is by creating subclasses and defining new operations. 
    This is a recommended way of extending torch.autograd.
    
    

Backward() function

Backward is the function which actually calculates the gradient by passing it’s argument (1x1 unit tensor by default) 
through the backward graph all the way up to every leaf node traceable from the calling root tensor. 
The calculated gradients are then stored in .grad of every leaf node. 
Remember, the backward graph is already made dynamically during the forward pass. 
Backward function only calculates the gradient using the already made graph and stores them in leaf nodes.

An important thing to notice is that when z.backward() is called, 
a tensor is automatically passed as z.backward(torch.tensor(1.0)). 
The torch.tensor(1.0)is the external gradient provided to terminate the chain rule gradient multiplications. 
This external gradient is passed as the input to the MulBackward function to further calculate the gradient of x. 
The dimension of tensor passed into .backward()
must be the same as the dimension of the tensor whose gradient is being calculated.


- A dataloader 
creates batches of the dataset and shuffles them



---
Neural networks

























