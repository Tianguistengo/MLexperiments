The Master Algorithm: Here, the books’ central hypothesis is presented:

    ‘All knowledge — past, present, and future — 
    can be deduced from data 
    by a single, universal learning algorithm.’
    
 ---------------
 
 The possibility to imagine counterfactuals
 
 ---------------
 
 The ability to EXPLORE THE WORLD
 in the hope/with the task of learning - active learning
 
 We don't learn pixels, we learn fuzzy high level abstractons
 we need ai to generate features at the level of unobserverd latent variables 
 
 -------
 
 GANs (but not only) 
 
 estimate and maximize 
 MUTUAL INFORMATION
 
 ------------
 
 Neural nets are vector processing machines 
 
 images are seen as vectors with a topology
 
 Neural nets are function approximation algorithms
 
 --------------
 
 LATENT   space / variables 
 related to conscious /unconsicous separation


    (of a quality or state) 
    existing but not yet developed or manifest; hidden or concealed.
  

    (Biology)
    lying dormant or hidden until circumstances are suitable for development or manifestation.
    
----------------    
    
 AI has been developed mainly (but not exclusively) by a technical community
 but IMAGINATION - CREATIVITY - POETRY - INTUITION - BEAUTY - FUZZY THINKING - MAGIC
 will bring the biggest/fastest changes

-----------------
https://blogs.sciencemag.org/pipeline/archives/2019/10/02/automated-discovery


I think that the classification scheme in the paper is a useful one to start to deal with these objections.
They divide scientific discoveries impacted by automation into three categories: 

1. physical matter (a drug candidate, a new metal alloy, new crystal form, etc.),

2. processes (such as new chemical reactions), 

3. models (new laws, rules of thumb, correlations, and connections). 

The authors argue that all three of these are fundamentally 
>>search problems
– they just differ in the knowledge space being searched, 
which is a process of validation and feedback. 

That holds whether you’re talking about a hypothesis-first (Popperian) mode of discovery 
or an observation-first (Baconian) one; 
the difference between the two is (to a large extent) 
where you enter that cycle of observation and experimentation.

The paper makes the key point that in every example of machine-aided discovery so far, 
>>the search space has been far larger 
than what was (or even could be) explored. 

When you look closer, >>>>>    it’s human input that has narrowed the terms and the search space.

The authors also note the three factors that are enabling automation in all of these classes 
– access to large amounts of data, 
-the increasing computing power to process it all, 
-the advances in hardware to mechanically manipulate the physical tools of experimentation

Now one gets to the question of just how automated/autonomous things really are (or really can get):

    Here, we propose a set of questions to ask when evaluating the extent to which a discovery process or workflow is autonomous: 
    
    (i) How broadly is the goal defined? 
    (ii) How constrained is the search/design space? 
    (iii) How are experiments for validation/feedback selected? 
    (iv) How superior to a brute force search is navigation of the design space?
    (v) How are experiments for validation/feedback performed? 
    (vi) How are results organized and interpreted? 
    (vii) Does the discovery outcome contribute to broader scientific knowledge?


---------------
Maithra Raghu and Eric Schmid

>> two central considerations for many scientific use cases.

    1.Techniques to use deep learning with less data 
    2.Better interpret these complex models 
 
 >> Thrree ways deep learning can help
 
 1. Prediction problems -> mapping inputs to outputs
 2. From prediction to understanding -> understand properites of the data, features, hidden representations. How the model works.
 3. Complex transformations of input data -> for efficient analysis and automated processing of petabytes of scientific data
 
 2.3 Deep Learning or Not?
 
As a final note before diving into the different deep learning techniques,
when formulating a problem, it is important to consider whether deep learning provides the right set of tools to solve it.
The powerful underlying neural network models offer many sophisticated functionalities, 
such learned complex image transforms. 
However, in many settings, deep learning may not be the best technique to start with
or best suited to the problem. Below we very briefly overview some of the most ubiquitous machine learning methods, 
particularly in scientific contexts.

Dimensionality Reduction and Clustering 

In scientific settings, the ultimate goal of data analysis is often understanding
— identifying the underlying mechanisms that give rise to patterns in the data.
When this is the goal, dimensionality reduction, and/or clustering are simple (unsupervised) 
but highly effective methods to reveal hidden properties in the data. 
They are often very useful in the important first step of exploring and visualizing the data 
(even if more complex methods are applied later.)

Dimensionality Reduction: 
Dimensionality reduction methods are either linear, relying on a linear transformation to reduce data dimensionality, 
or non-linear, reducing dimensionality while approximately preserving the non-linear (manifold) structure of the data. 
Popular linear dimensionality reduction methods include PCA and non-negative matrix factorization, 
with some popular non-linear methods including t- SNE [141] and UMAP [148].

Clustering: 
Often used in combination with dimensionality reduction, 
clustering methods provide a powerful, unsupervised way to identify similarities and differences across the data population. 
Commonly used clustering methods include k-means (particularly the k-means++ variant), Gaussian Mixture Models(GMMs),
hierarchical clustering and spectral clustering. 




  A significant obstacle in beginning to use deep learning is simply knowing where to start.
  The vast research literature, 
  coupled with the enormous number of underlying models, 
  tasks and training methods makes it very difficult to identify which techniques might be most appropriate to try, 
  or the best way to start implementing them.
  
   From the perspective of aiding scientific applications, the survey describes in detail:
   (i) methods to use deep learning with less data (self-supervision, semi-supervised learning, and others)
   and (ii) techniques for interpretability and representation analysis (for going beyond predictive tasks).
   These are two exciting and rapidly developing research areas, and are also of particular significance to possible scientific use cases.
 
1. From Predictions to Understanding 

One fundamental difference between scientific questions and core machine learning problems is the 
emphasis of science on understanding the underlying mechanisms.

>>interpretability and representational analysis,
a set of techniques focused on gaining insights into the internals of the end-to-end system: 
identifying important features in the data,
understanding its effect on model outputs 
and discovering properties of model hidden representations. 

These are very important for many scientific problems which emphasise understanding
over predictive accuracy, 
and may be of broader interest for e.g. aiding model debugging and preemptively identifying failure modes.

There has been significant work on both tools to understand 
what features of the input are most critical to the output prediction, 
as well as techniques to 
>>> directly analyze the hidden representations 
of the neural network models, 
which can reveal important properties of the underlying data.

2. Complex Transformations of Input Data 

In many scientific domains, >>the amount of generated data, particularly visual data 
(e.g. fluorescence microscopy, spatial sequencing, specimen videos [177, 97]) has grown dramatically, 
>>and there is an urgent need for 
efficient analysis 
and automated processing. 

Deep learning techniques, which are capable of many complex transformations of data, 
can be highly effective for such settings, for example, using a deep neural network based segmentation model to automatically
identify the nuclei in images of cells, 
or a pose estimation system to rapidly label behaviors seen in videos of mice for neuroscience analysis.

3. Prediction Problems

We can also think of this predictive use case as 
>>>>>getting the model to learn a target function,
in our example, mapping from input visual features to the cancer/no cancer output.

Using deep learning in this way also encapsulates 
>> settings where the target function is very complex, 
with no mathematical closed form or logical set of rules that describe how to go from input to output. 

For instance, we might use a deep learning model to (black-box) >> simulate a complex process
(e.g. climate modelling), that is very challenging to explicitly model.


6.2.4 Co-training

Another way to provide feedback on unlabelled data
is to train two (many) neural network models, >> each on a different view of the raw data.

For example, with text data, each model might see a different part of the input sentence. 
These models can then be given feedback to be maximally consistent with each other,
or with a different model which sees all of the data, 
or even used for self-training,
with each different model providing pseudo labels on the instances it is most confident on.

This post https://ruder.io/semi-supervised/ gives a nice overview of different co-training schemes.



------------
Uni camb

robot-scientist-becomes-first-machine-to-discover-new-scientific-knowledge
2009
https://www.cam.ac.uk/research/news/robot-scientist-becomes-first-machine-to-discover-new-scientific-knowledge

"As we start to consider living systems in a holistic manner, 
the complexity of such systems 
means that it will become increasingly difficult for scientists 
to formulate hypotheses unaided. 

Thus it will be necessary
for human and robot scientists to work together 
to achieve the goals of biological research.

"It is not the management and analysis of complex data 
that is the big deal about Adam, 
it is the ability of the machine to reason with those data 
>>>>>and make proposals about how a living thing works."

Prof Ross King, who led the research at Aberystwyth University, said: 
"Ultimately we hope to have teams of human and robot scientists working together in laboratories. 
Because biological organisms are so complex 
it is important that the details of biological experiments (y yo diria mined tambien) are recorded in great detail. 
This is difficult and irksome for human scientists, but easy for Robot Scientists."



Artificially-intelligent Robot Scientist ‘Eve’ could boost search for new drugs
2015
https://www.cam.ac.uk/research/news/artificially-intelligent-robot-scientist-eve-could-boost-search-for-new-drugs

>>>>>>>> they describe how the robot can help identify promising new drug candidates for
>>>>>>>.malaria and neglected tropical diseases such as African sleeping sickness and Chagas’ disease.

Eve, an artificially-intelligent ‘robot scientist’ 
could make drug discovery
faster 
and much cheaper.

Discovered that a compound shown to have anti-cancer properties 
might also be used in the fight against malaria.

[Eve's artificial intelligence] reduces the 
costs,
uncertainty, 
and time involved in drug screening.

They can automatically 
develop and test hypotheses to explain observations, 
run experiments using laboratory robotics, 
interpret the results to amend their hypotheses, 
and then repeat the cycle, 
automating high-throughput hypothesis-led research. 

Robot scientists are also well suited to 
>> recording scientific knowledge: 
as the experiments are conceived 
and executed 
automatically by computer, 
it is possible to completely capture
and digitally curate 
>> all aspects of the scientific process.

 help identify promising new drug candidates 
 
 Eve exploits its artificial intelligence to 
 learn from early successes in her screens 
 and select compounds that have a high probability of being active 
 against the chosen drug target.
 
  A smart screening system, 
  based on genetically engineered yeast, is used.
  This allows Eve to exclude compounds that are toxic to cells 
  and select those that block the action of the parasite protein while leaving any equivalent human protein unscathed. 
  This reduces the costs, uncertainty, and time involved in drug screening.

Eve is designed to automate early-stage drug design. 
First, she systematically tests each member from a large set of compounds 
in the standard brute-force way of conventional mass screening. 

The compounds are screened against assays (tests) designed to be automatically engineered, 
and can be generated much faster and more cheaply than the bespoke assays that are currently standard. 

This enables more types of assay to be applied, 
more efficient use of screening facilities to be made, 
and thereby increases the probability of a discovery within a given budget.
 
 
To test the viability of the approach, 
the researchers developed assays targeting key molecules from parasites 
responsible for diseases such as malaria, Chagas’ disease and schistosomiasis 
and tested against these a library of approximately 1,500 clinically approved compounds. 

Through this, Eve showed that a compound that has previously been investigated as an anti-cancer drug
inhibits a key molecule known as DHFR in the malaria parasite. 

Drugs that inhibit this molecule are currently routinely used to protect against malaria,
and are given to over a million children; 
however, the emergence of strains of parasites resistant to existing drugs means that 
the search for new drugs is becoming increasingly more urgent.

“Despite extensive efforts, no one has been able to find a new antimalarial 
that targets DHFR and is able to pass clinical trials,” adds Professor King.

“Eve’s discovery could be even more significant than just demonstrating a new approach to drug discovery.”





----------
https://www.graphcore.ai/posts/why-artificial-intelligence-will-allow-us-to-make-new-scientific-discoveries

Why AI will enable new scientific discoveries

Andrew Briggs
is Professor of Nanomaterials in the Department of Materials at the University of Oxford

In materials science, as in other branches of experimental science such as drug discovery,
we are able to obtain ever larger amounts of data from ever more sophisticated experiments and modelling. 
And yet 
>>> the choice of what data to collect, 
>>> and the process of how best to analyse it, 
at times seem like a cottage industry compared to the sophistication of internet searches at Google or Baidu. 
Our current approaches to data in science fall well short of 
the advanced machine learning techniques social media platforms use to recognize the friend in a photo.

I predict that by 2025, 
AI will be as ubiquitous for >> running experiments 
as computers are today for controlling instrumentation and logging data. 

The paradigm shift will be from AI used for analysing the data which has already been obtained, 
>>> to AI deciding what to measure next. 

The key advances in AI that we are currently exploring to make this possible are
reinforcement learning 
and Bayesian optimisation.

Uncertainty, 
which could be reduced by acquiring more data,
but the data are costly to acquire. 
How should we choose what experiment to do next?

>>These same methods can also be used to optimise a system for a given set of criteria.
By analysing the results of new experiments and new material discoveries,
>>we can further tune our approaches, 
saving time, money and improving outcomes.

 Initially this can be applied to prototypes which have already been made, 
 but eventually it can be applied to >> the whole design 
 and even to >> the manufacturing process. 
 
 This could make a significant difference in the cost of production for advanced materials.

(AI) will allow us to 
> understand 
>and explore 
>the use 
>of new materials 
>and new techniques. 

More importantly, 
new machine learning techniques applied to scientific discovery will quickly 
>>enable complex research challenges 
to be addressed that could never be solved by humans alone working 
within feasible time limits and resources.

--------------------

https://news.uchicago.edu/story/how-ai-could-change-science

     “While the tech industry is often focused on short-term returns,
     realizing the full potential of AI to improve our world 
     requires long-term vision.” 

—Prof. Rebecca Willett

New materials
essentially are just >>different combinations<< of chemicals and molecules, 
but there are literally billions of such combinations. 
How do scientists pick which ones to make and test in the labs? AI could quickly narrow down the list.

“There are many areas where the Edisonian approach
—that is, having an army of assistants make and test hundreds of different options for the lightbulb—
just isn’t practical,” Ferguson said.

Then there’s the question of what happens if AI takes a turn at
>>> being the scientist. 

Some are wondering whether 
>>AI models could propose 
new experiments that 
>>might never have occurred to their human counterparts.

---------

https://www.the-scientist.com/reading-frames/can-artificial-intelligence-make-scientific-discoveries--65790

Swanson and University of Illinois at Chicago psychiatry professor Neil Smalheiser 
developed a computer program called Arrowsmith 
that plucked out such hypotheses from medical research databases, 
>>> with a focus on theories generated out of links between disparate specialties. 

Swanson later hypothesized a relationship between magnesium deficiency and migraine headaches 
that was also supported by subsequent clinical research.

Over the years, Arrowsmith has had limited effect, but Swanson’s early foray suggests that 
>> finding relationships in data from disparate fields 
>> can help tap into undiscovered knowledge hidden in data. 

Although Swanson’s efforts were fully manual,
such a process can indeed be automated to help uncover knowledge that scientists might not have discovered yet.

------
Limits of deep learning

Lex & F Chollet

 you've talked about local versus extreme generalization 
 
 you mentioned the neural
networks don't generalize well humans do

so there's this gap so and you've also
00:26
mentioned that generalization extreme
00:28
generalization requires something like a
00:30
reasoning to fill those gaps 


it's not obvious but I
think it's doable

there is no hard limitations to
what you can learn with a deep neural network as long as 
the search space is rich enough is flexible enough
and as long as 
you have this dense sampling of the input cross output space

the problem is that you know distance
sampling could mean anything from 10,000
examples to like trillions and trillions 


think what kind of problems can be
solved by getting a huge amounts of data
and thereby creating a dense mapping


----------

Can scientific discovery be automated?

https://www.theatlantic.com/science/archive/2017/04/can-scientific-discovery-be-automated/524136/

Challenges of too much quantity (of research)
andthe finite neurological capacity of the human mind. 

Scientists are deriving hypotheses from a smaller and smaller fraction of our collective knowledge
and consequently, more and more, asking the wrong questions, or >> asking ones that have already been answered.

Human creativity is influenced by the stochasticity of previous experiences
—particular life events that allow a researcher to notice something others do not.

It could even begin another scientific revolution. That huge possibility hinges on an equally huge question: 
>>> Can scientific discovery really be automated?

The first reiterations of the scientific method can be traced back many centuries earlier
to Muslim thinkers such as Ibn al-Haytham, who emphasized both empiricism and experimentation. 
However, it was Francis Bacon who first formalized the scientific method and made it a subject of study. 
In his book Novum Organum (1620), he proposed a model for discovery that is still known as the Baconian method.
He argued against syllogistic logic for scientific synthesis, which he considered to be unreliable. 
Instead, he proposed an approach in which relevant observations about a specific phenomenon are systematically collected,
tabulated and objectively analyzed using inductive logic to generate generalizable ideas. 
In his view, truth could be uncovered only when the mind is free from incomplete (and hence false) axioms.

The Baconian method attempted to >remove logical bias from the process of observation and conceptualization,
by delineating the steps of scientific synthesis and optimizing each one separately. 

>>Bacon’s vision was to leverage a community of observers to collect vast amounts of information about nature
and tabulate it into a central record accessible to inductive analysis. 

The Baconian method is rarely used today. It proved too laborious and extravagantly expensive.

However, at the time the formalization of a scientific method marked a revolutionary advance. 
Before it, science was metaphysical, accessible only to a few learned men, mostly of noble birth. 
By delineating the steps of discovery, 
Bacon created a blueprint that would allow anyone, regardless of background, to become a scientist.

Bacon’s insights also revealed an important hidden truth: the discovery process is inherently algorithmic.
It is the outcome of a finite number of steps that are repeated until a meaningful result is uncovered. 
Bacon explicitly used the word “machine” in describing his method.
His scientific algorithm has three essential components: 
First, observations have to be collected and integrated into the total corpus of knowledge.
Second, the new observations are used to generate new hypotheses. 
Third, the hypotheses are tested through carefully designed experiments.

>> IFFF science is algorithmic, then it must have the potential for automation. 


---------

https://michaelnielsen.org/blog/the-artist-and-the-machine/

Water in Suspense reveals a hidden world. 
We discover a rich structure immanent in the water droplet, a structure not ordinarily accessible to our senses.
In this way it’s similar to the Hubble Extreme Deep Field, which also reveals a hidden world. 

Both are examples of what I call Super-realist art, art which doesn’t just capture what we can see directly 
with our eyes or hear with our ears, but which uses new sensors and methods of visualization to reveal a world
that we cannot directly perceive. It’s art being used to reveal science.

Suppose, for example, we could zoom in on his face, seeing it at 10 times magnification, then 100, then 1,000, and so on.
At each level of magnification a new world would be revealed, of pores and cells and microbes. 
And there is so much structure to understand in each of these worlds, 
from the incredibly complex ecology of microbes on the skin, to the inner workings of cells. 

 Virtuosity becomes about revealing hidden worlds and discovering new aesthetics. 
 Take a look at the following video, which shows — in extreme slow motion! — a packet of light passing through a bottle.
 Ordinarily that passage takes less than a nanosecond, but the video slows the passage down by a factor of 10 billion, 
 so we can see it.
 
 In a way, Super-realism is a return to the aims of Realism, back to representing reality. 
 But what’s new about Super-realism is the challenge (and opportunity!) 
 to find ways of representing previously unseen parts of the world.


-------------

Scientific intuition inspired by machine learning generated hypotheses

Pascal Friederich, Mario Krenn, Isaac Tamblyn,and Alan Aspuru-Guzik

>>>>>>>>>>>chingos de good papers en REFERENCES  -> subraye los que se ven buenos

A key challenge lies in >> the design of experiments
which creates certain desired quantum systems. 

The difficulty arises from >>counter-intuitive quantum phenomena, 
which raises the question of >> whether human intuition is the best way to design new experiments. 

Several studies have therefore developed automated and machine-learning augmented approaches 
>for the design of experiments [39–44]. 
The goal in our approach is to tackle this challenge in a completely different way, 
>> namely by improving the scientist’s intuition about these systems.

Logically combined features:
We can logically combine graph features, and find the most significant macro-features for quantum experiments.
In Fig. 7a, two small sub-experiments are combined with a logical -AND-,
i.e. the feature is the combination of both structures.

Individually, the presence of the first feature has a negative influence on nQ.
The second feature, a parity sorter followed by two detectors, influences nQ positively.
Surprisingly, their combination has a significant negative influence on nQ 
and can be seen as an almost sufficient condition for nQ ≈ 4 .

IV. CONCLUSION AND OUTLOOK

We presented a data-driven machine learning workflow for
automated generation 
and verification 
of hypotheses about observations in natural sciences. 

We presented examples from chemistry and physics, but our method is directly applicable to most applications, 
>> where structures can be represented as graphs, 
e.g. to DNA/RNA data in biology [50, 51], chemical reaction networks [52, 53] or graphs in social sciences.

In chemistry, the workflow “rediscovers” widely known relations regarding solubility and electronic properties of molecules
(often referred to as chemical intuition). 

In physics, the algorithm discovers rules 
to generate highly entangled three-photon states in quantum optical experiments. 

These rules are interpretable by human experts in retrospect, yet not known or postulated before,
>> and even contradicting some of the field’s current understanding. 

Finding such rules will not only help researchers to >> understand complex scientific relationships
>>and thus design better experiments, 
>> but also reduce unavoidable and often undetectable bias 
generated by prior knowledge and anticipations.

For hypothesis verification,
The second option is based on finding other experimental setups within the whole database 
that are as similar to the reference experiment as possible, 
with the exception of the feature that is currently analysed. 
This procedure is computationally costly as well but does not require new computations.


-----------------

Using deep networks for scientific discovery in physiological signals

Tom Beer, Bar Eini-Porat, Sebastian Goodfellow, Danny Eytan, Uri Shalit

Abstract
Deep neural networks (DNN) have shown remarkable success in the > classification of physiological signals.
In this study we propose a method for examining to what extent 
does a DNN’s performance rely on rediscovering existing features of the signals, 
as opposed to discovering genuinely new features. 

Moreover, we offer a novel method of “removing” a hand-engineered feature from the network’s hypothesis space,
thus forcing it to try and learn representations which are different from known ones, 
as a method of scientific exploration.

We then build on existing work in the field of interpretability, specifically class activation maps,
to try and infer what new features the network has learned. 

We demonstrate this approach using ECG and EEG signals.
With respect to ECG signals we show that for the specific task of classifying atrial fibrillation,
DNNs are likely rediscovering known features. We also show how our method could be used to discover new features,
by selectively removing some ECG features and “rediscovering” them. 

We further examine how could our method be used as a tool for 
>> examining scientific hypotheses. 

We simulate this scenario by looking into the importance of eye movements in classifying sleep from EEG.
We show that our tool can successfully focus a researcher’s attention 
>> by bringing to light patterns in the data that would be hidden otherwise.

 The key advantage of deep neural networks compared to previous learning approaches is that their
 inductive bias and expressive structure 
 makes them >> superb feature extractors (Sharif Razavian et al., 2014),
 often replacing the need for hand-engineering features.
 
In many cases these hand-engineered features 
>>> represent the accumulation of decades of scientific research
and understanding of the underlying physical processes. 


In general, while methods exist for learning a classifier while ignoring predefined aspects of the input,
we wish to emphasize that our goal is slightly different: 
we wish to learn “over and above” the information extant in the predefined features. 
In a sense we wish to 
>>fit the residual left after predicting the label with the known feature. 
That is distinct from the methods such as that by Kim et al. (2019)
above which focus on ignoring the predefined features altogether.


5. Related Work

The goal of augmenting neural representations with external feature spaces, 
with or without inducing independence, has been studied to some extent in various domains.

Most of these works have attempted to either remove certain biases from the learned representation, 
or to improve its robustness in domain adaptation tasks.

In the computer vision domain, 

Kim et al. (2019) propose a mutual information regularization term 
that encourages the network to ’unlearn’ a predefined bias attribute. 

Wang et al. (2019) employ an adversarial approach 
to remove a protected attribute (e.g. gender) from the input space,
yielding a “debiased” dataset. 

Singh et al. (2020) propose a loss term that penalizes overlap between class activation mappings of two co-occurring classes,
mitigating spurious correlations. While they do succeed in learning a more robust representation of their target classes,
their process still requires having prior knowledge about the label co-occurrence.

Atzmon et al. (2020) model object-attribute pairs 
by means of the intervention that has generated the image. 
They also apply HSIC to induce independence between object and attribute embeddings.

In natural language processing, 

there is a long tradition of injecting external knowledge to models.

For example, Kiperwasser and Goldberg (2016) aims to extract a complementary representation
for a known hand-crafted feature set. The boosted performance 
presented in the above work suggests that complementary information is indeed learned,
but there might be redundancy between this complementary representation and the hand crafted features
as no notion of independence is enforced.

Elazar and Goldberg (2018) adversarially remove demographic attributes from a text corpus.

6. discussion

In this work we presented a new method for exploring to what degree Deep Neural Network representations 
replicate existing knowledge in terms of feature engineering for physiological signals.

This method can further be used to discover new knowledge embedded within the representations of neural networks,
by removing the traces of existing features from the learned representation 
enabling greater focus on potentially novel features.

After validating our method indeed succeeds in learning representations which are 
independent and non-redundant 
with respect to given hand engineered features,

>>> Re ECG, , it is impressive to find that DNNs can automatically discover what took years of human research. 
On the other hand it is perhaps disappointing to see that current DNN models cannot tell us much new 
about the ECG signal in as much as it relates to AF. 

We further show that our method could in principle be used to discover new signals,
by showing how, once the RR features are removed, the activation focuses strongly on the P-Wave section,
discovering its importance for classifying AF. 

Second, we show in EEG signals that our method can help focus the attention of researchers
on potentially interesting signal attributes.
Simulating a situation where a researcher has a hypothesis about the existence of a signature of a physiological event
(eye movement) in the EEG trace and the ability to locate such events in a separate signal (EOG in our case),
we show how applying our method can focus the activation signal around a hypothesized event,
>>helping the researchers validate their hypothesis.

Limitations 

1. The framework we propose is relevant mostly to local features due to the nature of the Class Activation Maps. 

2. Class activation mappings require a specific kind of architecture: 
feature maps must directly precede softmax layers, so it is only applicable to a particular kind of CNN architecture 
performing global average pooling over convolutional maps immediately prior to prediction. 

3.Furthermore, the real challenge of feature discovery is not nearly solved by our method.
A domain expert needs to translate the activation templates and come up with interpretable functions 
or scientific understanding that capture the exposed pattern.

Future Work 

We wish to extend our work to apply to >>global features more easily. 
More broadly, we would like to be able to interface with a much broader spectrum of methods 
for explaining and understanding DNNs, >>instead of relying heavily on CAMs.

We also believe this approach can be extended to other physiological signals and possibly to other modalities
such as images to drive scientific discovery and hypothesis testing.

--------------------

How Artificial Intelligence Is Changing Science
https://www.quantamagazine.org/how-artificial-intelligence-is-changing-science-20190311/

No human, or team of humans, could possibly keep up with the avalanche of information 
produced by many of today’s physics and astronomy experiments.
 can plow through mountains of data, highlighting anomalies and detecting patterns 
 that humans could never have spotted.

But some scientists are arguing that the latest techniques in machine learning and AI 
represent a fundamentally new way of doing science.

>>>>>>>>"Let’s erase everything we know about astrophysics.   borrar el human bias/verlo desde otra perspectiva
To what degree could we rediscover that knowledge, just using the data itself?"
Kevin Schawinski

Both 1.observation and 2.simulation help scientists generate hypotheses that can then be tested with further observations. 
3. Generative modeling differs from both of these approaches.

“It’s basically a third approach, between observation and simulation,” 
says Kevin Schawinski, an astrophysicist 
>>>>>>>>>>“It’s a different way to attack a problem.”

Some scientists see generative modeling and other new techniques 
>>>> simply as power tools for doing traditional science. >> la diferencia entre horse power y doing things differently

hablando de galaxy zoo, el experimento para crowdsource recognition of galaxies, fue exitoso pero,
“Today, a talented scientist with a background in machine learning and access to cloud computing
could do the whole thing in an afternoon.”

“It’s not fully automated science 
— but it demonstrates that we’re capable of at least in part building the tools 
that make the process of science automatic,” Schawinski said.

Hardworking Assistants

Polsterer sees these new AI-based systems as “hardworking assistants” that can > comb through data for hours on end 
without getting bored or complaining about the working conditions. 
These systems can do all the tedious grunt work, he said, 
leaving you “to do the cool, interesting science on your own.” >> (buen punto de debate)

But they’re not perfect. 
In particular, Polsterer cautions, the algorithms can only do > what they’ve been trained to do.
“It comes back to you, the researcher. You’re the one in charge of doing > the interpretation.”

AI is essential because of what Melko calls “the exponential curse of dimensionality.”
>>That is, the possibilities for the form of a wave function 
grow exponentially with the number of particles in the system it describes. 
The difficulty is similar to trying to work out the best move in a game like chess or Go:
You try to peer ahead to the next move, imagining what your opponent will play, and then choose the best response, 
but with each move, >>> the number of possibilities proliferates.

Whether Schawinski is right in claiming that he’s found a “third way” of doing science,
or whether, as Hogg says, it’s merely traditional observation and data analysis “on steroids,” 
it’s clear AI is changing the flavor of scientific discovery, and it’s certainly accelerating it. 
How far will the AI revolution go in science?

------------

Norbert Wiener published a book on the place of machines in society that ended with a warning:
>>>>"we shall never receive the right answers to our questions unless we ask the right questions.... 
The hour is very late, and the choice of good and evil knocks at our door." 
https://www.frankawilczek.com/single-post/2018/09/05/possible-minds-twenty-five-ways-of-looking-at-ai-hardcover-february-19-2019-by-john-bro

-------------

>>>>>>>>>>>>>>>. A Beautiful Question ----Frank Wilczek












