https://www.newyorker.com/culture/annals-of-inquiry/the-mechanical-muse

There are more resonances between programming and poetry than you might think.
Computer science is an art form of words and punctuation,
thoughtfully placed and goal-oriented, even if not necessarily deployed to evoke surprise or longing. 
Laid out on a page, every program uses indentations, stanzas, and a distinctive visual hierarchy to convey meaning. 
In the best cases, a close-reader of code will be rewarded with a sense of awe 
for the way ideas have been captured in words.

Many programmers have links to poetry—Ada Lovelace, the acknowledged first programmer ever,
was Lord Byron’s daughter—but it’s a challenge to fully bridge the gap. 

The Turing Test has long been a standard for assessing artificial intelligence, but, 
in the context of making art—rather than simulating consciousness
—it may not be the most valuable, or the most interesting, metric.
One of my colleagues, Mary Flanagan, a poet, artist, and professor of digital humanities, 
thinks the notion that machine-generated poems should be expected to pass the Turing Test is boring. 
>>>“Humans are already good at producing human-sounding sonnets, so why get a computer to do that? Do something new!”

As we explore the ways in which new high-tech tools seem familiar or jarring,
we continue to discover aspects of what it means to be human, as writers and readers, creators and consumers.
Can a machine write poetry? What is poetry? The goalposts move again.

----------------------
The Myth and Magic of Generating New Ideas

https://www.newyorker.com/culture/annals-of-inquiry/the-myth-and-magic-of-generating-new-ideas

Where do ideas come from? That’s a big question. 

It’s not at all obvious how to go about thinking up some new twist on these things
—the transformation from test-taker to theorem poser and then theorem prover is >> difficult to articulate. 
My ideas have always felt contingent and magical to me. I don’t think I’m alone, at least as far as the magic goes.

Henri Poincaré, the father of chaos theory and the co-discoverer of special relativity, 
is famous for a story that appears in his 1908 book “Science and Method,”
about an insight being jarred loose while boarding a bus: 
“At the moment when I put my foot on the step, the idea came to me, 
without anything in my former thoughts seeming to have paved the way for it.” 

The Irish mathematician Sir William Rowan Hamilton, 
who devoted many years to searching for a way to multiply numbers in higher dimensions, had a similar epiphany, in 1843, 
just as he was strolling by the Brougham Bridge, in Dublin, while on a walk with his wife.
He was so delighted that he stopped and carved the defining algebraic equation into the bridge:
i2=j2=k2=ijk=-1. One person’s graffiti is another person’s breakthrough.

These stories suggest that an initial period of concentration—conscious, directed attention
—needs to be followed by some amount of unconscious processing.
In this view of creative momentum, the key to solving a problem is to take a break from worrying (trabajar),
to move the problem to the back burner, to let the unwatched pot boil.

For me, the quest for a breakthrough often requires getting myself into literal motion;
one small step for Poincaré but a whole sequence of steps for me. 
Perhaps it is the momentary feeling of being untethered that gives the mind free rein—the space to have a good idea.
The key here isn’t fitness >> it’s just a feeling of being free,
of forgetting for a moment that we are bound by gravity and logic and convention,
of letting the magic happen.

Whereas Erdős sought hyper-focussed vigilance (con drogas), 
other eminent mathematicians have found a hazy drowsiness to be the most fertile state of mind. 

Poincaré described lying in bed in a half-dream state as the ideal condition for coming up with new ideas. 

The philosopher and mathematician René Descartes famously loved to lounge in bed in the morning and think.
It was on one such morning—as the story goes—while dreamily watching the path of a fly flitting around on the ceiling, 
that he came up with the xy plane of Cartesian coordinates.

This kind of hallucinatory visual phenomenon is one of a host of well-known phantasms. 
Despite the ultimate frustrations of my dreamed mathematics, I sometimes wonder if they did in fact help spur
my creative process in a subconscious way. 
Many of my topologist colleagues, who study the properties of geometric objects, 
seem to live in an alternative, imagined world.
It’s reassuring to remember that all sorts of intellectual—and hallucinatory—predilections 
can find a way into mathematical work.

The origin stories of big ideas, whether in math or any other field, generally highlight the eureka moments. 
Chance really does favor the prepared mind; 
when the moments of discovery came, often unexpectedly, my hours of hard work felt newly valuable.

My waking, working life, like my dream life, can sometimes feel like a series of epiphanies
that are just beyond my reach—nonsensical symbols that I can’t read and invisible objects that I can’t see. 

I strike the ball and lift the weights, knowing that 
there is something about moving my body that will help move my mind.

---------------------------
https://www.amazon.com/What-Believe-but-Cannot-Prove/

Scientific theory, more often than not, is born of bold assumption, 
disparate bits of unconnected evidence,
and educated leaps of faith. 
Some of the most potent beliefs among brilliant minds are based on supposition alone
-- yet that is enough to push those minds toward making the theory viable.

Eminent cultural impresario, editor, and publisher of Edge (www.edge.org), John Brockman 
asked a group of leading scientists and thinkers to answer the question: 
What do you believe to be true even though you cannot prove it?
This book brings together the very best answers from the most distinguished contributors.

------------------------

https://www.quantamagazine.org/how-artificial-intelligence-is-changing-science-20190311/

Another oft-heard argument is that science requires creativity,
and that — at least so far — we have no idea how to program that into a machine. 
(Simply trying everything, like Cronin’s robo-chemist, doesn’t seem especially creative.) 

“Coming up with a theory, with reasoning, I think demands creativity,” Polsterer said. (mmmmmm)
“Every time you need creativity, you will need a human.”(mmmm)
And where does creativity come from? Polsterer suspects it is related to boredom 
— something that, he says, a machine cannot experience. 
“To be creative, you have to dislike being bored. And I don’t think a computer will ever feel bored.”
On the other hand, words like “creative” and “inspired” have often been used to describe programs 
like Deep Blue and AlphaGo. 
And the struggle to describe what goes on inside the “mind” of a machine 
is mirrored by the difficulty we have in probing our own thought processes.

------------------

https://www.quantamagazine.org/computers-evolve-a-new-path-toward-human-intelligence-20191106/

Stanley’s realization led to what he calls the steppingstone principle — and, with it,
a way of designing algorithms that more fully embraces the endlessly creative potential of biological evolution.

Evolutionary algorithms have been around for a long time. 

The steppingstone principle goes beyond traditional evolutionary approaches.
>Instead of optimizing for a specific goal, 
>it embraces creative exploration of all possible solutions. 
By doing so, it has paid off with groundbreaking results. 
Earlier this year, one system based on the steppingstone principle mastered two video games 
that had stumped popular machine learning methods.
And in a paper published last week in Nature, DeepMind 
reported success in combining deep learning with the evolution of a diverse population of solutions.

Biological evolution is also the only system to produce human intelligence,
which is the ultimate dream of many AI researchers. 
Because of biology’s track record, Stanley and others have come to believe that if we want algorithms 
that can navigate the physical and social world as easily as we can — or better! —
>we need to imitate nature’s tactics. 
Instead of hard-coding the rules of reasoning, or having computers learn to score highly on specific performance metrics, 
>> we must let a population of solutions blossom. 
>>Make them prioritize novelty or interestingness instead of the ability to walk or talk. 
>>They may discover an indirect path, a set of steppingstones,
>>and wind up walking and talking better than if they’d sought those skills directly.

If I run an algorithm that’s creative to such an extent that I’m not sure what it will produce.

He hoped to show that by simply following ideas in interesting directions, 
algorithms could not only produce a diversity of results, 
but solve problems.

More audaciously, he aimed to show that completely ignoring an objective can get you there faster than pursuing it. 
He did this through an approach called novelty search.

For Kenneth Stanley, the steppingstone principle explains innovation.

In neuroevolution, you start by assigning random values to the weights between layers.
This randomness means the network won’t be very good at its job. But from this sorry state,
you then create a set of random mutations — offspring neural networks with slightly different weights — 
and evaluate their abilities. You keep the best ones, produce more offspring, and repeat. 
>>Neuroevolution is a meta-algorithm, an algorithm for designing algorithms. 
And eventually, the algorithms get pretty good at their job.

To test the steppingstone principle, Stanley and his student Joel Lehman tweaked the selection process. 
Instead of selecting the networks that performed best on a task, 
novelty search selected them for how different they were from the ones with behaviors most similar to theirs.
(In Picbreeder, people rewarded interestingness. Here, as a proxy for interestingness, novelty search rewarded novelty.)

 A key element of these algorithms is that they foster steppingstones. 
 >Instead of constantly prioritizing one overall best solution,
 >they maintain a diverse set of vibrant niches, 
 >>any one of which could contribute a winner. 
 And the best solution might descend from a lineage that has hopped between niches.

One of its biggest drawbacks, is the amount of computation it requires.
In traditional machine learning, as you train a neural network, it gradually gets better and better.
With neuroevolution, the weights change randomly, so the network’s performance might degrade before it improves.

Another drawback is the basic fact that most people have a particular problem that they’d like to solve.
A search strategy that optimizes for interestingness might get you to a creative solution for that particular problem.
But it could lead you astray before it puts you on the right path.

Within the past year, AI based on the steppingstone principle finally managed 
to crack a number of long-standing challenges in the field.

Eventually, (cuando buscas en random)
one of those states is the state of winning the game. 
And Panama Joe has in his memory all the actions he took to get there. 
He’s done it with no neural network or reinforcement learning
— no rewards for collecting keys or nearing the labyrinth’s end — 
>>just random exploration and a clever way to collect and connect steppingstones. 
>>This approach managed to beat not only the best algorithms but also the human world record for the game.

The same technique, which the researchers call >>>>> Go-Explore,
https://arxiv.org/pdf/1901.10995.pdf
was used to beat human experts at Pitfall!, a game where Pitfall Harry navigates a jungle 
in search of treasure while avoiding crocodiles and quicksand.
No other machine learning AI had scored above zero.

Now even DeepMind, that powerhouse of reinforcement learning, has revealed its growing interest in neuroevolution.

Jeff Clune, 
>>>>>>>>argues that open-ended discovery is likely the fastest way to achieve humanlike artificial intelligence.

As in the children’s game rock-paper-scissors, >> there is no single best game strategy in StarCraft II. 
So DeepMind encouraged its population of agents to evolve >> a diversity of strategies 
— not as steppingstones but as an end in itself. 
When AlphaStar beat two pros each five games to none, it combined the strategies from five different agents
in its population. The five agents had been chosen so that not all of them would be vulnerable to any one opponent strategy.
>>>>>>Their strength was in their diversity.

AlphaStar demonstrates one of the main uses of evolutionary algorithms:
>>maintaining a population of different solutions. 
Another recent DeepMind project shows the other use: 
>>optimizing a single solution. 
Working with Waymo, Alphabet’s autonomous car project, the team evolved algorithms for identifying pedestrians. 
>To avoid getting stuck with an approach that works fairly well, but that isn’t the best possible strategy,
they maintained “niches” or subpopulations, 
so that novel solutions would have time to develop before being crushed by the established top performers.

 “I think it’s an important area of research for AI,” she said, 
 “because it is complementary to the deep learning approaches that have driven the field.”

-------

creativity is about an explosion of ideas 
but then being critical and judging which one really is worth putting forward

----------

An artist never really finishes his work, he merely abandons it.

Paul Valéry

-------
CANs
Creative Adversarial Networks

-------
Creativity and artificial intelligence
Margaret A.Boden

Abstract

Creativity is a fundamental feature of human intelligence, and a challenge for AI. 
AI techniques can be used to create new ideas in three ways: 
1. Combinatorial creativity - by producing novel combinations of familiar ideas; 
2. Exploratory creativity -by exploring the potential of conceptual spaces; 
3. Transformational creativity - by making transformations that enable the generation of previously impossible ideas. 

AI will have less difficulty in modelling the generation of new ideas 
>>than in automating their evaluation.

 All three can be modeled by AI—in some cases, with impressive results.
 AI techniques underlie various types of computer art. (hay otros)
 >>Whether computers could “really” be creative isn’t a scientific question but a philosophical one,
 >>to which there’s no clear answer.

-------------
Risto Miikkulainen
https://venturebeat.com/2018/05/17/evolutionary-computation-will-drive-the-future-of-creative-ai/

This means we could view evolutionary computation as the next step in the progress of AI.
This type of AI is based on algorithms inspired by biological evolution.
Through reproduction, mutation, recombination, and selection, evolutionary computation performs a
parallel, exploratory search for solutions. 
>>Because this technology is based on a population of solutions rather than a single, continuously refined solution,
it can afford to try out novel ideas and discover solutions that are surprising and creative. 

In evolutionary computation, researchers harness these processes to reach a specific goal,
such as creating a website that maximizes conversions or crafting a procedure to grow the tastiest basil plants.

>>>  Evolutionary computation is focused on creating solutions that do not yet exist.

Evolutionary computation makes it possible to discover such new designs and behaviors through 
exuberant, but guided, exploration. 
It is, in a sense, the next step forward from deep learning: the form of AI that can think outside the box. 

e.g. https://en.wikipedia.org/wiki/Evolved_antenna

e.g. https://openai.com/blog/evolution-strategies/
One example is OpenAI using evolution to design neural networks for reinforcement learning, 
which showed that it performs as well and parallelizes better than gradient descent techniques. 
Such an approach takes advantage of massively parallel compute in evaluating population members.

e.g. The Uber.ai group also demonstrated how evolution affords broader exploration
with a more explicit emphasis on novel solutions. https://eng.uber.com/deep-neuroevolution/

e.g. DeepMind outlined how such exploration of neural network architectures may lead to the 
successive discovery of new behaviors.  https://deepmind.com/research/publications/pathnet-evolution-channels-gradient-descent-super-neural-networks










