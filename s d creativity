https://www.newyorker.com/culture/annals-of-inquiry/the-mechanical-muse

There are more resonances between programming and poetry than you might think.
Computer science is an art form of words and punctuation,
thoughtfully placed and goal-oriented, even if not necessarily deployed to evoke surprise or longing. 
Laid out on a page, every program uses indentations, stanzas, and a distinctive visual hierarchy to convey meaning. 
In the best cases, a close-reader of code will be rewarded with a sense of awe 
for the way ideas have been captured in words.

Many programmers have links to poetry—Ada Lovelace, the acknowledged first programmer ever,
was Lord Byron’s daughter—but it’s a challenge to fully bridge the gap. 

The Turing Test has long been a standard for assessing artificial intelligence, but, 
in the context of making art—rather than simulating consciousness
—it may not be the most valuable, or the most interesting, metric.
One of my colleagues, Mary Flanagan, a poet, artist, and professor of digital humanities, 
thinks the notion that machine-generated poems should be expected to pass the Turing Test is boring. 
>>>“Humans are already good at producing human-sounding sonnets, so why get a computer to do that? Do something new!”

As we explore the ways in which new high-tech tools seem familiar or jarring,
we continue to discover aspects of what it means to be human, as writers and readers, creators and consumers.
Can a machine write poetry? What is poetry? The goalposts move again.

----------------------
The Myth and Magic of Generating New Ideas

https://www.newyorker.com/culture/annals-of-inquiry/the-myth-and-magic-of-generating-new-ideas

Where do ideas come from? That’s a big question. 

It’s not at all obvious how to go about thinking up some new twist on these things
—the transformation from test-taker to theorem poser and then theorem prover is >> difficult to articulate. 
My ideas have always felt contingent and magical to me. I don’t think I’m alone, at least as far as the magic goes.

Henri Poincaré, the father of chaos theory and the co-discoverer of special relativity, 
is famous for a story that appears in his 1908 book “Science and Method,”
about an insight being jarred loose while boarding a bus: 
“At the moment when I put my foot on the step, the idea came to me, 
without anything in my former thoughts seeming to have paved the way for it.” 

The Irish mathematician Sir William Rowan Hamilton, 
who devoted many years to searching for a way to multiply numbers in higher dimensions, had a similar epiphany, in 1843, 
just as he was strolling by the Brougham Bridge, in Dublin, while on a walk with his wife.
He was so delighted that he stopped and carved the defining algebraic equation into the bridge:
i2=j2=k2=ijk=-1. One person’s graffiti is another person’s breakthrough.

These stories suggest that an initial period of concentration—conscious, directed attention
—needs to be followed by some amount of unconscious processing.
In this view of creative momentum, the key to solving a problem is to take a break from worrying (trabajar),
to move the problem to the back burner, to let the unwatched pot boil.

For me, the quest for a breakthrough often requires getting myself into literal motion;
one small step for Poincaré but a whole sequence of steps for me. 
Perhaps it is the momentary feeling of being untethered that gives the mind free rein—the space to have a good idea.
The key here isn’t fitness >> it’s just a feeling of being free,
of forgetting for a moment that we are bound by gravity and logic and convention,
of letting the magic happen.

Whereas Erdős sought hyper-focussed vigilance (con drogas), 
other eminent mathematicians have found a hazy drowsiness to be the most fertile state of mind. 

Poincaré described lying in bed in a half-dream state as the ideal condition for coming up with new ideas. 

The philosopher and mathematician René Descartes famously loved to lounge in bed in the morning and think.
It was on one such morning—as the story goes—while dreamily watching the path of a fly flitting around on the ceiling, 
that he came up with the xy plane of Cartesian coordinates.

This kind of hallucinatory visual phenomenon is one of a host of well-known phantasms. 
Despite the ultimate frustrations of my dreamed mathematics, I sometimes wonder if they did in fact help spur
my creative process in a subconscious way. 
Many of my topologist colleagues, who study the properties of geometric objects, 
seem to live in an alternative, imagined world.
It’s reassuring to remember that all sorts of intellectual—and hallucinatory—predilections 
can find a way into mathematical work.

The origin stories of big ideas, whether in math or any other field, generally highlight the eureka moments. 
Chance really does favor the prepared mind; 
when the moments of discovery came, often unexpectedly, my hours of hard work felt newly valuable.

My waking, working life, like my dream life, can sometimes feel like a series of epiphanies
that are just beyond my reach—nonsensical symbols that I can’t read and invisible objects that I can’t see. 

I strike the ball and lift the weights, knowing that 
there is something about moving my body that will help move my mind.

---------------------------
https://www.amazon.com/What-Believe-but-Cannot-Prove/

Scientific theory, more often than not, is born of bold assumption, 
disparate bits of unconnected evidence,
and educated leaps of faith. 
Some of the most potent beliefs among brilliant minds are based on supposition alone
-- yet that is enough to push those minds toward making the theory viable.

Eminent cultural impresario, editor, and publisher of Edge (www.edge.org), John Brockman 
asked a group of leading scientists and thinkers to answer the question: 
What do you believe to be true even though you cannot prove it?
This book brings together the very best answers from the most distinguished contributors.

------------------------

https://www.quantamagazine.org/how-artificial-intelligence-is-changing-science-20190311/

Another oft-heard argument is that science requires creativity,
and that — at least so far — we have no idea how to program that into a machine. 
(Simply trying everything, like Cronin’s robo-chemist, doesn’t seem especially creative.) 

“Coming up with a theory, with reasoning, I think demands creativity,” Polsterer said. (mmmmmm)
“Every time you need creativity, you will need a human.”(mmmm)
And where does creativity come from? Polsterer suspects it is related to boredom 
— something that, he says, a machine cannot experience. 
“To be creative, you have to dislike being bored. And I don’t think a computer will ever feel bored.”
On the other hand, words like “creative” and “inspired” have often been used to describe programs 
like Deep Blue and AlphaGo. 
And the struggle to describe what goes on inside the “mind” of a machine 
is mirrored by the difficulty we have in probing our own thought processes.

------------------

https://www.quantamagazine.org/computers-evolve-a-new-path-toward-human-intelligence-20191106/

Stanley’s realization led to what he calls the steppingstone principle — and, with it,
a way of designing algorithms that more fully embraces the endlessly creative potential of biological evolution.

Evolutionary algorithms have been around for a long time. 

The steppingstone principle goes beyond traditional evolutionary approaches.
>Instead of optimizing for a specific goal, 
>it embraces creative exploration of all possible solutions. 
By doing so, it has paid off with groundbreaking results. 
Earlier this year, one system based on the steppingstone principle mastered two video games 
that had stumped popular machine learning methods.
And in a paper published last week in Nature, DeepMind 
reported success in combining deep learning with the evolution of a diverse population of solutions.

Biological evolution is also the only system to produce human intelligence,
which is the ultimate dream of many AI researchers. 
Because of biology’s track record, Stanley and others have come to believe that if we want algorithms 
that can navigate the physical and social world as easily as we can — or better! —
>we need to imitate nature’s tactics. 
Instead of hard-coding the rules of reasoning, or having computers learn to score highly on specific performance metrics, 
>> we must let a population of solutions blossom. 
>>Make them prioritize novelty or interestingness instead of the ability to walk or talk. 
>>They may discover an indirect path, a set of steppingstones,
>>and wind up walking and talking better than if they’d sought those skills directly.

If I run an algorithm that’s creative to such an extent that I’m not sure what it will produce.

He hoped to show that by simply following ideas in interesting directions, 
algorithms could not only produce a diversity of results, 
but solve problems.

More audaciously, he aimed to show that completely ignoring an objective can get you there faster than pursuing it. 
He did this through an approach called novelty search.

For Kenneth Stanley, the steppingstone principle explains innovation.

In neuroevolution, you start by assigning random values to the weights between layers.
This randomness means the network won’t be very good at its job. But from this sorry state,
you then create a set of random mutations — offspring neural networks with slightly different weights — 
and evaluate their abilities. You keep the best ones, produce more offspring, and repeat. 
>>Neuroevolution is a meta-algorithm, an algorithm for designing algorithms. 
And eventually, the algorithms get pretty good at their job.

To test the steppingstone principle, Stanley and his student Joel Lehman tweaked the selection process. 
Instead of selecting the networks that performed best on a task, 
novelty search selected them for how different they were from the ones with behaviors most similar to theirs.
(In Picbreeder, people rewarded interestingness. Here, as a proxy for interestingness, novelty search rewarded novelty.)

 A key element of these algorithms is that they foster steppingstones. 
 >Instead of constantly prioritizing one overall best solution,
 >they maintain a diverse set of vibrant niches, 
 >>any one of which could contribute a winner. 
 And the best solution might descend from a lineage that has hopped between niches.

One of its biggest drawbacks, is the amount of computation it requires.
In traditional machine learning, as you train a neural network, it gradually gets better and better.
With neuroevolution, the weights change randomly, so the network’s performance might degrade before it improves.

Another drawback is the basic fact that most people have a particular problem that they’d like to solve.
A search strategy that optimizes for interestingness might get you to a creative solution for that particular problem.
But it could lead you astray before it puts you on the right path.

Within the past year, AI based on the steppingstone principle finally managed 
to crack a number of long-standing challenges in the field.

Eventually, (cuando buscas en random)
one of those states is the state of winning the game. 
And Panama Joe has in his memory all the actions he took to get there. 
He’s done it with no neural network or reinforcement learning
— no rewards for collecting keys or nearing the labyrinth’s end — 
>>just random exploration and a clever way to collect and connect steppingstones. 
>>This approach managed to beat not only the best algorithms but also the human world record for the game.

The same technique, which the researchers call >>>>> Go-Explore,
https://arxiv.org/pdf/1901.10995.pdf
was used to beat human experts at Pitfall!, a game where Pitfall Harry navigates a jungle 
in search of treasure while avoiding crocodiles and quicksand.
No other machine learning AI had scored above zero.

Now even DeepMind, that powerhouse of reinforcement learning, has revealed its growing interest in neuroevolution.

Jeff Clune, 
>>>>>>>>argues that open-ended discovery is likely the fastest way to achieve humanlike artificial intelligence.

As in the children’s game rock-paper-scissors, >> there is no single best game strategy in StarCraft II. 
So DeepMind encouraged its population of agents to evolve >> a diversity of strategies 
— not as steppingstones but as an end in itself. 
When AlphaStar beat two pros each five games to none, it combined the strategies from five different agents
in its population. The five agents had been chosen so that not all of them would be vulnerable to any one opponent strategy.
>>>>>>Their strength was in their diversity.

AlphaStar demonstrates one of the main uses of evolutionary algorithms:
>>maintaining a population of different solutions. 
Another recent DeepMind project shows the other use: 
>>optimizing a single solution. 
Working with Waymo, Alphabet’s autonomous car project, the team evolved algorithms for identifying pedestrians. 
>To avoid getting stuck with an approach that works fairly well, but that isn’t the best possible strategy,
they maintained “niches” or subpopulations, 
so that novel solutions would have time to develop before being crushed by the established top performers.

 “I think it’s an important area of research for AI,” she said, 
 “because it is complementary to the deep learning approaches that have driven the field.”

-------

creativity is about an explosion of ideas 
but then being critical and judging which one really is worth putting forward

----------

An artist never really finishes his work, he merely abandons it.

Paul Valéry

-------
CANs
Creative Adversarial Networks

-------
Creativity and artificial intelligence
Margaret A.Boden

Abstract

Creativity is a fundamental feature of human intelligence, and a challenge for AI. 
AI techniques can be used to create new ideas in three ways: 
1. Combinatorial creativity - by producing novel combinations of familiar ideas; 
2. Exploratory creativity -by exploring the potential of conceptual spaces; 
3. Transformational creativity - by making transformations that enable the generation of previously impossible ideas. 

AI will have less difficulty in modelling the generation of new ideas 
>>than in automating their evaluation.

 All three can be modeled by AI—in some cases, with impressive results.
 AI techniques underlie various types of computer art. (hay otros)
 >>Whether computers could “really” be creative isn’t a scientific question but a philosophical one,
 >>to which there’s no clear answer.

-------------
Risto Miikkulainen
https://venturebeat.com/2018/05/17/evolutionary-computation-will-drive-the-future-of-creative-ai/

This means we could view evolutionary computation as the next step in the progress of AI.
This type of AI is based on algorithms inspired by biological evolution.
Through reproduction, mutation, recombination, and selection, evolutionary computation performs a
parallel, exploratory search for solutions. 
>>Because this technology is based on a population of solutions rather than a single, continuously refined solution,
it can afford to try out novel ideas and discover solutions that are surprising and creative. 

In evolutionary computation, researchers harness these processes to reach a specific goal,
such as creating a website that maximizes conversions or crafting a procedure to grow the tastiest basil plants.

>>>  Evolutionary computation is focused on creating solutions that do not yet exist.

Evolutionary computation makes it possible to discover such new designs and behaviors through 
exuberant, but guided, exploration. 
It is, in a sense, the next step forward from deep learning: the form of AI that can think outside the box. 

e.g. https://en.wikipedia.org/wiki/Evolved_antenna

e.g. https://openai.com/blog/evolution-strategies/
One example is OpenAI using evolution to design neural networks for reinforcement learning, 
which showed that it performs as well and parallelizes better than gradient descent techniques. 
Such an approach takes advantage of massively parallel compute in evaluating population members.

e.g. The Uber.ai group also demonstrated how evolution affords broader exploration
with a more explicit emphasis on novel solutions. https://eng.uber.com/deep-neuroevolution/

e.g. DeepMind outlined how such exploration of neural network architectures may lead to the 
successive discovery of new behaviors.  https://deepmind.com/research/publications/pathnet-evolution-channels-gradient-descent-super-neural-networks

-------------------------------
https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/

Open-endedness: The last grand challenge you’ve never heard of

The first life form, likely something resembling a single prokaryotic cell, emerged billions of years ago.
Though we do not know the full story of its origin,
we do know that over the next few billion years, 
somehow this humble cell radiated into the full diversity of life on Earth we appreciate today.

Think about it computationally—evolution on Earth is like a single run 
of a single algorithm that invented all of nature.

 If we run a machine learning algorithm today, we’re happy if it gives us a single solution, or maybe a few,
 but then it’s over—the problem is either solved or not, and the program is finished.
 But evolution on Earth is amazingly different—it never seems to end. 
 It’s true that “never” is a strong word, but several billion years of continual creativity comes as close to never 
 as we’ll likely ever see. So yes, we call it evolution, but it’s also >> the never-ending algorithm, 
 a process that tirelessly invents ever-greater complexity and novelty across incomprehensible spans of time.

Some machine learning researchers have further suggested that EAs are inferior optimizers 
and that alternative algorithms (such as deep learning) are simply better suited for optimization. 
But that kind of critique misses the real issue. 
Evolution serves as inspiration for algorithms in computer science 
not because it is a great optimizer for a particular problem, 
>> but because it invented all of nature.

>>>>  For a moment, imagine if we could actually program a genuine open-ended algorithm.
The implications would be extraordinary. Are you interested in new schools of architecture,
new car designs, new computer algorithms, new inventions in general? 
And how about generating them ceaselessly? And with increasing complexity?

The power of nature is the power of creation, and it’s entirely encapsulated within the mystery of open-endedness.

The incredible self-generation of nature on Earth is but one infinitesimal slice of what might be possible.

While the multi-trillion-connection brain is a product of natural evolution, 
it is possible that the process of evolution itself is simpler to describe or implement. 
So, maybe we can actually figure it out and identify its necessary conditions—maybe even sometime soon.

Importantly, evolution itself is only one of many possible realizations of open-endedness.
For example, the human brain seems to exhibit its own open-ended creativity,
so it’s likely that an open-ended process can emerge from many different substrates, 
including within deep learning.

The potential here is to introduce a generic never-ending creative algorithm, in whatever domain you want.
( o sea si, si no termina se vuelve mas complejo y con mas potencial pero tampoco necesitas correrrlo
infinitamente para tener buenos resultados)
It’s also important to highlight that there are likely interesting degrees of open-endedness 
(an idea indeed embraced by Bedau’s tests). That is, open-endedness is not just a binary either/or proposition.
While we are more likely to obtain the deepest possible insight by aiming high,
as we will see, in practice, even systems and ideas that capture a limited aspect of open-endedness 
can still serve useful purposes and teach us important lessons on the road to full-scale never-ending innovation.

The hypothesis is that what we observe in nature is only one instance of a whole class of possible open-ended systems. 

The components of the process—interaction among individuals (or what machine learning researchers might call “candidates”),
selection, reproduction, etc.,—appear fundamentally generic. 

A brief history of open-endedness

Because of the inspiration from natural evolution, so far the study of open-ended algorithms has focused primarily on EAs,
though it is certainly conceivable that a non-evolutionary process 
(such as an individual neural network generating new ideas) could exhibit open-ended properties.

Nevertheless, because of the historical evolutionary focus, 
researchers in this area often refer to it as “open-ended evolution.” 
While technically these are types of evolutionary algorithms, Open-ended evolution is a whole different ballgame.

For one thing, unlike in most of machine learning, when researchers build an open-ended algorithm, 
they usually aren’t aiming to solve any problem in particular. 
>> Rather, they hope to observe a kind of complexity explosion, 
where evolving artifacts diversify into ever-more intricate and inventive forms. 

For that reason, much of the early work in the field centered on artificial “worlds” (often called “alife worlds”)
where theories could be tested on the kinds of dynamics that might lead to open-endedness. 
These virtual worlds are usually populated with “creatures” that perform behaviors like running around 
and eating other creatures. Usually, the expectation is that the creatures in the artificial world 
might evolve increasingly complex strategies (and sometimes body plans) as they strive to outcompete each other.

In 1992 Mark Bedau introduced a set of measurements called “activity statistics” 
meant to measure the degree of open-endedness exhibited by such systems. 
This outcome led to some debate on what open-endedness actually means—do Bedau’s tests somehow miss the real issue? 
The bar should be high, after all.

While scientists often seem allergic to any flavor of subjectivity, 
there may be ways to embrace subjectivity in open-endedness without sacrificing scientific rigor. 
For example, we can still try to formalize subjective notions such as impressiveness
(in work by Ken Stanley and Joel Lehman). 
>> For us to make progress, we may ultimately need to retreat somewhat from grasping after a foolproof objective measure,

una dificultad para disenar open-endedness:
complex features can arise through unpredictable routes in evolution,
illuminating how seemingly intractable complexity can evolve through random mutations

The more closed process of conventional EAs, which generally push evolution toward a particular desired outcome.
In this conventional kind of algorithm, the opportunities for unhampered discovery are limited 
because selection pressure directly seeks to improve performance with respect to the problem’s objective. 
The idea in novelty search is to take precisely the opposite approach—instead of selecting for “improvement,” 
novelty search selects only for novelty. Novelty search is open-ended because it tends to open up new paths of search
rather than closing them off.

At first, it may seem that this approach is closely related to random search, and therefore of little use, 
but actually, it turns out to be much more interesting than that. The difference is that computing the novelty
of a candidate in the search space >> requires real information (which random search would ignore)
on how the current candidate differs behaviorally from previous discoveries.
So, we might ask how a current robot’s gait is different from its predecessor’s gaits.
And if it’s different enough, then it’s considered novel and selected for further evolution.
The result is a rapid branching (i.e., divergence) and proliferation of new strategies
(e.g., new robot gaits in a walking simulator).

After the introduction of novelty search, a new class of algorithms began to appear that aimed to 
>> combine a notion of novelty with a more objective sense of progress or quality.
For example, you might want to search for all the possible ways a robot can walk,
but in order to discover the best possible version of each such variant. 
These algorithms (including novelty search with local competition, or NSLC 
and the multi-dimensional archive of phenotypic elites, or MAP-Elites) came to be known as
>>> quality diversity (QD) algorithms.

They are still missing a key ingredient of open-endedness
—they are destined to stop producing anything interesting within days of launching, and there’s nothing we can do about it.
The reason is that their problem domains contain only a narrow set of possibilities.  (entonces usa domains mas locos)
There are only so many interesting ways you can walk before it just starts being silly.

>>>>>In this way, the mystery is, while we have a good sense of how to push a search process
to keep finding what is possible within a search space, 
>>>>we don’t really understand how concurrently to expand the space of possibilities itself. 
>>>That is, the highest level of open-endedness requires not only generating new solutions, but also new problems to solve.

Overall, the field of open-endedness is interdisciplinary, young, and relatively unexplored.
Results so far have revealed hints of what’s possible from open-ended systems and 
uncovered thorny philosophical issues yet to be resolved. 

The necessary ingredients

We are accumulating general insight—for example, that full-blown open-endedness likely requires 
>the interaction of coevolving entities
>and that it goes beyond novelty or QD. 
Such thoughts about what is core to open-endedness motivate attempts to formalize its necessary conditions. 

 the more interesting implication of this experiment is the potential for all kinds of 
 unimagined pairings and elaborations in the same kind of MCC setup. 

Even as attempts at open-endedness like MCC improve, as we continue to revisit, 
creativity on the level of complexity of nature remains still far beyond our reach.

Open-endedness in practical domains

One potentially important application of open-ended systems is in creative design.

Open-endedness can vastly expand the scope of conceivable options and generate unimagined new possibilities. 

 It can potentially partner with humans, who could influence the search 
 (which sometimes goes by the name interactive evolution),
 or it could be left to generate ideas by itself in perpetuity,
 where humans can view the latest creations at their leisure.

 It’s true that much of our appreciation of this history is subjective, 
 but there is nothing in principle to stop us from accelerating our exploration of subjective domains that we enjoy. 
 ( no hay tanta bronca que sea subjectivo )
 
 Given some initial seed set of axioms from a curious mathematician, 
 could an algorithmic open-ended system continually produce interesting and surprising proofs?

 Evolve endlessly into new and unimagined forms. 

Open-endedness in AI

how different it is from how most AI research is approached today, 
where optimization toward specific objective performance is ubiquitous—
almost the perfect opposite of how an open-ended system generates its products.

So many in AI lean toward definitions of intelligence that involve “solving problems” or “learning to solve problems efficiently.”
But open-endedness is left out of this equation. A “general” open-ended system is not a machine-like problem solver, 
but rather a creative master that wanders the space of the imagination. 
That is certainly within the pantheon of human intelligence.

Intelligence is clearly a factor in cultural progress; after all, culture is a product of brains.
But a network of millions of brains seems to produce an effect far amplified beyond what any single brain can do, 
where ideas propagate throughout the network to become stepping stones for new ideas,which then propagate again, and so on. 

In particular, fields involving search within >> large, high-dimensional spaces, 
namely evolutionary computation and deep learning, stand to be profoundly impacted by advances in open-endedness.

We have only a preliminary understanding of the breadth of open-ended systems,
the degrees of open-endedness that are possible, or the necessary conditions for triggering an open-ended complexity explosion.
The potential weapons in our arsenal—evolutionary computation, neuroevolution, deep learning,
alife worlds, novelty and divergence, coevolution and self-play, minimal criteria, etc., are powerful and compelling,
but we have only a faint understanding of how they fit together to compose the big picture.

>>Open-endedness defies the dominant paradigm in computer science and AI or machine learning today of
>> “problems” and “solutions.” 
In those fields, you can choose a problem and showcase improving results with respect to some benchmark. 
Open-endedness isn’t like that.
Open-endedness is for those yearning for an adventure without a clear destination
—it is the road of creation itself, and  >>>> the entire point is to generate what we presently cannot imagine. 
Welcome to the newest grand challenge.











