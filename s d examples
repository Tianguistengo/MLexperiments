https://openai.com/blog/dall-e/

DALL·E[1]

We decided to name our model using a portmanteau of the artist Salvador Dalí and Pixar’s WALL·E.
is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,
>> using a dataset of text–image pairs. 
We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects,
>> combining unrelated concepts in plausible ways, 
rendering text, and applying transformations to existing images.

Overview

Like GPT-3, DALL·E is a transformer language model. 
It receives both the text and the image as a single stream of data containing up to 1280 tokens,
and is trained using maximum likelihood to generate all of the tokens, one after another.

A token is any symbol from a discrete vocabulary;
for humans, each English letter is a token from a 26-letter alphabet.
DALL·E’s vocabulary has tokens for both text and image concepts. 
Specifically, each image caption is represented using a maximum of 256 BPE-encoded tokens with a vocabulary size of 16384,
and the image is represented using 1024 tokens with a vocabulary size of 8192.

This training procedure allows DALL·E to not only generate an image from scratch,
but also to regenerate any rectangular region of an existing image that extends to the bottom-right corner,
in a way that is consistent with the text prompt.

Drawing multiple objects

Simultaneously controlling multiple objects, their attributes, and their spatial relationships presents a new challenge.
For example, consider the phrase “a hedgehog wearing a red hat, yellow gloves, blue shirt, and green pants.” 
To correctly interpret this sentence, DALL·E must not only correctly compose each piece of apparel with the animal,
but also form the associations (hat, red), (gloves, yellow), (shirt, blue), and (pants, green) without mixing them up.[4]

This task is called variable binding, and has been extensively studied in the literature.17181920
We test DALL·E’s ability to do this for relative positioning, stacking objects, and controlling multiple attributes.

While DALL·E does offer some level of controllability over the attributes and positions of a small number of objects,
the success rate can depend on how the caption is phrased. 
As more objects are introduced, DALL·E is prone to confusing the associations between the objects and their colors,
and the success rate decreases sharply.

-Visualizing perspective and three-dimensionality

-Visualizing internal and external structure

-Inferring contextual details
Depending on the orientation of the capybara, it may be necessary to draw a shadow,
though this detail is never mentioned explicitly.
>We explore DALL·E’s ability to resolve underspecification in three cases: 
changing style, setting, and time; 
drawing the same object in a variety of different situations; 
and generating an image of an object with specific text written on it.

>>>> -Combining unrelated concepts

The compositional nature of language 
allows us to put together concepts to describe both real and imaginary things. 
We find that DALL·E also has the ability to combine disparate ideas to synthesize objects,
some of which are unlikely to exist in the real world. 
We explore this ability in two instances: transferring qualities from various concepts to animals,
and designing products by taking inspiration from unrelated concepts.

"an armchair in the shape of an avocado. an armchair imitating an avocado. "

-Animal illustrations

In the previous section, we explored DALL·E’s ability to combine unrelated concepts when generating images of real-world objects.
Here, we explore this ability in the context of >>>> art, for three kinds of illustrations:
anthropomorphized versions of animals and objects, animal chimeras, and emojis.

"an illustration of a baby daikon radish in a tutu walking a dog"

-Zero-shot visual reasoning

GPT-3 can be instructed to perform many kinds of tasks solely from a description 
and a cue to generate the answer supplied in its prompt, without any additional training. 
For example, when prompted with the phrase 
“here is the sentence ‘a person walking his dog in the park’ translated into French:”, 
GPT-3 answers “un homme qui promène son chien dans le parc.” This capability is called zero-shot reasoning.

Motivated by these results, we measure DALL·E’s aptitude for >>analogical reasoning problems 
by testing it on Raven’s progressive matrices, a visual IQ test that saw widespread use in the 20th century.

-Geographic knowledge

We find that DALL·E has learned about geographic facts, landmarks, and neighborhoods. 
Its knowledge of these concepts is surprisingly precise in some ways and flawed in others.

-Temporal knowledge

In addition to exploring DALL·E’s knowledge of concepts that vary over space, 
we also explore its knowledge of concepts that vary over time.
"a photo of a phone from the 20s"

Summary

DALL·E is a simple decoder-only transformer t
hat receives both the text and the image >>as a single stream of 1280 tokens—256 for the text and 1024 for the image—
and models all of them autoregressively. 
The attention mask at each of its 64 self-attention layers 
allows each image token to attend to all text tokens. 
DALL·E uses the standard causal mask for the text tokens, 
and sparse attention for the image tokens 
with either a row, column, or convolutional attention pattern, depending on the layer. 

Text-to-image synthesis 
has been an active area of research since the pioneering work of Reed et. al,
whose approach uses a GAN conditioned on text embeddings. 

we use CLIP to rerank the top 32 of 512 samples for each caption. 
This procedure can also be seen as a kind of language-guided search, 
and can have a dramatic impact on sample quality.

"an illustration of a baby daikon radish in a tutu walking a dog [caption 1, best 8 of 2048]" 

______________________

Quantum physicists like Roger Melko 
of the Perimeter Institute for Theoretical Physics and the University of Waterloo in Ontario 
have used neural networks to solve some of the toughest and most important problems in that field,
>>such as how to represent the mathematical “wave function” describing a many-particle system.

-----------------------

Using generative modeling, astrophysicists could investigate how galaxies change 
when they go from low-density regions of the cosmos to high-density regions,
and what physical processes are responsible for these changes.
Schawinski
https://www.quantamagazine.org/how-artificial-intelligence-is-changing-science-20190311/

-------------
Adam

Occasionally, grand claims are made regarding the achievements of a “robo-scientist.”
A decade ago, an AI robot chemist named Adam investigated the genome of baker’s yeast 
and worked out >>which genes are responsible for making certain amino acids.
(Adam did this by observing strains of yeast that had certain genes missing, 
and comparing the results to the behavior of strains that had the genes.) 
Wired’s headline read, “Robot Makes Scientific Discovery All by Itself.”

-----------

Lee Cronin

More recently, Lee Cronin, a chemist at the University of Glasgow, has been using a robot to
>>>>>>>>>>>>>>>>>>>randomly mix chemicals, to see what sorts of new compounds are formed.

Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine,
and an infrared spectrometer,
>>>>the system eventually learned to predict which combinations would be the most reactive. 

Even if it doesn’t lead to further discoveries, Cronin has said, 
the robotic system could allow chemists to >>>>>speed up their research by about 90 percent.













