The Master Algorithm: Here, the books’ central hypothesis is presented:

    ‘All knowledge — past, present, and future — 
    can be deduced from data 
    by a single, universal learning algorithm.’
    
 ---------------
 
 The possibility to imagine counterfactuals
 
 ---------------
 
 The ability to EXPLORE THE WORLD
 in the hope/with the task of learning - active learning
 
 We don't learn pixels, we learn fuzzy high level abstractons
 we need ai to generate features at the level of unobserverd latent variables 
 
 -------
 
 GANs (but not only) 
 
 estimate and maximize 
 MUTUAL INFORMATION
 
 ------------
 
 Neural nets are vector processing machines 
 
 images are seen as vectors with a topology
 
 Neural nets are function approximation algorithms
 
 --------------
 
 LATENT   space / variables 
 related to conscious /unconsicous separation


    (of a quality or state) 
    existing but not yet developed or manifest; hidden or concealed.
  

    (Biology)
    lying dormant or hidden until circumstances are suitable for development or manifestation.
    
----------------    
    
 AI has been developed mainly (but not exclusively) by a technical community
 but IMAGINATION - CREATIVITY - POETRY - INTUITION - BEAUTY - FUZZY THINKING - MAGIC
 will bring the biggest/fastest changes

-----------------
https://blogs.sciencemag.org/pipeline/archives/2019/10/02/automated-discovery


I think that the classification scheme in the paper is a useful one to start to deal with these objections.
They divide scientific discoveries impacted by automation into three categories: 

1. physical matter (a drug candidate, a new metal alloy, new crystal form, etc.),

2. processes (such as new chemical reactions), 

3. models (new laws, rules of thumb, correlations, and connections). 

The authors argue that all three of these are fundamentally 
>>search problems
– they just differ in the knowledge space being searched, 
which is a process of validation and feedback. 

That holds whether you’re talking about a hypothesis-first (Popperian) mode of discovery 
or an observation-first (Baconian) one; 
the difference between the two is (to a large extent) 
where you enter that cycle of observation and experimentation.

The paper makes the key point that in every example of machine-aided discovery so far, 
>>the search space has been far larger 
than what was (or even could be) explored. 

When you look closer, >>>>>    it’s human input that has narrowed the terms and the search space.

The authors also note the three factors that are enabling automation in all of these classes 
– access to large amounts of data, 
-the increasing computing power to process it all, 
-the advances in hardware to mechanically manipulate the physical tools of experimentation

Now one gets to the question of just how automated/autonomous things really are (or really can get):

    Here, we propose a set of questions to ask when evaluating the extent to which a discovery process or workflow is autonomous: 
    
    (i) How broadly is the goal defined? 
    (ii) How constrained is the search/design space? 
    (iii) How are experiments for validation/feedback selected? 
    (iv) How superior to a brute force search is navigation of the design space?
    (v) How are experiments for validation/feedback performed? 
    (vi) How are results organized and interpreted? 
    (vii) Does the discovery outcome contribute to broader scientific knowledge?


---------------
Maithra Raghu and Eric Schmid

1. From Predictions to Understanding 

One fundamental difference between scientific questions and core machine learning problems is the 
emphasis of science on understanding the underlying mechanisms.

>>interpretability and representational analysis,
a set of techniques focused on gaining insights into the internals of the end-to-end system: 
identifying important features in the data,
understanding its effect on model outputs 
and discovering properties of model hidden representations. 

These are very important for many scientific problems which emphasise understanding
over predictive accuracy, 
and may be of broader interest for e.g. aiding model debugging and preemptively identifying failure modes.

There has been significant work on both tools to understand 
what features of the input are most critical to the output prediction, 
as well as techniques to 
>>> directly analyze the hidden representations 
of the neural network models, 
which can reveal important properties of the underlying data.

2. Complex Transformations of Input Data 

In many scientific domains, the amount of generated data, particularly visual data 
(e.g. fluorescence microscopy, spatial sequencing, specimen videos [177, 97]) has grown dramatically, 
and there is an urgent need for 
efficient analysis 
and automated processing. 
Deep learning techniques, which are capable of many complex transformations of data, 
can be highly effective for such settings, for example, using a deep neural network based segmentation model to automatically
identify the nuclei in images of cells, 
or a pose estimation system to rapidly label behaviors seen in videos of mice for neuroscience analysis.

3. Prediction Problems
We can also think of this predictive use case as 
>>>>>getting the model to learn a target function,
in our example, mapping from input visual features to the cancer/no cancer output.

Using deep learning in this way also encapsulates 
>> settings where the target function is very complex, 
with no mathematical closed form or logical set of rules that describe how to go from input to output. 

For instance, we might use a deep learning model to (black-box) simulate a complex process
(e.g. climate modelling), that is very challenging to explicitly model.


6.2.4 Co-training

Another way to provide feedback on unlabelled data
is to train two (many) neural network models, >> each on a different view of the raw data.

For example, with text data, each model might see a different part of the input sentence. 
These models can then be given feedback to be maximally consistent with each other,
or with a different model which sees all of the data, 
or even used for self-training,
with each different model providing pseudo labels on the instances it is most confident on.

This post https://ruder.io/semi-supervised/ gives a nice overview of different co-training schemes.


------------
Uni camb

robot-scientist-becomes-first-machine-to-discover-new-scientific-knowledge
2009
https://www.cam.ac.uk/research/news/robot-scientist-becomes-first-machine-to-discover-new-scientific-knowledge

"As we start to consider living systems in a holistic manner, 
the complexity of such systems 
means that it will become increasingly difficult for scientists 
to formulate hypotheses unaided. 

Thus it will be necessary
for human and robot scientists to work together 
to achieve the goals of biological research.

"It is not the management and analysis of complex data 
that is the big deal about Adam, 
it is the ability of the machine to reason with those data 
>>>>>and make proposals about how a living thing works."

Prof Ross King, who led the research at Aberystwyth University, said: 
"Ultimately we hope to have teams of human and robot scientists working together in laboratories. 
Because biological organisms are so complex 
it is important that the details of biological experiments (y yo diria mined tambien) are recorded in great detail. 
This is difficult and irksome for human scientists, but easy for Robot Scientists."



Artificially-intelligent Robot Scientist ‘Eve’ could boost search for new drugs
2015
https://www.cam.ac.uk/research/news/artificially-intelligent-robot-scientist-eve-could-boost-search-for-new-drugs

>>>>>>>> they describe how the robot can help identify promising new drug candidates for
>>>>>>>.malaria and neglected tropical diseases such as African sleeping sickness and Chagas’ disease.

Eve, an artificially-intelligent ‘robot scientist’ 
could make drug discovery
faster 
and much cheaper.

Discovered that a compound shown to have anti-cancer properties 
might also be used in the fight against malaria.

[Eve's artificial intelligence] reduces the 
costs,
uncertainty, 
and time involved in drug screening.

They can automatically 
develop and test hypotheses to explain observations, 
run experiments using laboratory robotics, 
interpret the results to amend their hypotheses, 
and then repeat the cycle, 
automating high-throughput hypothesis-led research. 

Robot scientists are also well suited to 
>> recording scientific knowledge: 
as the experiments are conceived 
and executed 
automatically by computer, 
it is possible to completely capture
and digitally curate 
>> all aspects of the scientific process.

 help identify promising new drug candidates 
 
 Eve exploits its artificial intelligence to 
 learn from early successes in her screens 
 and select compounds that have a high probability of being active 
 against the chosen drug target.
 
  A smart screening system, 
  based on genetically engineered yeast, is used.
  This allows Eve to exclude compounds that are toxic to cells 
  and select those that block the action of the parasite protein while leaving any equivalent human protein unscathed. 
  This reduces the costs, uncertainty, and time involved in drug screening.

Eve is designed to automate early-stage drug design. 
First, she systematically tests each member from a large set of compounds 
in the standard brute-force way of conventional mass screening. 

The compounds are screened against assays (tests) designed to be automatically engineered, 
and can be generated much faster and more cheaply than the bespoke assays that are currently standard. 

This enables more types of assay to be applied, 
more efficient use of screening facilities to be made, 
and thereby increases the probability of a discovery within a given budget.
 
 
To test the viability of the approach, 
the researchers developed assays targeting key molecules from parasites 
responsible for diseases such as malaria, Chagas’ disease and schistosomiasis 
and tested against these a library of approximately 1,500 clinically approved compounds. 

Through this, Eve showed that a compound that has previously been investigated as an anti-cancer drug
inhibits a key molecule known as DHFR in the malaria parasite. 

Drugs that inhibit this molecule are currently routinely used to protect against malaria,
and are given to over a million children; 
however, the emergence of strains of parasites resistant to existing drugs means that 
the search for new drugs is becoming increasingly more urgent.

“Despite extensive efforts, no one has been able to find a new antimalarial 
that targets DHFR and is able to pass clinical trials,” adds Professor King.

“Eve’s discovery could be even more significant than just demonstrating a new approach to drug discovery.”















