The Master Algorithm: Here, the books’ central hypothesis is presented:

    ‘All knowledge — past, present, and future — 
    can be deduced from data 
    by a single, universal learning algorithm.’
    
 ---------------
 
 The possibility to imagine counterfactuals
 
 ---------------
 
 The ability to EXPLORE THE WORLD
 in the hope/with the task of learning - active learning
 
 We don't learn pixels, we learn fuzzy high level abstractons
 we need ai to generate features at the level of unobserverd latent variables 
 
 -------
 
 GANs (but not only) 
 
 estimate and maximize 
 MUTUAL INFORMATION
 
 ------------
 
 Neural nets are vector processing machines 
 
 images are seen as vectors with a topology
 
 Neural nets are function approximation algorithms
 
 --------------
 
 LATENT   space / variables 
 related to conscious /unconsicous separation


    (of a quality or state) 
    existing but not yet developed or manifest; hidden or concealed.
  

    (Biology)
    lying dormant or hidden until circumstances are suitable for development or manifestation.
    
----------------    
    
 AI has been developed mainly (but not exclusively) by a technical community
 but IMAGINATION - CREATIVITY - POETRY - INTUITION - BEAUTY - FUZZY THINKING - MAGIC
 will bring the biggest/fastest changes

-----------------
https://blogs.sciencemag.org/pipeline/archives/2019/10/02/automated-discovery


I think that the classification scheme in the paper is a useful one to start to deal with these objections.
They divide scientific discoveries impacted by automation into three categories: 

1. physical matter (a drug candidate, a new metal alloy, new crystal form, etc.),

2. processes (such as new chemical reactions), 

3. models (new laws, rules of thumb, correlations, and connections). 

The authors argue that all three of these are fundamentally 
>>search problems
– they just differ in the knowledge space being searched, 
which is a process of validation and feedback. 

That holds whether you’re talking about a hypothesis-first (Popperian) mode of discovery 
or an observation-first (Baconian) one; 
the difference between the two is (to a large extent) 
where you enter that cycle of observation and experimentation.

The paper makes the key point that in every example of machine-aided discovery so far, 
>>the search space has been far larger 
than what was (or even could be) explored. 

When you look closer, >>>>>    it’s human input that has narrowed the terms and the search space.

The authors also note the three factors that are enabling automation in all of these classes 
– access to large amounts of data, 
-the increasing computing power to process it all, 
-the advances in hardware to mechanically manipulate the physical tools of experimentation

Now one gets to the question of just how automated/autonomous things really are (or really can get):

    Here, we propose a set of questions to ask when evaluating the extent to which a discovery process or workflow is autonomous: 
    
    (i) How broadly is the goal defined? 
    (ii) How constrained is the search/design space? 
    (iii) How are experiments for validation/feedback selected? 
    (iv) How superior to a brute force search is navigation of the design space?
    (v) How are experiments for validation/feedback performed? 
    (vi) How are results organized and interpreted? 
    (vii) Does the discovery outcome contribute to broader scientific knowledge?


---------------
Maithra Raghu and Eric Schmid

1. From Predictions to Understanding 

One fundamental difference between scientific questions and core machine learning problems is the 
emphasis of science on understanding the underlying mechanisms.

>>interpretability and representational analysis,
a set of techniques focused on gaining insights into the internals of the end-to-end system: 
identifying important features in the data,
understanding its effect on model outputs 
and discovering properties of model hidden representations. 

These are very important for many scientific problems which emphasise understanding
over predictive accuracy, 
and may be of broader interest for e.g. aiding model debugging and preemptively identifying failure modes.

There has been significant work on both tools to understand 
what features of the input are most critical to the output prediction, 
as well as techniques to 
>>> directly analyze the hidden representations 
of the neural network models, 
which can reveal important properties of the underlying data.

2. Complex Transformations of Input Data 

In many scientific domains, the amount of generated data, particularly visual data 
(e.g. fluorescence microscopy, spatial sequencing, specimen videos [177, 97]) has grown dramatically, 
and there is an urgent need for 
efficient analysis 
and automated processing. 
Deep learning techniques, which are capable of many complex transformations of data, 
can be highly effective for such settings, for example, using a deep neural network based segmentation model to automatically
identify the nuclei in images of cells, 
or a pose estimation system to rapidly label behaviors seen in videos of mice for neuroscience analysis.

3. Prediction Problems
We can also think of this predictive use case as 
>>>>>getting the model to learn a target function,
in our example, mapping from input visual features to the cancer/no cancer output.

Using deep learning in this way also encapsulates 
>> settings where the target function is very complex, 
with no mathematical closed form or logical set of rules that describe how to go from input to output. 

For instance, we might use a deep learning model to (black-box) simulate a complex process
(e.g. climate modelling), that is very challenging to explicitly model.


6.2.4 Co-training

Another way to provide feedback on unlabelled data
is to train two (many) neural network models, >> each on a different view of the raw data.

For example, with text data, each model might see a different part of the input sentence. 
These models can then be given feedback to be maximally consistent with each other,
or with a different model which sees all of the data, 
or even used for self-training,
with each different model providing pseudo labels on the instances it is most confident on.

This post https://ruder.io/semi-supervised/ gives a nice overview of different co-training schemes.


------------
Uni camb

robot-scientist-becomes-first-machine-to-discover-new-scientific-knowledge
2009
https://www.cam.ac.uk/research/news/robot-scientist-becomes-first-machine-to-discover-new-scientific-knowledge

"As we start to consider living systems in a holistic manner, 
the complexity of such systems 
means that it will become increasingly difficult for scientists 
to formulate hypotheses unaided. 

Thus it will be necessary
for human and robot scientists to work together 
to achieve the goals of biological research.

"It is not the management and analysis of complex data 
that is the big deal about Adam, 
it is the ability of the machine to reason with those data 
>>>>>and make proposals about how a living thing works."

Prof Ross King, who led the research at Aberystwyth University, said: 
"Ultimately we hope to have teams of human and robot scientists working together in laboratories. 
Because biological organisms are so complex 
it is important that the details of biological experiments (y yo diria mined tambien) are recorded in great detail. 
This is difficult and irksome for human scientists, but easy for Robot Scientists."



Artificially-intelligent Robot Scientist ‘Eve’ could boost search for new drugs
2015
https://www.cam.ac.uk/research/news/artificially-intelligent-robot-scientist-eve-could-boost-search-for-new-drugs

>>>>>>>> they describe how the robot can help identify promising new drug candidates for
>>>>>>>.malaria and neglected tropical diseases such as African sleeping sickness and Chagas’ disease.

Eve, an artificially-intelligent ‘robot scientist’ 
could make drug discovery
faster 
and much cheaper.

Discovered that a compound shown to have anti-cancer properties 
might also be used in the fight against malaria.

[Eve's artificial intelligence] reduces the 
costs,
uncertainty, 
and time involved in drug screening.

They can automatically 
develop and test hypotheses to explain observations, 
run experiments using laboratory robotics, 
interpret the results to amend their hypotheses, 
and then repeat the cycle, 
automating high-throughput hypothesis-led research. 

Robot scientists are also well suited to 
>> recording scientific knowledge: 
as the experiments are conceived 
and executed 
automatically by computer, 
it is possible to completely capture
and digitally curate 
>> all aspects of the scientific process.

 help identify promising new drug candidates 
 
 Eve exploits its artificial intelligence to 
 learn from early successes in her screens 
 and select compounds that have a high probability of being active 
 against the chosen drug target.
 
  A smart screening system, 
  based on genetically engineered yeast, is used.
  This allows Eve to exclude compounds that are toxic to cells 
  and select those that block the action of the parasite protein while leaving any equivalent human protein unscathed. 
  This reduces the costs, uncertainty, and time involved in drug screening.

Eve is designed to automate early-stage drug design. 
First, she systematically tests each member from a large set of compounds 
in the standard brute-force way of conventional mass screening. 

The compounds are screened against assays (tests) designed to be automatically engineered, 
and can be generated much faster and more cheaply than the bespoke assays that are currently standard. 

This enables more types of assay to be applied, 
more efficient use of screening facilities to be made, 
and thereby increases the probability of a discovery within a given budget.
 
 
To test the viability of the approach, 
the researchers developed assays targeting key molecules from parasites 
responsible for diseases such as malaria, Chagas’ disease and schistosomiasis 
and tested against these a library of approximately 1,500 clinically approved compounds. 

Through this, Eve showed that a compound that has previously been investigated as an anti-cancer drug
inhibits a key molecule known as DHFR in the malaria parasite. 

Drugs that inhibit this molecule are currently routinely used to protect against malaria,
and are given to over a million children; 
however, the emergence of strains of parasites resistant to existing drugs means that 
the search for new drugs is becoming increasingly more urgent.

“Despite extensive efforts, no one has been able to find a new antimalarial 
that targets DHFR and is able to pass clinical trials,” adds Professor King.

“Eve’s discovery could be even more significant than just demonstrating a new approach to drug discovery.”


--------------
AI Makes New Scientific Discoveries by Analyzing Old Research Papers
"Unsupervised word embeddings capture latent knowledge from materials science literature"

https://thenewstack.io/ai-makes-new-scientific-discoveries-by-analyzing-old-research-papers/

orignal paper - https://www.researchgate.net/publication/334209824_Unsupervised_word_embeddings_capture_latent_knowledge_from_materials_science_literature

the team gathered 3.3 million abstracts from scientific papers published in over 1,000 journals between 1922 and 2018.
The algorithm then processed the roughly 500,000 unique words found in the abstracts 
and transformed each into an array of 200 vectors. 
Though the AI had no prior training in materials science, after this process it was nevertheless able to ‘learn’
scientific concepts and
infer relationships between data points, 
simply by analyzing the >>placement of words in the abstracts 
and when they >>co-occur with one another.

these embeddings capture complex materials science concepts 
such as the underlying structure of the periodic table 
and structure–property relationships in materials.

" you can use this algorithm to >> address gaps 
in materials research, things that people 
>>should study but haven’t studied so far.”

In particular, the algorithm proved that it could 
>> predict novel thermoelectric materials,
which convert heat to electricity efficiently. 
During the team’s tests, the algorithm came up with a variety of predictions for possible thermoelectric materials,
with the top ten predictions demonstrating higher-than-average thermoelectric properties

it works in an unsupervised capacity to find novel connections that might have been overlooked
— perhaps even years ahead of time.
Moreover, according to the team, this method could be used to automatically 
extract knowledge that is still hidden in older scientific papers,
and which might not be apparent to human eyes.

Furthermore, we demonstrate that an unsupervised method 
can recommend materials for functional applications several years before their discovery.

>>>This suggests that LATENT KNOWLEDGE regarding future discoveries 
>>>is to a large extent embedded in past publications. 

Our findings highlight the possibility of extracting knowledge and relationships 
from the massive body of scientific literature in a collective manner,
>>>>>and point towards a generalized approach to the mining of scientific literature.


----------
https://www.graphcore.ai/posts/why-artificial-intelligence-will-allow-us-to-make-new-scientific-discoveries

Why AI will enable new scientific discoveries

Andrew Briggs
is Professor of Nanomaterials in the Department of Materials at the University of Oxford

In materials science, as in other branches of experimental science such as drug discovery,
we are able to obtain ever larger amounts of data from ever more sophisticated experiments and modelling. 
And yet 
>>> the choice of what data to collect, 
>>> and the process of how best to analyse it, 
at times seem like a cottage industry compared to the sophistication of internet searches at Google or Baidu. 
Our current approaches to data in science fall well short of 
the advanced machine learning techniques social media platforms use to recognize the friend in a photo.

I predict that by 2025, 
AI will be as ubiquitous for >> running experiments 
as computers are today for controlling instrumentation and logging data. 

The paradigm shift will be from AI used for analysing the data which has already been obtained, 
>>> to AI deciding what to measure next. 

The key advances in AI that we are currently exploring to make this possible are
reinforcement learning 
and Bayesian optimisation.

Uncertainty, 
which could be reduced by acquiring more data,
but the data are costly to acquire. 
How should we choose what experiment to do next?

>>These same methods can also be used to optimise a system for a given set of criteria.
By analysing the results of new experiments and new material discoveries,
>>we can further tune our approaches, 
saving time, money and improving outcomes.

 Initially this can be applied to prototypes which have already been made, 
 but eventually it can be applied to >> the whole design 
 and even to >> the manufacturing process. 
 
 This could make a significant difference in the cost of production for advanced materials.

(AI) will allow us to 
> understand 
>and explore 
>the use 
>of new materials 
>and new techniques. 

More importantly, 
new machine learning techniques applied to scientific discovery will quickly 
>>enable complex research challenges 
to be addressed that could never be solved by humans alone working 
within feasible time limits and resources.

--------------------

https://news.uchicago.edu/story/how-ai-could-change-science

     “While the tech industry is often focused on short-term returns,
     realizing the full potential of AI to improve our world 
     requires long-term vision.” 

—Prof. Rebecca Willett

New materials
essentially are just >>different combinations<< of chemicals and molecules, 
but there are literally billions of such combinations. 
How do scientists pick which ones to make and test in the labs? AI could quickly narrow down the list.

“There are many areas where the Edisonian approach
—that is, having an army of assistants make and test hundreds of different options for the lightbulb—
just isn’t practical,” Ferguson said.

Then there’s the question of what happens if AI takes a turn at
>>> being the scientist. 

Some are wondering whether 
>>AI models could propose 
new experiments that 
>>might never have occurred to their human counterparts.

---------

https://www.the-scientist.com/reading-frames/can-artificial-intelligence-make-scientific-discoveries--65790

Swanson and University of Illinois at Chicago psychiatry professor Neil Smalheiser 
developed a computer program called Arrowsmith 
that plucked out such hypotheses from medical research databases, 
>>> with a focus on theories generated out of links between disparate specialties. 

Swanson later hypothesized a relationship between magnesium deficiency and migraine headaches 
that was also supported by subsequent clinical research.

Over the years, Arrowsmith has had limited effect, but Swanson’s early foray suggests that 
>> finding relationships in data from disparate fields 
>> can help tap into undiscovered knowledge hidden in data. 

Although Swanson’s efforts were fully manual,
such a process can indeed be automated to help uncover knowledge that scientists might not have discovered yet.

------
Limits of deep learning

Lex & F Chollet

 you've talked about local versus extreme generalization 
 
 you mentioned the neural
networks don't generalize well humans do

so there's this gap so and you've also
00:26
mentioned that generalization extreme
00:28
generalization requires something like a
00:30
reasoning to fill those gaps 


it's not obvious but I
think it's doable

there is no hard limitations to
what you can learn with a deep neural network as long as 
the search space is rich enough is flexible enough
and as long as 
you have this dense sampling of the input cross output space

the problem is that you know distance
sampling could mean anything from 10,000
examples to like trillions and trillions 


think what kind of problems can be
solved by getting a huge amounts of data
and thereby creating a dense mapping


----------

Can scientific discovery be automated?

https://www.theatlantic.com/science/archive/2017/04/can-scientific-discovery-be-automated/524136/

Challenges of too much quantity (of research)
andthe finite neurological capacity of the human mind. 

Scientists are deriving hypotheses from a smaller and smaller fraction of our collective knowledge
and consequently, more and more, asking the wrong questions, or >> asking ones that have already been answered.

Human creativity is influenced by the stochasticity of previous experiences
—particular life events that allow a researcher to notice something others do not.

It could even begin another scientific revolution. That huge possibility hinges on an equally huge question: 
>>> Can scientific discovery really be automated?

The first reiterations of the scientific method can be traced back many centuries earlier
to Muslim thinkers such as Ibn al-Haytham, who emphasized both empiricism and experimentation. 
However, it was Francis Bacon who first formalized the scientific method and made it a subject of study. 
In his book Novum Organum (1620), he proposed a model for discovery that is still known as the Baconian method.
He argued against syllogistic logic for scientific synthesis, which he considered to be unreliable. 
Instead, he proposed an approach in which relevant observations about a specific phenomenon are systematically collected,
tabulated and objectively analyzed using inductive logic to generate generalizable ideas. 
In his view, truth could be uncovered only when the mind is free from incomplete (and hence false) axioms.

The Baconian method attempted to >remove logical bias from the process of observation and conceptualization,
by delineating the steps of scientific synthesis and optimizing each one separately. 

>>Bacon’s vision was to leverage a community of observers to collect vast amounts of information about nature
and tabulate it into a central record accessible to inductive analysis. 

The Baconian method is rarely used today. It proved too laborious and extravagantly expensive.

However, at the time the formalization of a scientific method marked a revolutionary advance. 
Before it, science was metaphysical, accessible only to a few learned men, mostly of noble birth. 
By delineating the steps of discovery, 
Bacon created a blueprint that would allow anyone, regardless of background, to become a scientist.

Bacon’s insights also revealed an important hidden truth: the discovery process is inherently algorithmic.
It is the outcome of a finite number of steps that are repeated until a meaningful result is uncovered. 
Bacon explicitly used the word “machine” in describing his method.
His scientific algorithm has three essential components: 
First, observations have to be collected and integrated into the total corpus of knowledge.
Second, the new observations are used to generate new hypotheses. 
Third, the hypotheses are tested through carefully designed experiments.

>> IFFF science is algorithmic, then it must have the potential for automation. 


---------

https://michaelnielsen.org/blog/the-artist-and-the-machine/

Water in Suspense reveals a hidden world. 
We discover a rich structure immanent in the water droplet, a structure not ordinarily accessible to our senses.
In this way it’s similar to the Hubble Extreme Deep Field, which also reveals a hidden world. 

Both are examples of what I call Super-realist art, art which doesn’t just capture what we can see directly 
with our eyes or hear with our ears, but which uses new sensors and methods of visualization to reveal a world
that we cannot directly perceive. It’s art being used to reveal science.

Suppose, for example, we could zoom in on his face, seeing it at 10 times magnification, then 100, then 1,000, and so on.
At each level of magnification a new world would be revealed, of pores and cells and microbes. 
And there is so much structure to understand in each of these worlds, 
from the incredibly complex ecology of microbes on the skin, to the inner workings of cells. 

 Virtuosity becomes about revealing hidden worlds and discovering new aesthetics. 
 Take a look at the following video, which shows — in extreme slow motion! — a packet of light passing through a bottle.
 Ordinarily that passage takes less than a nanosecond, but the video slows the passage down by a factor of 10 billion, 
 so we can see it.
 
 In a way, Super-realism is a return to the aims of Realism, back to representing reality. 
 But what’s new about Super-realism is the challenge (and opportunity!) 
 to find ways of representing previously unseen parts of the world.


-------------

Scientific intuition inspired by machine learning generated hypotheses

Pascal Friederich, Mario Krenn, Isaac Tamblyn,and Alan Aspuru-Guzik

>>>>>>>>>>>chingos de good papers en REFERENCES  -> subraye los que se ven buenos

A key challenge lies in >> the design of experiments
which creates certain desired quantum systems. 

The difficulty arises from >>counter-intuitive quantum phenomena, 
which raises the question of >> whether human intuition is the best way to design new experiments. 

Several studies have therefore developed automated and machine-learning augmented approaches 
>for the design of experiments [39–44]. 
The goal in our approach is to tackle this challenge in a completely different way, 
>> namely by improving the scientist’s intuition about these systems.

Logically combined features:
We can logically combine graph features, and find the most significant macro-features for quantum experiments.
In Fig. 7a, two small sub-experiments are combined with a logical -AND-,
i.e. the feature is the combination of both structures.

Individually, the presence of the first feature has a negative influence on nQ.
The second feature, a parity sorter followed by two detectors, influences nQ positively.
Surprisingly, their combination has a significant negative influence on nQ 
and can be seen as an almost sufficient condition for nQ ≈ 4 .

IV. CONCLUSION AND OUTLOOK

We presented a data-driven machine learning workflow for
automated generation 
and verification 
of hypotheses about observations in natural sciences. 

We presented examples from chemistry and physics, but our method is directly applicable to most applications, 
>> where structures can be represented as graphs, 
e.g. to DNA/RNA data in biology [50, 51], chemical reaction networks [52, 53] or graphs in social sciences.

In chemistry, the workflow “rediscovers” widely known relations regarding solubility and electronic properties of molecules
(often referred to as chemical intuition). 

In physics, the algorithm discovers rules 
to generate highly entangled three-photon states in quantum optical experiments. 

These rules are interpretable by human experts in retrospect, yet not known or postulated before,
>> and even contradicting some of the field’s current understanding. 

Finding such rules will not only help researchers to >> understand complex scientific relationships
>>and thus design better experiments, 
>> but also reduce unavoidable and often undetectable bias 
generated by prior knowledge and anticipations.

For hypothesis verification,
The second option is based on finding other experimental setups within the whole database 
that are as similar to the reference experiment as possible, 
with the exception of the feature that is currently analysed. 
This procedure is computationally costly as well but does not require new computations.



















