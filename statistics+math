
STANDARD DEVIATION 
is a number used to tell 
how measurements for a group are spread out from the average (mean), or expected value. 
A low standard deviation means that most of the numbers are close to the average.
A high standard deviation means that the numbers are more spread out.


------------------->  Derivatives/Math

In mathematics, the derivative is a way to --->  show rate of change:
that is,  --->  the amount by which a function is changing at_one_given_point.

For functions that act on the real numbers, 
it is --->  the slope of the tangent line at a point on a graph. 

The derivative is often written using "dy over dx" (meaning the difference in y divided by the difference in x). 

A function (black) and a tangent (red).
The derivative at the point is the slope of the tangent.




----->  Sigmoid Function 
by google glossary
(s-shaped , used by biological neurons)

A function that maps logistic or multinomial regression output (log odds) to probabilities,
returning a value between 0 and 1. 

In some neural networks, the sigmoid function acts as the >>activation function.

The sigmoid function is used for the -->  two-class logistic regression, 
whereas the softmax function is used for the -->  multiclass logistic regression

Getting to the point, the basic practical difference between Sigmoid and Softmax
is that while both give output in [0,1] range,
softmax ensures that the sum of outputs along channels (as per specified dimension) is 1 i.e., they are probabilities.
Sigmoid just makes output between 0 to 1.



----CUDA
CUDA (Computer Unified Device Architecture)
is NVIDIA's parallel computing architecture 
that enables dramatic increases in computing performance by harnessing the power of the GPU.


-
16 bit precision

 revision of IEEE 754, published in 2008, defines a floating point format that occupies only 16 bits. 
 Known as binary16, it is primarily intended to reduce storage and memory bandwidth requirements. 
 Since it provides only "half" precision, its use for actual computation is problematic.
 
  It is intended for storage of floating-point values in applications where
  >higher precision is not essential for performing arithmetic computations. 
  
  
  
  -
  What is Gradient Clipping?

Gradient clipping is a technique to >>> prevent exploding gradients.
There are many ways to compute gradient clipping,
but a common one is to rescale gradients so that their norm is at most a particular value. 

With gradient clipping, pre-determined gradient threshold be introduced,
and  then gradients norms that exceed this threshold are scaled down to match the norm.  
This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped. 

There is an introduced bias in the resulting values from the gradient, 
but gradient clipping can keep things stable. 

Why is this Useful?
Training recurrent neural networks can be very difficult. 
>>Two common issues with training recurrent neural networks are vanishing gradients and exploding gradients.


-
Checkpointing

is a technique that provides fault tolerance for computing systems. 
It basically consists of saving a snapshot of the application's state,
so that applications can restart from that point in case of failure.

-
In computer science, ------>>>>>>>>Monte Carlo tree search (MCTS) 

is a heuristic search algorithm for some kinds of decision processes, 
most notably those employed in game play. 
MCTS was introduced in 2006 for computer Go.
It has been used in other board games like chess and shogi,
games with incomplete information such as bridge and poker,
as well as in turn-based-strategy video games

Monte Carlo Tree Search is a method usually used in games 
to predict the path (moves) that should be taken by the policy to reach the final winning solution.


--- 
NAIVES BAYES

why is it called naives?


---
Difference between gradients and derivatives

The gradient is a vector; it points in the direction of steepest ascent.

The directional derivative is a number; it is the rate of change when your point in ℝ3
moves in that direction. 

Be careful that directional derivative of a function is a scalar while gradient is a vector.

The only difference between derivative and directional derivative is the definition of those terms. Remember:

    Directional derivative is the instantaneous rate of change (which is a scalar) of 𝑓(𝑥,𝑦)
     in the direction of the unit vector 𝑢

   Derivative is the rate of change of 𝑓(𝑥,𝑦)
   , which can be thought of the slope of the function at a point (𝑥0,𝑦0).
   
   
   -------
   
    Church–Turing thesis (also known as computability thesis)
    
     is a hypothesis about >>the nature of computable functions. 
     
     It states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine.
   
   The Church-Turing thesis (CTT)
   underlies tantalizing open questions concerning >>the fundamental place of computing in the physical universe.
   
   For example, is every physical system computable?
   Is the universe essentially computational in nature? 
   What are the implications for computer science of recent speculation about physical uncomputability? 
   Does CTT place a fundamental logical limit on what can be computed, a computational "barrier" that cannot be broken,
   no matter how far and in what multitude of ways computers develop? 
   Or could new types of hardware, based perhaps on quantum or relativistic phenomena, lead to radically new computing paradigms 
   that do breach the Church-Turing barrier, in which the uncomputable becomes computable, in an upgraded sense of "computable"?
   
   Before addressing these questions, we first look back to the 1930s to consider how Alonzo Church and Alan Turing formulated, and sought to justify, 
   their versions of CTT. With this necessary history under our belts, 
   >we then turn to today's dramatically more powerful versions of CTT.
   
   ---
   
   What is quantization?

Quantization isn’t something new, it has been around for decades since the creation of digital electronics. 
When we take photo on our cellphone, the real world scene which is analog is captured by the camera and digitized into computer files. 

Quantization is part of that process that convert a continuous data (can be infinitely small or big)
to discrete numbers within a fixed range, say numbers 0, 1, 2, .., 255 which are commonly used in digital image files.

In deep learning, quantization generally refers to converting from floating point (with dynamic range of the order of 1^-38 to 1x10³⁸)
to fixed point integer (e.g. 8-bit integer between 0 and 255).

Some information will be lost in quantization 
but researches show that with tricks in training, the loss in accuracy is manageable.

Why quantization?

There are two main reasons.
Deep neural network consists of many parameters which are known as weights, for example, the famous VGG network has over 100 million parameters!! 
In many scenarios, the bottleneck of running deep neural network is in transferring the weights and data between main memory and compute cores. 

Therefore, as opposed to GPU that have one large shared pool of memory,
modern AI chips have a lot of local memories around the compute cores to minimize the data transfer latency.

Now, by using 8-bit integer rather than 32-bit, we instantly speed up the memory transfer by 4x!
This also bring other benefits that accompanies smaller file size i.e. less memory storage, faster download time etc.
   

--

Tarski's    undefinability theorem


Tarski's undefinability theorem, stated and proved by Alfred Tarski in 1933, 
is an important limitative result in mathematical logic, 
the foundations of mathematics, and in formal semantics. 

Informally, the theorem states that arithmetical truth cannot be defined in arithmetic.

The theorem applies more generally to any sufficiently strong formal system, 
showing that 
>>> truth in the standard model of the system 
>>>cannot be defined within the system. 

--

The Halting problem -- Turing

In computability theory, the halting problem is >>the problem of determining,
from a description of an arbitrary computer program and an input, 
>>whether the program will finish running, or continue to run forever. 

Alan Turing proved in 1936 that 
>a general algorithm to solve the halting problem for all possible program-input pairs 
>cannot exist.
 
 
--

Gödel's incompleteness theorems 


are two theorems of mathematical logic that 
>>demonstrate the inherent limitations of every formal axiomatic system capable of modelling basic arithmetic. 

These results, published by Kurt Gödel in 1931, are important both in mathematical logic and in the philosophy of mathematics. 
The theorems are widely, but not universally, interpreted as 
>>showing that Hilbert's program to find a complete and consistent set of axioms for all mathematics is impossible.

1. The first incompleteness theorem states that 
>>no consistent system of axioms 
whose theorems can be listed by an effective procedure (i.e., an algorithm) 
>>is capable of proving all truths about the arithmetic of natural numbers. 

For any such consistent formal system, 
there will always be statements about natural numbers that are true, 
>but that are unprovable within the system. 


2.The second incompleteness theorem, an extension of the first, 
>>shows that the system cannot demonstrate its own consistency.


Employing a diagonal argument, 
Gödel's incompleteness theorems were the first of several closely related theorems
>>on the limitations of formal systems.

>They were followed by Tarski's undefinability theorem on the formal undefinability of truth,
>Church's proof that Hilbert's Entscheidungsproblem is unsolvable, 
>and Turing's theorem that there is no algorithm to solve the halting problem
   
   
 --
 Entscheidungsproblem


In mathematics and computer science, the Entscheidungsproblem 
(pronounced [ɛntˈʃaɪ̯dʊŋspʁoˌbleːm], German for "decision problem") 
is a challenge posed by David Hilbert and Wilhelm Ackermann in 1928.

[1] The problem asks for an algorithm that considers, as input, 
a statement
and answers "Yes" or "No" according to whether the statement is universally valid
, i.e., valid in every structure satisfying the axioms. 
   
   
  --
  
  Using-neural-networks-to-solve-advanced-mathematics-equations
  
https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/

Building this data set required us to incorporate a range of data cleaning and generation techniques.
For our symbolic integration equations, for example, we flipped the translation approach around:
Instead of generating problems and finding their solutions, 
we generated solutions and found their problem (their derivative), 
which is a much easier task. 

This approach of generating problems from their solutions — what engineers sometimes refer to as trapdoor problems — 
made it feasible to create millions of integration examples. 
Our resulting translation-inspired data set consists of roughly 100M paired examples,
with subsets of integration problems as well as first- and second-order differential equations.
   
   
   
   --
   
   FLOPS
   
   Short for floating-point operations per second, 
   a common benchmark measurement for rating the speed of microprocessors. 
   
   Floating-point operations include any operations that involve fractional numbers. 
   Such operations, which take much longer to compute than integer operations, occur often in some applications.

Most modern microprocessors include a floating-point unit (FPU),
which is a specialized part of the microprocessor responsible for executing floating-point operations. 
The FLOPS measurement, therefore, actually measures the speed of the FPU. 
One of the most common benchmark tests used to measure FLOPS is called Linpack.

Many experts feel that FLOPS is not a relevant measurement because it fails to take into account factors 
such as the condition under which the microprocessor is running (e.g., heavy or light loads) 
and which exact operations are included as floating-point operations. 
For this reason, a consortium of vendors created the Standard Performance Evaluation Corporation (SPEC), 
which provides more meaningful benchmark values. 
   
   
   ---
   
    Scalar functions
    
    (sometimes referred to as User-Defined Functions / UDFs)
    return a single value as a return value, not as a result set.


