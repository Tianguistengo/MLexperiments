
STANDARD DEVIATION 
is a number used to tell 
how measurements for a group are spread out from the average (mean), or expected value. 
A low standard deviation means that most of the numbers are close to the average.
A high standard deviation means that the numbers are more spread out.


------------------->  Derivatives/Math

In mathematics, the derivative is a way to --->  show rate of change:
that is,  --->  the amount by which a function is changing at_one_given_point.

For functions that act on the real numbers, 
it is --->  the slope of the tangent line at a point on a graph. 

The derivative is often written using "dy over dx" (meaning the difference in y divided by the difference in x). 

A function (black) and a tangent (red).
The derivative at the point is the slope of the tangent.




----->  Sigmoid Function 
by google glossary
(s-shaped , used by biological neurons)

A function that maps logistic or multinomial regression output (log odds) to probabilities,
returning a value between 0 and 1. 

In some neural networks, the sigmoid function acts as the >>activation function.

The sigmoid function is used for the -->  two-class logistic regression, 
whereas the softmax function is used for the -->  multiclass logistic regression

Getting to the point, the basic practical difference between Sigmoid and Softmax
is that while both give output in [0,1] range,
softmax ensures that the sum of outputs along channels (as per specified dimension) is 1 i.e., they are probabilities.
Sigmoid just makes output between 0 to 1.



----CUDA
CUDA (Computer Unified Device Architecture)
is NVIDIA's parallel computing architecture 
that enables dramatic increases in computing performance by harnessing the power of the GPU.


-
16 bit precision

 revision of IEEE 754, published in 2008, defines a floating point format that occupies only 16 bits. 
 Known as binary16, it is primarily intended to reduce storage and memory bandwidth requirements. 
 Since it provides only "half" precision, its use for actual computation is problematic.
 
  It is intended for storage of floating-point values in applications where
  >higher precision is not essential for performing arithmetic computations. 
  
  
  
  -
  What is Gradient Clipping?

Gradient clipping is a technique to >>> prevent exploding gradients.
There are many ways to compute gradient clipping,
but a common one is to rescale gradients so that their norm is at most a particular value. 

With gradient clipping, pre-determined gradient threshold be introduced,
and  then gradients norms that exceed this threshold are scaled down to match the norm.  
This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped. 

There is an introduced bias in the resulting values from the gradient, 
but gradient clipping can keep things stable. 

Why is this Useful?
Training recurrent neural networks can be very difficult. 
>>Two common issues with training recurrent neural networks are vanishing gradients and exploding gradients.


-
Checkpointing

is a technique that provides fault tolerance for computing systems. 
It basically consists of saving a snapshot of the application's state,
so that applications can restart from that point in case of failure.

-
In computer science, ------>>>>>>>>Monte Carlo tree search (MCTS) 

is a heuristic search algorithm for some kinds of decision processes, 
most notably those employed in game play. 
MCTS was introduced in 2006 for computer Go.
It has been used in other board games like chess and shogi,
games with incomplete information such as bridge and poker,
as well as in turn-based-strategy video games

Monte Carlo Tree Search is a method usually used in games 
to predict the path (moves) that should be taken by the policy to reach the final winning solution.


--- 
NAIVES BAYES

why is it called naives?


---
Difference between gradients and derivatives

The gradient is a vector; it points in the direction of steepest ascent.

The directional derivative is a number; it is the rate of change when your point in ℝ3
moves in that direction. 

Be careful that directional derivative of a function is a scalar while gradient is a vector.

The only difference between derivative and directional derivative is the definition of those terms. Remember:

    Directional derivative is the instantaneous rate of change (which is a scalar) of 𝑓(𝑥,𝑦)
     in the direction of the unit vector 𝑢

   Derivative is the rate of change of 𝑓(𝑥,𝑦)
   , which can be thought of the slope of the function at a point (𝑥0,𝑦0).
   
   
   -------
   
    Church–Turing thesis (also known as computability thesis)
    
     is a hypothesis about >>the nature of computable functions. 
     
     It states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine.
   
   The Church-Turing thesis (CTT)
   underlies tantalizing open questions concerning >>the fundamental place of computing in the physical universe.
   
   For example, is every physical system computable?
   Is the universe essentially computational in nature? 
   What are the implications for computer science of recent speculation about physical uncomputability? 
   Does CTT place a fundamental logical limit on what can be computed, a computational "barrier" that cannot be broken,
   no matter how far and in what multitude of ways computers develop? 
   Or could new types of hardware, based perhaps on quantum or relativistic phenomena, lead to radically new computing paradigms 
   that do breach the Church-Turing barrier, in which the uncomputable becomes computable, in an upgraded sense of "computable"?
   
   Before addressing these questions, we first look back to the 1930s to consider how Alonzo Church and Alan Turing formulated, and sought to justify, 
   their versions of CTT. With this necessary history under our belts, 
   >we then turn to today's dramatically more powerful versions of CTT.
   
   ---
   
   What is quantization?

Quantization isn’t something new, it has been around for decades since the creation of digital electronics. 
When we take photo on our cellphone, the real world scene which is analog is captured by the camera and digitized into computer files. 

Quantization is part of that process that convert a continuous data (can be infinitely small or big)
to discrete numbers within a fixed range, say numbers 0, 1, 2, .., 255 which are commonly used in digital image files.

In deep learning, quantization generally refers to converting from floating point (with dynamic range of the order of 1^-38 to 1x10³⁸)
to fixed point integer (e.g. 8-bit integer between 0 and 255).

Some information will be lost in quantization 
but researches show that with tricks in training, the loss in accuracy is manageable.

Why quantization?

There are two main reasons.
Deep neural network consists of many parameters which are known as weights, for example, the famous VGG network has over 100 million parameters!! 
In many scenarios, the bottleneck of running deep neural network is in transferring the weights and data between main memory and compute cores. 

Therefore, as opposed to GPU that have one large shared pool of memory,
modern AI chips have a lot of local memories around the compute cores to minimize the data transfer latency.

Now, by using 8-bit integer rather than 32-bit, we instantly speed up the memory transfer by 4x!
This also bring other benefits that accompanies smaller file size i.e. less memory storage, faster download time etc.
   

--

Tarski's    undefinability theorem


Tarski's undefinability theorem, stated and proved by Alfred Tarski in 1933, 
is an important limitative result in mathematical logic, 
the foundations of mathematics, and in formal semantics. 

Informally, the theorem states that arithmetical truth cannot be defined in arithmetic.

The theorem applies more generally to any sufficiently strong formal system, 
showing that 
>>> truth in the standard model of the system 
>>>cannot be defined within the system. 

--

The Halting problem -- Turing

In computability theory, the halting problem is >>the problem of determining,
from a description of an arbitrary computer program and an input, 
>>whether the program will finish running, or continue to run forever. 

Alan Turing proved in 1936 that 
>a general algorithm to solve the halting problem for all possible program-input pairs 
>cannot exist.
 
 
--

Gödel's incompleteness theorems 


are two theorems of mathematical logic that 
>>demonstrate the inherent limitations of every formal axiomatic system capable of modelling basic arithmetic. 

These results, published by Kurt Gödel in 1931, are important both in mathematical logic and in the philosophy of mathematics. 
The theorems are widely, but not universally, interpreted as 
>>showing that Hilbert's program to find a complete and consistent set of axioms for all mathematics is impossible.

1. The first incompleteness theorem states that 
>>no consistent system of axioms 
whose theorems can be listed by an effective procedure (i.e., an algorithm) 
>>is capable of proving all truths about the arithmetic of natural numbers. 

For any such consistent formal system, 
there will always be statements about natural numbers that are true, 
>but that are unprovable within the system. 


2.The second incompleteness theorem, an extension of the first, 
>>shows that the system cannot demonstrate its own consistency.


Employing a diagonal argument, 
Gödel's incompleteness theorems were the first of several closely related theorems
>>on the limitations of formal systems.

>They were followed by Tarski's undefinability theorem on the formal undefinability of truth,
>Church's proof that Hilbert's Entscheidungsproblem is unsolvable, 
>and Turing's theorem that there is no algorithm to solve the halting problem
   
   
 --
 Entscheidungsproblem


In mathematics and computer science, the Entscheidungsproblem 
(pronounced [ɛntˈʃaɪ̯dʊŋspʁoˌbleːm], German for "decision problem") 
is a challenge posed by David Hilbert and Wilhelm Ackermann in 1928.

[1] The problem asks for an algorithm that considers, as input, 
a statement
and answers "Yes" or "No" according to whether the statement is universally valid
, i.e., valid in every structure satisfying the axioms. 
   
   
  --
  
  Using-neural-networks-to-solve-advanced-mathematics-equations
  
https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/

Building this data set required us to incorporate a range of data cleaning and generation techniques.
For our symbolic integration equations, for example, we flipped the translation approach around:
Instead of generating problems and finding their solutions, 
we generated solutions and found their problem (their derivative), 
which is a much easier task. 

This approach of generating problems from their solutions — what engineers sometimes refer to as trapdoor problems — 
made it feasible to create millions of integration examples. 
Our resulting translation-inspired data set consists of roughly 100M paired examples,
with subsets of integration problems as well as first- and second-order differential equations.
   
   
   
   --
   
   FLOPS
   
   Short for floating-point operations per second, 
   a common benchmark measurement for rating the speed of microprocessors. 
   
   Floating-point operations include any operations that involve fractional numbers. 
   Such operations, which take much longer to compute than integer operations, occur often in some applications.

Most modern microprocessors include a floating-point unit (FPU),
which is a specialized part of the microprocessor responsible for executing floating-point operations. 
The FLOPS measurement, therefore, actually measures the speed of the FPU. 
One of the most common benchmark tests used to measure FLOPS is called Linpack.

Many experts feel that FLOPS is not a relevant measurement because it fails to take into account factors 
such as the condition under which the microprocessor is running (e.g., heavy or light loads) 
and which exact operations are included as floating-point operations. 
For this reason, a consortium of vendors created the Standard Performance Evaluation Corporation (SPEC), 
which provides more meaningful benchmark values. 
   
   
   ---
   
    Scalar functions
    
    (sometimes referred to as User-Defined Functions / UDFs)
    return a single value as a return value, not as a result set.

--
re capsule networks 

Vectors help because the help us encode more information, 
and not just any kind of information, relational and relative information.

Imagine that instead of taking just the scalar activation of a feature (like a CNN does), 
we considered its vector containing something like  [likelihood, orientation, size]. 

The original scalar version might work something like the diagram on the left;
it detects a face even though the eyes and lips are huge relative to the face (95% likelihood)! 

Capsules on the other hand due to their richer vector information 
see that the sizes of the features are different and therefore output a lower likelihood for the detection of the face! 

---
Fourier 

In mathematics, 
>Fourier analysis 
is the study of the way general functions may be represented or approximated
by sums of simpler trigonometric functions.

>a Fourier transform 
is a mathematical transform that decomposes a function into its constituent frequencies,
such as the expression of a musical chord 
in terms of the volumes and frequencies of 
its constituent notes.

What are Fourier transforms used for?
The Fourier Transform is an important image processing tool 
which is used to decompose an image into its sine and cosine components. 

The output of the transformation represents the image in the Fourier or frequency domain,
while the input image is the spatial domain equivalent.


----------

Sine, Cosine and Tangent

https://www.mathsisfun.com/sine-cosine-tangent.html

Sine, Cosine and Tangent (often shortened to sin, cos and tan) are each a ratio of sides of a right angled triangle

To calculate them:

Divide the length of one side by another side
Example: What is the sine of 35°?

triangle with 2.8, 4.0 and 4.9 sides

sin(35°) 	= Opposite / Hypotenuse

2.8 / 4.9
  	= 0.57...

Size Does Not Matter

The triangle can be large or small and the ratio of sides stays the same.

Only the angle changes the ratio.


Why are these functions important?

    >>Because they let us work out angles when we know sides
   >> And they let us work out sides when we know angles


---

Hadamard product (or just product or element-wise product)

Multiplying elements in the same position 

a= [1,3]
b= [2,4]

A ∘ B = [1*2, 3*4] = [2,12]

The resultant vector consists of the ---entry wise product--- of u and v.

In mathematics, the Hadamard product is a binary operation 
that takes two matrices of the same dimensions and produces another matrix of the same dimension as the operands, 
where each element i, j 
is the product of elements i, j of the original two matrices. 



In Pytorch, we can also perform Hadamard product
with just 1 line of code and assign it to a variable z. 

z= a*b


----

Dot product  --  inner product ---   torch.dot()

The dot product is a single number
>>> that represents how similar the two vectors are.

between two vectors of the >>>  same size. 
One thing to keep in mind is that the dot product only works if the vectors are of the same size.
If not, it will spit out an error complaining about inconsistent number of elements.

The first element of t1 is multiplied with the first element of t2 
and the second element of t1 is multiplied with the second element of t2 and so on and so forth. 
>>These products are then summed together. 

Note that a dot product between 2 vectors always returns a scalar value.

We multiply the first component from v and u.
We then multiply the second component      and >> add the result together.

The result is 
>>>>a number that represents how similar the two vectors are. 

a= [1,3]
b= [2,4]

z= (1*2) + (3*4) = 14

We can also perform dot product using the pythorch function “dot”.

torch.dot(a,b)




# Calculate dot product of u, v

u = torch.tensor([1, 2])
v = torch.tensor([3, 2])

print("Dot Product of u, v:", torch.dot(u,v))

The result is tensor(7). The function is 1 x 3 + 2 x 2 = 7.

-----

torch.mm()

torch.mm() is responsible for multiplication between 2 matrices. 
Similar to vector multiplication, matrix multiplication makes use of dot product and requires the matrices to have certain sizes.
(but not the same sizes, as torch.dot() requires, rather,
The number of columns of the first matrix must be equal to the number of rows of the second matrix.
Each row of the first matrix will be >>transposed >>and multiplied against each column in the second matrix. 

This is basically a vector multiplication where each row in the first matrix is transposed
to make sure it has the same dimension as each column in the second matrix.

For example, the dot product is valid 
if the first matrix has a dimension of (3, 2) 
and the second matrix has a dimension of (2, 2). 
But not the other way around. 

A bunch of words might not help much, so let's look at a couple of examples.


In [5]:

# Example 1 - working

t1 = torch.tensor([
    [1, 2, 3],
    [1, 2, 3]
])
t2 = torch.tensor([
    [1, 2],
    [1, 2],
    [1, 2]
])

torch.mm(t1, t2)


Out[5]:

tensor([[ 6, 12],
        [ 6, 12]])



Example below shows that it is important to make sure 
the rows of the first matrix 
have the same number of entries as 
the columns of the second matrix.


In [7]:

# Example 3 - breaking (to illustrate when it breaks)
t1 = torch.tensor([
    [1, 2, 3],
    [1, 2, 3]
])

torch.mm(t1, t1)

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-7-36387735bdb2> in <module>
      5 ])




Example 2 (below) uses the same arguments as example 1,
>except that the order of the arguments is swapped.

We can see that swapping the order results in a completely different outcome. 

So, unlike the dot product between 2 vectors,
matrix multiplication is not commutative; t1 x t2 != t2 x t1.


In [6]:

# Example 2 - working
torch.mm(t2, t1)

Out[6]:

tensor([[3, 6, 9],
        [3, 6, 9],
        [3, 6, 9]])



----

 torch.matmul ()
 is more general as depending on the inputs, it can correspond to dot, mm or bmm.
 

This function performs multiplication, 
>>>>but it is not limited to certain shapes of tensors. 
(like .dot (same size) or .mm (first matrix rows = second metrix colums) are)

torch.matmul() allows us to do multiplication for different ranks of tensors.
This function behaves according to the dimensionality of the input tensors. 
For instance, if both arguments are vectors of the same size, it will behave exactly like torch.dot().

If both arguments are matrices, it will perform matrix multiplication similar to torch.mm(). 

It also supports multiplication between a scalar and a matrix, 
by converting the scalar value into a rank-2 tensor so these 2 tensors will be compatible. 

>>>>In other words, it supports broadcasting. 


In [9]:

# Example 2 - working
t1 = torch.randn(2)
t2 = torch.randn(2, 3)

torch.matmul(t1, t2)

Out[9]:

tensor([1.0438, 0.5157, 1.9654])



In the above example, t1 is a scalar value and t2 is a matrix. 
The way matmul handles this is by pre-pending a 1 to the dimension of t1 so the new dimension becomes (1, 2).

It is now compatible with t2 that has a dimension of (2, 3).
The pre-pended 1 is removed after multiplication is performed.

https://dev.to/aishahsofea/linear-algebra-operations-with-pytorch-1bma


----

torch.transpose()

Sometimes the tensor that we have is not the shape or dimension that we desire,
and this happens a lot. 
So this is where transposing an array or a matrix comes in handy.

One of the applications is when doing an operation within matrices itself,
which I mentioned earlier in torch.mm() section.

In terms of a matrix, transposing can be thought of as flipping the elements over the diagonal axis. 

torch.transpose() accepts 3 arguments; 
first argument being the tensor,
second and third arguments being the dimensions that we want to swap.


In [11]:

# Example 1 - working
t1 = torch.tensor([
    [1, 2],
    [3, 4]
])

torch.transpose(t1, 0, 1)

Out[11]:

tensor([[1, 3],
        [2, 4]])


 

-------

linspace

A useful function for plotting mathematical functions is "linspace". 

>>Linspace returns evenly spaced numbers over a specified interval. 

We specify the starting point of the sequence, the ending point of the sequence, 
and the parameter ”steps" indicates the number of samples to generate

torch.linspace(-2,2, steps=5)

---

Eigenvalues and eigenvectors

wiki
"Characteristic root" redirects here.

In linear algebra, an eigenvector or characteristic vector 
of a linear transformation is a nonzero vector that changes by a scalar factor 
when that -linear- transformation is applied to it. 

The corresponding eigenvalue, often denoted by λ {\displaystyle \lambda } \lambda ,[1] 
is the factor by which the eigenvector is --scaled.

Geometrically, an eigenvector, 
corresponding to a real nonzero eigenvalue, 
>points in a direction in which it is stretched by the transformation 

>and the eigenvalue is the factor by which it is stretched. 


If the eigenvalue is negative, the direction is reversed.



---------

A p-value is a measure of the probability 
that an observed difference could have occurred just by random chance. 

The lower the p-value, the greater the statistical significance of the observed difference. 


----

A natural number 

is an integer greater than 0. 
Natural numbers begin at 1 and increment to infinity: 1, 2, 3, 4, 5, etc.

Natural numbers are also called "counting numbers" because they are used for counting. 
For example, if you are timing something in seconds, 
you would use natural numbers (usually starting with 1).

When written, natural numbers do not have a decimal point (since they are integers), 
but large natural numbers may include commas, e.g. 1,000 and 234,567,890. 

Natural numbers will never include a minus symbol (-) because they cannot be negative.


----

Real numbers

The type of number we normally use, such as 1, 15.82, −0.1, 3/4, etc.

Positive or negative, large or small, whole numbers or decimal numbers are all Real Numbers.

They are called "Real Numbers" because they are not Imaginary Numbers.









