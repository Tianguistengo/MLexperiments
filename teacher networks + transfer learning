



In unsupervised learning the training data is unlabeled.
The system tries to learn without a teacher.

Some of the most important unsupervised learning algorithms:

• Clustering

— k-Means
— Hierarchical Cluster Analysis (HCA)
— Expectation Maximization

• Visualization and dimensionality reduction

— Principal Component Analysis (PCA)
— Kernel PCA
— Locally-Linear Embedding (LLE)
— t-distributed Stochastic Neighbor Embedding (t-SNE)

• Association rule learning

— Apriori
— Eclat


-Self-training with Noisy Student improves ImageNet classification
https://arxiv.org/abs/1911.04252?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=80984419&_hsenc=p2ANqtz-8uTBb8iElUUB1-PtwhrLwcwyixcDm8YH1BQjtTCkZXpG1qW6-GZedGCLY8gu_sT41XSGJ--OCoEBIxItP0wCZNDvzpfg&_hsmi=80984419



-Google taught this robotic dog to learn new tricks BY IMITATING A REAL ONE

Google researchers are using imitation learning to teach autonomous robots 
how to pace, spin, and move in more agile ways.

What they did: Using a data set of motion capture data recorded from various sensors attached to a dog, 
the researchers taught a quadruped robot named Laikago 
several different movements that are hard to achieve through traditional hand-coded robotic controls.




--> Model Zoo

Discover open source deep learning code and pretrained models.
https://modelzoo.co/

-Torchvision has loads of pretrained networks too.




-
https://arxiv.org/abs/2003.10580?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=87877396&_hsenc=p2ANqtz-_ZL2dcAL98Qqiwx1e4NnPu9XvPQ8qz51EW3UEeJB3SIV5X17W94YLUh7KAuKZLiT7uGvcoQAhxBSTp-fWcxkTI4ZTt7Q&_hsmi=87877396


Flexible Teachers, Smarter Students

Human teachers can teach more effectively by adjusting their methods in response to student feedback. It turns out that teacher networks can do the same.
What’s new: Hieu Pham led joint work by Carnegie Mellon and Google Brain that trained teacher models (larger, pretrained networks) to educate student models (smaller networks that learn from the teacher’s predictions) more effectively by observing and adjusting to student performance. The method’s name, Meta Pseudo Labels, refers to meta-learning: in this case, learning from predictions that have been tweaked to optimize their educational value rather than their accuracy. Pseudo labels are teacher classifications, either binary or values between 0 and 1, that a student learns to re-create.
Key insight: A teacher may generate predictions showing that one dog looks more like a wolf than a cat, while another dog falls in between. But its pseudo label “dog” doesn’t capture that difference. For instance, a model considering two images may output [0.8, 0.2, 0.0] and [0.6, 0.2, 0.2] to express its confidence that they depict a dog, wolf, or cat. Both classifications reflect high confidence that the image is a dog, but they contain more nuanced information. Rather than receiving only the highest-confidence classifications, the student will learn better if the teacher adjusts its predictions to exaggerate, say, the dogishness of wolfish dogs. For example, the teacher may change [0.8, 0.2, 0.0] to [0.9, 0.1, 0.0].
How it works: WideResNet-28-2 and ResNet 50 teachers taught EfficientNet students how to recognize images from CIFAR-10 , SVHN, and ImageNet.

    The student learns from a minibatch of images classified by the teacher. Then the student makes predictions on some of the validation set. The teacher learns to minimize the student’s validation loss. The student learns from the teacher’s prediction distribution, so backpropagation can update the teacher based on student errors. Then the process repeats for the next minibatch.
    It may take many training steps before the teacher learns a better distribution. (As any teacher will tell you, the longer students are confused, the less they learn, and the more the teacher must adjust.) The teacher also learns from a small amount of labeled data in the validation set to prevent mis-teaching the student early in training.
    Training the teacher on the validation set may look like a bad idea, but the student is never directly exposed to the validation set’s labels. The teacher’s additional knowledge helps the student generalize without overfitting the validation set.

Results: Meta Pseudo Labels produced a student with higher ImageNet accuracy (86.9 percent) than a supervised model (84.5 percent). The improvements remained when using a limited number of labels from each dataset, where MPL achieved CIFAR-10 accuracy of 83.7 percent compared with a supervised model’s 82.1, and SVHN accuracy to 91.9 percent compared with 88.2. 
Why it matters: Student-teacher training began as a compression technique. But lately Noisy Student and Meta Pseudo Labels are making it a competitive approach to training models that generalize.
We’re thinking: At deeplearning.ai, we aim to keep improving our instruction based on student feedback — but please make your feedback differentiable.


-----------

How to Learn from Others:
Transfer Machine Learning 
with Additive Regression Models
to Improve Sales Forecasting

https://arxiv.org/abs/2005.10698

In a variety of business situations, the introduction or improvement of machine learning approaches is impaired 
> as these cannot draw on existing analytical models. 

However, in many cases similar problems may have already been solved elsewhere
>> but the accumulated analytical knowledge cannot be tapped to solve a new problem, 
e.g., because of privacy barriers. For the particular purpose of sales forecasting for similar entities, 
we propose a transfer machine learning approach based on 
>> additive regression models 
that lets new entities benefit from models of existing entities.

We evaluate the approach on a rich, multi-year dataset of multiple restaurant branches.
We differentiate the options to simply 
transfer models from one branch to another ("zero shot") 
or to transfer and adapt them. 

We analyze feasibility and performance against several forecasting benchmarks.
The results show the potential of the approach to 
>> exploit the collectively available analytical knowledge. 

Thus, we contribute an approach that is generalizable 
beyond sales forecasting and the specific use case in particular. 
In addition, we demonstrate its feasibility for a typical use case as well as the potential for improving forecasting quality.
These results should inform academia, as they 
>> help to leverage knowledge across various entities, 
and have immediate practical application in industry. 

-------------

Mean teachers are better role models:
Weight-averaged consistency targets 
improve 
semi-supervised deep learning results

https://arxiv.org/pdf/1703.01780.pdf
https://github.com/CuriousAI/mean-teacher

Abstract
The recently proposed Temporal Ensembling 
has achieved state-of-the-art results in several semi-supervised learning benchmarks.

It maintains an exponential moving average of label predictions on each training example,
and penalizes predictions that are inconsistent with this target. 

However, because the targets change only once per epoch, 
Temporal Ensembling becomes unwieldy when learning large datasets.

To overcome this problem, we propose Mean Teacher,
a method that averages model weights instead of label predictions.

As an additional benefit, 
Mean Teacher improves  test  accuracy  
and  enables  training  with  fewer  labels  
than  Temporal Ensembling. 

Without changing the network architecture, 
Mean Teacher achieves anerror rate of 4.35% on SVHN with 250 labels,
outperforming Temporal Ensembling trained with 1000 labels. 

We also show that a good network architecture is crucial to performance.
Combining Mean Teacher and Residual Networks,
we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, 
and onImageNet 2012 with 10% of the labels from 35.24% to 9.11%.












