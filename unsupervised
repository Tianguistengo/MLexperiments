
"
In unsupervised learning, the training data is unlabeled.
The system tries to learn without a teacher.

Some of the most important unsupervised learning algorithms:

• Clustering

— k-Means
— Hierarchical Cluster Analysis (HCA)
— Expectation Maximization

• Visualization and dimensionality reduction

— Principal Component Analysis (PCA)
— Kernel PCA
— Locally-Linear Embedding (LLE)
— t-distributed Stochastic Neighbor Embedding (t-SNE)

• Association rule learning

— Apriori
— Eclat

"


" It is also being used to develop labels on top of the supervised models."

""2 broad categories of unsupervised:


Association: An association rule learning problem is where you want to discover rules 
that describe large portions of your data, such as people that buy X also tend to buy Y.

Clustering: A clustering problem is where one can find the inherent groupings in the data, 
such as grouping customers by purchasing behaviour.

Some of the common clustering algorithms are hierarchical clustering, Gaussian mixture models,
and K-means clustering. The last one is considered one of the simplest unsupervised learning algorithms,
wherein data is split into k distinct clusters based on distance to the centroid of a cluster. ""


-Inertia:

It measures how spread out the clusters are (the lower {the more together points are} the better).
How far samples are from the centroid.

print(model.inertia_)

A good rule of thumb is to choose an "elbow" (a piont where intertia starts to decrease more slowly)
in the intertia plot (not too many clusters but not too few either).

-on feature scaling or transforming features for better clustering:
StandardScaler (sklearn)
transforms each features to have mean 0 and variance 1
features are said to be "standardized"

"Note that Normalizer() is different to StandardScaler().
While StandardScaler() standardizes FEATURES 
(such as the features of the fish data from the previous exercise)
by removing the mean and scaling to unit variance,
Normalizer() rescales EACH SAMPLE
- here, each company's stock price - independently of the other. 


-Hierarchical clustering 
can be agglomerative (put dif units into a bigger group)
or divisive (the opposite)

"the LINKAGE method defines how the distance between clusters is measured. 
In complete linkage, the distance between clusters is the distance between the furthest points of the clusters.
In single linkage, the distance between clusters is the distance between the closest points of the clusters.
Different linkage, different hierarchical clustering!

-Flat 

Flat clustering is where the scientist tells the machine how many categories to cluster the data into.

Hierarchical

Hierarchical clustering is where the machine is allowed to decide how many clusters to create based on its own algorithms.


-     t-SNE

Maps samples to 2D space (or 3D)
great for inspecting datasets

-"Matplotlib allows you to adjust the TRANSPARENCY of a graph plot using the ALPHA attribute. 
By default, alpha=1. If you want to make the graph plot more transparent, then you can make alpha less than 1

-Dimensionality reduction
Remove less-informative "noisy" features, and keep the important/informative ones.

-PCA
principal components = directions of variance
PCA aligns principal components with the axes

instrinsic dimension = number of PCA features with a high varianace



















