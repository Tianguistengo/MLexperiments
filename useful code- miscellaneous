Fine-tuning a model:

"1.Grid Search: set up a grid of hyperparameter values and for each combination, 
train a model and score on the validation data. In this approach, every single combination of hyperparameters
values is tried which can be very inefficient!

2. Random search: set up a grid of hyperparameter values and select random combinations
to train the model and score. The number of search iterations is set based on time/resources.

3. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization,
or evolutionary algorithms to conduct a guided search for the best hyperparameters.



- Gradient Boosting Machine (GBM) 

"The basics you need to know about the GBM are that it is an ensemble method 
that works by training many individual learners, almost always decision trees. However, unlike in a random forest
where the trees are trained in parallel, in a GBM, the trees are trained sequentially 
with each tree learning from 
the mistakes of the previous ones. 
The hundreds or thousands of weak learners are combined to make a single strong 
ensemble learner with the contributions of each individual learned during training using Gradient Descent 
(the weights of the individual trees would therefore be a model parameter).

The GBM has many hyperparameters to tune that control both the overall ensemble (such as the learning rate) and the individual decision trees (such as the number of leaves in the tree or the maximum depth of the tree).
