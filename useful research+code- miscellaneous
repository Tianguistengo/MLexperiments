Fine-tuning a model:

"1.Grid Search: set up a grid of hyperparameter values and for each combination, 
train a model and score on the validation data. In this approach, every single combination of hyperparameters
values is tried which can be very inefficient!

2. Random search: set up a grid of hyperparameter values and select random combinations
to train the model and score. The number of search iterations is set based on time/resources.

3. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization,
or evolutionary algorithms to conduct a guided search for the best hyperparameters.

-"  We will use cross validation to determine the performance of model hyperparameters 
and early stopping with the GBM so we do not have to tune the number of estimators.

The basic strategy for both grid and random search is simple: for each hyperparameter value combination, 
evaluate the cross validation score and record the results along with the hyperparameters. 

Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score,
train the model on all the training data, and make predictions on the test data.

-To "test" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. 
When we do hyperparameter tuning, it's crucial to         
not tune the hyperparameters on the testing data. 
We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data. 

- The performance of each set of hyperparameters is determined by
Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.

-Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have 
a large enough training set, we can probably get away with just using a single separate validation set,
but cross validation is a safer method to avoid overfitting.

-# Cross validation with early stopping

cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, 
                    metrics = 'auc', nfold = N_FOLDS, seed = 42)
                    
  We can use this result as a baseline model to beat. To find out how well the model does on our "test" data,
  we will retrain it on all the training data with the best number of estimators
  found during cross validation with early stopping.                  


- Gradient Boosting Machine (GBM) 

"The basics you need to know about the GBM are that it is an ensemble method 
that works by training many individual learners, almost always decision trees. However, unlike in a random forest
where the trees are trained in parallel, in a GBM, the trees are trained sequentially 
with each tree learning from 
the mistakes of the previous ones. 
The hundreds or thousands of weak learners are combined to make a single strong 
ensemble learner with the contributions of each individual learned during training using Gradient Descent 
(the weights of the individual trees would therefore be a model parameter).

The GBM has many hyperparameters to tune 
that control both the overall ensemble (such as the learning rate) 
and the individual decision trees 
(such as the number of leaves in the tree or the maximum depth of the tree).


-One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators 
(the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, 
but there's a better method: early stopping. Early stopping means training until the validation error
does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, 
and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation 
error has not decreased for 100 rounds. Then, the number of estimators 
that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.
This is one of many forms of regularization that aims to improve generalization performance on the testing set 
by not overfitting to the training data.



-Histogram-based Gradient Boosting Classification Tree.
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier

This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).

This estimator has --native support for missing values (NaNs--).
During training, the tree grower learns at each split point whether samples with missing values should go to the left 
or right child, based on the potential gain.
When predicting, samples with missing values are assigned to the left or right child consequently. 
If no missing values were encountered for a given feature during training, then samples with missing values
are mapped to whichever child has the most samples.

This implementation is inspired by LightGBM.



--1.16. Probability calibration

https://scikit-learn.org/stable/modules/calibration.html

When performing classification you often want not only to predict the class label,
but also obtain a probability of the respective label.
This probability gives you some kind of confidence on the prediction. 
Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. 
The calibration module allows you 
to better calibrate the probabilities of a given model, or to add support for probability prediction.


- Working with small data:

"In the manufacturing system described above, the absolute number of examples was small. 
But the problem of small data also arises when
the dataset in aggregate is large, but the frequency of specific important classes is low.

Say you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia
in the training set, then the algorithm can obtain high training- and test-set accuracy, 
but still do poorly on cases of hernia

Small data (also called low data) problems are hard because most learning algorithms optimize a cost function 
that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes
and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes 
does not work, as it introduces excessive variance.

We have huge datasets for self-driving, 
but getting good performance on important but rare cases continues to be challenging.


How do we address small data? We are still in the early days of building small data algorithms, but some approaches include:

    Transfer learning, in which we learn from a related task and transfer knowledge over.
    This includes variations on self-supervised learning, in which the related tasks
    can be “made up” from cheap unlabeled data.
    
    One- or few-shot learning, in which we (meta-)learn from many related tasks with small training sets in the hope
    of doing well on the problem of interest.
    
    Relying on hand-coded knowledge, for example through designing more complex ML pipelines. 
    An AI system has two major sources of knowledge: (i) data and (ii) prior knowledge encoded by the engineering team.
    If we have small data, then we may need to encode more prior knowledge.
    
    Data augmentation and data synthesis.
    
    --  how to create a confusion matrix for error analysis 
    
  
  You need to make predictions using the cross_val_predict() function,
  and then call the confusion_matrix() function
    
     y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)
    
 conf_mx = confusion_matrix(y_train, y_train_pred)

 conf_mx
 

Let’s focus the plot on the errors. First, you need to divide each value in the confusion
matrix by the number of images in the corresponding class, so you can compare error
rates instead of absolute number of errors (which would make abundant classes look
unfairly bad):

row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums

-"Many machine learning algorithms are known to produce better models by discretizing continuous attributes."

-"NAIVE BAYES is a probabilistic machine learning algorithm that can be used in a wide variety of classification tasks.
 It is based on the works of Thomas Bayes (1702–61).

But why is it called ‘Naive’?

The name naive is used because it assumes the features that go into the model 
are independent of each other. 
That is changing the value of one feature, does not directly influence or change the value 
of any of the other features 
used in the algorithm.

Since it is a probabilistic model, the algorithm can be coded up easily and
the predictions made real quick. Real-time quick. Because of this, it is easily scalable and is trditionally the 
algorithm of choice for real-world applications (apps) that are required to respond to user’s requests instantaneously.

We need to understand what ‘Conditional Probability’ is and what is the ‘Bayes Rule’.

-"Bayesian Optimization is often used in applied machine learning to tune the hyperparameters
of a given well-performing model on a validation dataset.

If training is really long, you  might prefer a bayesian optimization approach.

Bayesian Optimization provides a probabilistically principled method for global optimization.

Global optimization is a challenging problem that involves black box and often non-convex,
non-linear, noisy, and computationally expensive objective functions.

Global function optimization, or function optimization for short, 
involves finding the minimum or maximum of an objective function.

Objective Function == Function that takes a sample and returns a cost.


Samples are drawn from the domain and evaluated by the objective function to give a score or cost.

-
-"Through hyperparameter optimization, a practitioner identifies free parameters in the model 
that can be tuned to achieve better model performance. There are a few commonly used methods: 
hand-tuning, grid search, random search, evolutionary algorithms and Bayesian optimization.

Grid search cannot efficiently optimize models with more than 4 dimensions, due to the curse of dimensionality.
Although random search is more capable than grid search, its naive approach is still both time-consuming and expensive 
and is more likely to settle in a local optima. Furthermore, it is unscalable beyond 10 parameters. 
Evolutionary algorithms are great if you have access to nearly unlimited compute that can be run in parallel, 
but is often difficult to implement if you do not.  

Bayesian optimization democratizes access to these algorithmic superpowers by relaxing each of these constraints. 

-"Evolutionary algorithms for hyperparameter optmizaiton  use the comparison of “mutation-like” configurations
with the best performing configurations to iterate on the model parameters.

-A.G. "Once you are confident about your final model, measure its performance on the
test set to estimate the generalization error.

Don’t tweak your model after measuring the generalization error:
you would just start overfitting the test set.

- A.G. "Fine-Tune the System

• You will want to use as much data as possible for this step, especially as you move
toward the end of fine-tuning.
• As always automate what you can.

-A.G. "Monitoring

Write monitoring code to check your system’s live performance at regular intervals 
and trigger alerts when it drops.

• Beware of slow degradation too: models tend to “rot” as data evolves.

• Measuring performance may require a human pipeline (e.g., via a crowdsourc‐
ing service).

• Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐
dom values, or another team’s output becoming stale). This is particularly
important for online learning systems.

Retrain your models on a regular basis on fresh data (automate as much as possi‐
ble).

--bias (math)

An intercept or offset from an origin. Bias (also known as the bias term)
is referred to as b or w0 in machine learning models. 

--convergence (Google ML Glossary)

Informally, often refers to a state reached during training in which training loss and validation loss
change very little or not at all with each iteration after a certain number of iterations. 
In other words, a model reaches convergence when additional training on the current data will not improve the model.

--A.G. "training a model means
searching for a combination of model parameters 
that minimizes a cost function (over the training set). 

It is a search in the model's "parameter space"

-- batch

The set of examples used in one iteration (that is, one gradient update) of model training.

This is why the algorithm is
called Batch Gradient Descent: it uses the whole batch of training
data at every step. As a result it is terribly slow on very large train‐
ing sets (but we will see much faster Gradient Descent algorithms
shortly). However, Gradient Descent scales well with the number of
features; training a Linear Regression model when there are hun‐
dreds of thousands of features is much faster using Gradient
Descent than using the Normal Equation.

-- "The risk of using external data sources is that schema or pattern of the data 
may change over time without notification or the team’s awareness. 

For example, someone may vandalize the open data, the method of contents generation may change,
or suppliers may alter the schema of the data without telling downstream users. 
The reasons above can threaten the performance of the ML model and create critical issues if left unnoticed for too long.

In order to monitor the ETL process effectively and prevent critical data issue to affect 
the model and its performance; 
integrity, consistency, and availability of the data are three aspects
to consider when selecting metrics for ETL monitoring.

The integrity of the data can be monitored by creating comprehensive validation rules
to catch and report the exceptions. Validation rules are usually developed during the 
exploratory data analysis and training of the model. Some metrics that can be used are, for instance,
the number of missing or unseen features or ratio of data points that fail each validation rule.

