Fine-tuning a model:

"1.Grid Search: set up a grid of hyperparameter values and for each combination, 
train a model and score on the validation data. In this approach, every single combination of hyperparameters
values is tried which can be very inefficient!

2. Random search: set up a grid of hyperparameter values and select random combinations
to train the model and score. The number of search iterations is set based on time/resources.

3. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization,
or evolutionary algorithms to conduct a guided search for the best hyperparameters.

-"  We will use cross validation to determine the performance of model hyperparameters 
and early stopping with the GBM so we do not have to tune the number of estimators.

The basic strategy for both grid and random search is simple: for each hyperparameter value combination, 
evaluate the cross validation score and record the results along with the hyperparameters. 

Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score,
train the model on all the training data, and make predictions on the test data.

-To "test" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. 
When we do hyperparameter tuning, it's crucial to         
not tune the hyperparameters on the testing data. 
We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data. 

- The performance of each set of hyperparameters is determined by
Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.

-Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have 
a large enough training set, we can probably get away with just using a single separate validation set,
but cross validation is a safer method to avoid overfitting.

-# Cross validation with early stopping

cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, 
                    metrics = 'auc', nfold = N_FOLDS, seed = 42)
                    
  We can use this result as a baseline model to beat. To find out how well the model does on our "test" data,
  we will retrain it on all the training data with the best number of estimators
  found during cross validation with early stopping.                  


- Gradient Boosting Machine (GBM) 

"The basics you need to know about the GBM are that it is an ensemble method 
that works by training many individual learners, almost always decision trees. However, unlike in a random forest
where the trees are trained in parallel, in a GBM, the trees are trained sequentially 
with each tree learning from 
the mistakes of the previous ones. 
The hundreds or thousands of weak learners are combined to make a single strong 
ensemble learner with the contributions of each individual learned during training using Gradient Descent 
(the weights of the individual trees would therefore be a model parameter).

The GBM has many hyperparameters to tune 
that control both the overall ensemble (such as the learning rate) 
and the individual decision trees 
(such as the number of leaves in the tree or the maximum depth of the tree).


-One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators 
(the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, 
but there's a better method: early stopping. Early stopping means training until the validation error
does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, 
and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation 
error has not decreased for 100 rounds. Then, the number of estimators 
that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.
This is one of many forms of regularization that aims to improve generalization performance on the testing set 
by not overfitting to the training data.



-Histogram-based Gradient Boosting Classification Tree.
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier

This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).

This estimator has --native support for missing values (NaNs--).
During training, the tree grower learns at each split point whether samples with missing values should go to the left 
or right child, based on the potential gain.
When predicting, samples with missing values are assigned to the left or right child consequently. 
If no missing values were encountered for a given feature during training, then samples with missing values
are mapped to whichever child has the most samples.

This implementation is inspired by LightGBM.



--1.16. Probability calibration

https://scikit-learn.org/stable/modules/calibration.html

When performing classification you often want not only to predict the class label,
but also obtain a probability of the respective label.
This probability gives you some kind of confidence on the prediction. 
Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. 
The calibration module allows you 
to better calibrate the probabilities of a given model, or to add support for probability prediction.


- Working with small data:

"In the manufacturing system described above, the absolute number of examples was small. 
But the problem of small data also arises when
the dataset in aggregate is large, but the frequency of specific important classes is low.

Say you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia
in the training set, then the algorithm can obtain high training- and test-set accuracy, 
but still do poorly on cases of hernia

Small data (also called low data) problems are hard because most learning algorithms optimize a cost function 
that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes
and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes 
does not work, as it introduces excessive variance.

We have huge datasets for self-driving, 
but getting good performance on important but rare cases continues to be challenging.


How do we address small data? We are still in the early days of building small data algorithms, but some approaches include:

    Transfer learning, in which we learn from a related task and transfer knowledge over.
    This includes variations on self-supervised learning, in which the related tasks
    can be “made up” from cheap unlabeled data.
    
    One- or few-shot learning, in which we (meta-)learn from many related tasks with small training sets in the hope
    of doing well on the problem of interest.
    
    Relying on hand-coded knowledge, for example through designing more complex ML pipelines. 
    An AI system has two major sources of knowledge: (i) data and (ii) prior knowledge encoded by the engineering team.
    If we have small data, then we may need to encode more prior knowledge.
    
    Data augmentation and data synthesis.
    
    --  how to create a confusion matrix for error analysis 
    
  
  You need to make predictions using the cross_val_predict() function,
  and then call the confusion_matrix() function
    
     y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)
    
 conf_mx = confusion_matrix(y_train, y_train_pred)

 conf_mx
 

Let’s focus the plot on the errors. First, you need to divide each value in the confusion
matrix by the number of images in the corresponding class, so you can compare error
rates instead of absolute number of errors (which would make abundant classes look
unfairly bad):

row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums

-"Many machine learning algorithms are known to produce better models by discretizing continuous attributes."

-"NAIVE BAYES is a probabilistic machine learning algorithm that can be used in a wide variety of classification tasks.
 It is based on the works of Thomas Bayes (1702–61).

But why is it called ‘Naive’?

The name naive is used because it assumes the features that go into the model 
are independent of each other. 
That is changing the value of one feature, does not directly influence or change the value 
of any of the other features 
used in the algorithm.

Since it is a probabilistic model, the algorithm can be coded up easily and
the predictions made real quick. Real-time quick. Because of this, it is easily scalable and is trditionally the 
algorithm of choice for real-world applications (apps) that are required to respond to user’s requests instantaneously.

We need to understand what ‘Conditional Probability’ is and what is the ‘Bayes Rule’.

-"Bayesian Optimization is often used in applied machine learning to tune the hyperparameters
of a given well-performing model on a validation dataset.

If training is really long, you  might prefer a bayesian optimization approach.

Bayesian Optimization provides a probabilistically principled method for global optimization.

Global optimization is a challenging problem that involves black box and often non-convex,
non-linear, noisy, and computationally expensive objective functions.

Global function optimization, or function optimization for short, 
involves finding the minimum or maximum of an objective function.

Objective Function == Function that takes a sample and returns a cost.


Samples are drawn from the domain and evaluated by the objective function to give a score or cost.

-
-"Through hyperparameter optimization, a practitioner identifies free parameters in the model 
that can be tuned to achieve better model performance. There are a few commonly used methods: 
hand-tuning, grid search, random search, evolutionary algorithms and Bayesian optimization.

Grid search cannot efficiently optimize models with more than 4 dimensions, due to the curse of dimensionality.
Although random search is more capable than grid search, its naive approach is still both time-consuming and expensive 
and is more likely to settle in a local optima. Furthermore, it is unscalable beyond 10 parameters. 
Evolutionary algorithms are great if you have access to nearly unlimited compute that can be run in parallel, 
but is often difficult to implement if you do not.  

Bayesian optimization democratizes access to these algorithmic superpowers by relaxing each of these constraints. 

-"Evolutionary algorithms for hyperparameter optmizaiton  use the comparison of “mutation-like” configurations
with the best performing configurations to iterate on the model parameters.

-A.G. "Once you are confident about your final model, measure its performance on the
test set to estimate the generalization error.

Don’t tweak your model after measuring the generalization error:
you would just start overfitting the test set.

- A.G. "Fine-Tune the System

• You will want to use as much data as possible for this step, especially as you move
toward the end of fine-tuning.
• As always automate what you can.

-A.G. "Monitoring

Write monitoring code to check your system’s live performance at regular intervals 
and trigger alerts when it drops.

• Beware of slow degradation too: models tend to “rot” as data evolves.

• Measuring performance may require a human pipeline (e.g., via a crowdsourc‐
ing service).

• Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐
dom values, or another team’s output becoming stale). This is particularly
important for online learning systems.

Retrain your models on a regular basis on fresh data (automate as much as possi‐
ble).

--bias (math)

An intercept or offset from an origin. Bias (also known as the bias term)
is referred to as b or w0 in machine learning models. 

--convergence (Google ML Glossary)

Informally, often refers to a state reached during training in which training loss and validation loss
change very little or not at all with each iteration after a certain number of iterations. 
In other words, a model reaches convergence when additional training on the current data will not improve the model.

--A.G. "training a model means
searching for a combination of model parameters 
that minimizes a cost function (over the training set). 

It is a search in the model's "parameter space"

-- batch

The set of examples used in one iteration (that is, one gradient update) of model training.

This is why the algorithm is
called Batch Gradient Descent: it uses the whole batch of training
data at every step. As a result it is terribly slow on very large train‐
ing sets (but we will see much faster Gradient Descent algorithms
shortly). However, Gradient Descent scales well with the number of
features; training a Linear Regression model when there are hun‐
dreds of thousands of features is much faster using Gradient
Descent than using the Normal Equation.

-- "The risk of using external data sources is that schema or pattern of the data 
may change over time without notification or the team’s awareness. 

For example, someone may vandalize the open data, the method of contents generation may change,
or suppliers may alter the schema of the data without telling downstream users. 
The reasons above can threaten the performance of the ML model and create critical issues if left unnoticed for too long.

In order to monitor the ETL process effectively and prevent critical data issue to affect 
the model and its performance; 
integrity, consistency, and availability of the data are three aspects
to consider when selecting metrics for ETL monitoring.

The integrity of the data can be monitored by creating comprehensive validation rules
to catch and report the exceptions. Validation rules are usually developed during the 
exploratory data analysis and training of the model. Some metrics that can be used are, for instance,
the number of missing or unseen features or ratio of data points that fail each validation rule.

The second principle for ETL monitoring metrics is the consistency of the data.
Consistency is important because a data point may pass the validation rules but the values in the features may
be vastly different from its history in which the model was trained from. The causes that impact the integrity 
of the data also affect the consistency of the data as well.
But in the case of user-generated data, 
the change in consistency can also happen organically
due to a shift in how users interact on the platform.

The availability of the data may not affect the precision of the ML model but it does affect the performance of the ML pipeline as a whole since the business objectives can’t be achieved without the data coming in (or coming late). Examples of KPIs that can be used to monitor the availability of the data are:

    Amount of delay time between each batch of data
    Total minutes, hours, or days, without data in the pipeline
    Number of times that data is delivered behind schedule
    

-"The most basic aspects of model performance  / KD Nuggets

is whether it is getting requests and how quickly it is producing results. 
No model will give good results if you don’t ask for them. 
And results are no good unless you actually get them in good time.

To monitor whether requests are arriving, you need to record the arrival each incoming request together with 
the name of the machine where the request has landed and an accurate timestamp. 
It is typically a bit better to record this data to a persistent stream on a distributed platform because 
log files can disappear if a machine goes down. It is also better to record
request arrival and completion as separate events 
so that you can distinguish failure to respond from lack of requests.

As a first cut, we can compute the distribution of the reported elapsed times. 
For latencies, the best way to do this is by using a non-uniform histogram.
We can compute such a histogram for each, say, five minute interval. To monitor performance, we can accumulate
a background over a fairly long period of time and plot the recent results against that background distribution.

These histograms can also be compared using more advanced, automated techniques based on comparing the counts 
in corresponding bins. One good way to do this is to use a statistical method known as the G-test. 

Are the results any good? Are they right? We have no real idea.
All we know so far is that requests are arriving and results are being produced at historically plausible levels and speeds. 

As nice as real-time ground truth feedback may be, there are lots of use cases where this just isn’t possible. 
For instance, we may not know even slightly whether a detected fraud is really just that for days or weeks.
There are still things that we can do to detect problems quickly, however.
Lots of models produce some sort of score or set of scores. Often these represent some kind of probability estimate. 
The method, then is to store digests of the score distribution every minute or so tagged with model version and such. 
There are two popular ways to compare distributions.

Nothing sings like a canary
In general, the more we know about what a model should be doing and the more specific we can be about comparing
against reference behavior, the quicker we can reliably detect changes. This is the motivation behind using a canary model.
The idea is that we send every request to the current production model as usually, 
but we also keep an older version of the model around and send every (or nearly every) request
to that older version as well. The older model is called a canary.
Because we are sending the exact same requests to both models and because the canary is a model that does nearly the same 
thing that we want the current model to do, we can compare the output of the two models request by request
to get a very specific idea about whether the new model is behaving as expected.

A canary model is also very handy when we are fielding a potential challenger to our current champion. 
If we are trying to quantify the risk of rolling out this new challenger to replace our current champion.
(you can analyze where the new model failed and the specifics of what is different)

This article has walked through a range of near real-time monitoring techniques for machine learning models
starting with monitors that look purely at the gross operational characteristics 
of request rate and response times.
Those are often very good for detecting systems level problems in evaluating requests.

For cases where we don’t know exactly what our model should output, 
we talked about methods for looking for changes in score distributions by comparing 
to the past performance of the model or by comparing to a canary model.


-"Tracking Experiments 
– Storing the model code, hyperparameters, and result metrics of every experiment is important 
to be able to discuss results, decide in which direction to go next or reproduce experiments. The simplest format 
for this could be a shared spreadsheet, but more sophisticated options are available
– though they often come with additional
requirements for the rest of the pipeline. 
As an example, here is a screenshot from MLflow`s experiment tracking server UI

-"Scalability 
– If you invest into building an ML pipeline, you may want to eliminate the need to rebuild it 
just because it can’t handle increasing volumes of data or the demands of a growing team of data scientists. 
Make sure that the pipeline is scalable by choosing building blocks that are scalable themselves.

-Automation – Even when a lot of steps are performed manually in the beginning, 
make sure that all steps can be automated later on via APIs or certain tools. 
Repeated manual tasks are boring and error-prone.

-Model and Code Versioning – Versioning the model code is essential for reproducing training runs. 
The pipeline should also be capable of managing versioned artifacts for at least every model 
that is supposed to be deployed in production. This is necessary for rollbacks and A/B tests in production.

---Sacred 
is a tool to help you configure, organize, log and reproduce experiments. 
It is designed to do all the tedious overhead work that you need to do around your actual experiment in order to:

    keep track of all the parameters of your experiment
    easily run your experiment for different settings
    save configurations for individual runs in a database
    reproduce your results
    
-Omniboard is a web dashboard for the Sacred machine learning experiment management tool.

It connects to the MongoDB database used by Sacred and helps in visualizing the experiments and metrics / logs 
collected in each experiment.     https://vivekratnavel.github.io/omniboard/#/

-One particular technical challenge often faced and highlighted is how to ensure that the same data transformations 
that are applied during training are also applied on prediction input. 
This includes constants that were computed during training (e. g. for normalization). Consider this early on in your
planning phase. If you are using TensorFlow, for example, take a look at TensorFlow Transform which addresses this challenge.

-What is Jenkins?

Jenkins is an open source automation tool written in Java with plugins built for Continuous Integration purpose.
Jenkins is used to build and test your software projects continuously making it easier for developers to integrate changes
to the project, and making it easier for users to obtain a fresh build. 
It also allows you to continuously deliver your software by integrating with a large number of testing and deployment 
technologies.

With Jenkins, organizations can accelerate the software development process through automation.
Jenkins integrates development life-cycle processes of all kinds, including build, document,
test, package, stage, deploy, static analysis and much more.

Jenkins achieves Continuous Integration with the help of plugins. 
Plugins allows the integration of Various DevOps stages.

-" What is an Objective Function?

Definition: The objective function is a mathematical term that describes how different variables contribute
to a certain value that is being sought to be optimized.

The objective function shows us how each of these inputs or variables contributes
to the value that is being optimized.


--Automated Machine Learning Hyperparameter Tuning in Python
A complete walk through using Bayesian optimization for automated hyperparameter tuning in Python

As a brief primer, Bayesian optimization finds the value that minimizes an objective function by 
building a surrogate function (probability model) based on past evaluation results of the objective. 
The surrogate is cheaper to optimize than the objective, 
so the next input values to evaluate are selected by applying a criterion to the surrogate (often Expected Improvement). 
Bayesian methods differ from random or grid search in that they use past evaluation results to choose the next values
to evaluate. The concept is: limit expensive evaluations 
of the objective function by choosing the next input values based on those that have done well in the past.
