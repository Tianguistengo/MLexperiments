Fine-tuning a model:

"1.Grid Search: set up a grid of hyperparameter values and for each combination, 
train a model and score on the validation data. In this approach, every single combination of hyperparameters
values is tried which can be very inefficient!

2. Random search: set up a grid of hyperparameter values and select random combinations
to train the model and score. The number of search iterations is set based on time/resources.

3. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization,
or evolutionary algorithms to conduct a guided search for the best hyperparameters.

-"  We will use cross validation to determine the performance of model hyperparameters 
and early stopping with the GBM so we do not have to tune the number of estimators.

The basic strategy for both grid and random search is simple: for each hyperparameter value combination, 
evaluate the cross validation score and record the results along with the hyperparameters. 

Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score,
train the model on all the training data, and make predictions on the test data.

-To "test" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. 
When we do hyperparameter tuning, it's crucial to         
not tune the hyperparameters on the testing data. 
We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data. 

- The performance of each set of hyperparameters is determined by
Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.

-Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have 
a large enough training set, we can probably get away with just using a single separate validation set,
but cross validation is a safer method to avoid overfitting.

-# Cross validation with early stopping

cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, 
                    metrics = 'auc', nfold = N_FOLDS, seed = 42)
                    
  We can use this result as a baseline model to beat. To find out how well the model does on our "test" data,
  we will retrain it on all the training data with the best number of estimators
  found during cross validation with early stopping.                  


- Gradient Boosting Machine (GBM) 

"The basics you need to know about the GBM are that it is an ensemble method 
that works by training many individual learners, almost always decision trees. However, unlike in a random forest
where the trees are trained in parallel, in a GBM, the trees are trained sequentially 
with each tree learning from 
the mistakes of the previous ones. 
The hundreds or thousands of weak learners are combined to make a single strong 
ensemble learner with the contributions of each individual learned during training using Gradient Descent 
(the weights of the individual trees would therefore be a model parameter).

The GBM has many hyperparameters to tune 
that control both the overall ensemble (such as the learning rate) 
and the individual decision trees 
(such as the number of leaves in the tree or the maximum depth of the tree).


-One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators 
(the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, 
but there's a better method: early stopping. Early stopping means training until the validation error
does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, 
and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation 
error has not decreased for 100 rounds. Then, the number of estimators 
that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.
This is one of many forms of regularization that aims to improve generalization performance on the testing set 
by not overfitting to the training data.



-Histogram-based Gradient Boosting Classification Tree.
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier

This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).

This estimator has --native support for missing values (NaNs--).
During training, the tree grower learns at each split point whether samples with missing values should go to the left 
or right child, based on the potential gain.
When predicting, samples with missing values are assigned to the left or right child consequently. 
If no missing values were encountered for a given feature during training, then samples with missing values
are mapped to whichever child has the most samples.

This implementation is inspired by LightGBM.



--1.16. Probability calibration

https://scikit-learn.org/stable/modules/calibration.html

When performing classification you often want not only to predict the class label,
but also obtain a probability of the respective label.
This probability gives you some kind of confidence on the prediction. 
Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. 
The calibration module allows you 
to better calibrate the probabilities of a given model, or to add support for probability prediction.


- Working with small data:

"In the manufacturing system described above, the absolute number of examples was small. 
But the problem of small data also arises when
the dataset in aggregate is large, but the frequency of specific important classes is low.

Say you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia
in the training set, then the algorithm can obtain high training- and test-set accuracy, 
but still do poorly on cases of hernia

Small data (also called low data) problems are hard because most learning algorithms optimize a cost function 
that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes
and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes 
does not work, as it introduces excessive variance.

We have huge datasets for self-driving, 
but getting good performance on important but rare cases continues to be challenging.


How do we address small data? We are still in the early days of building small data algorithms, but some approaches include:

    Transfer learning, in which we learn from a related task and transfer knowledge over.
    This includes variations on self-supervised learning, in which the related tasks
    can be “made up” from cheap unlabeled data.
    
    One- or few-shot learning, in which we (meta-)learn from many related tasks with small training sets in the hope
    of doing well on the problem of interest.
    
    Relying on hand-coded knowledge, for example through designing more complex ML pipelines. 
    An AI system has two major sources of knowledge: (i) data and (ii) prior knowledge encoded by the engineering team.
    If we have small data, then we may need to encode more prior knowledge.
    
    Data augmentation and data synthesis.
    
    --  how to create a confusion matrix for error analysis 
    
     y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)
    
 conf_mx = confusion_matrix(y_train, y_train_pred)

 conf_mx

-"Many machine learning algorithms are known to produce better models by discretizing continuous attributes."

-"NAIVE BAYES is a probabilistic machine learning algorithm that can be used in a wide variety of classification tasks.
 It is based on the works of Thomas Bayes (1702–61).

But why is it called ‘Naive’?

The name naive is used because it assumes the features that go into the model 
are independent of each other. 
That is changing the value of one feature, does not directly influence or change the value 
of any of the other features 
used in the algorithm.

Since it is a probabilistic model, the algorithm can be coded up easily and
the predictions made real quick. Real-time quick. Because of this, it is easily scalable and is trditionally the 
algorithm of choice for real-world applications (apps) that are required to respond to user’s requests instantaneously.
