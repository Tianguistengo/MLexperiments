Fine-tuning a model:

"1.Grid Search: set up a grid of hyperparameter values and for each combination, 
train a model and score on the validation data. In this approach, every single combination of hyperparameters
values is tried which can be very inefficient!

2. Random search: set up a grid of hyperparameter values and select random combinations
to train the model and score. The number of search iterations is set based on time/resources.

3. Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization,
or evolutionary algorithms to conduct a guided search for the best hyperparameters.

-"  We will use cross validation to determine the performance of model hyperparameters 
and early stopping with the GBM so we do not have to tune the number of estimators.

The basic strategy for both grid and random search is simple: for each hyperparameter value combination, 
evaluate the cross validation score and record the results along with the hyperparameters. 

Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score,
train the model on all the training data, and make predictions on the test data.

-To "test" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. 
When we do hyperparameter tuning, it's crucial to         
not tune the hyperparameters on the testing data. 
We can only use the testing data a single time when we evaluate the final model that has been tuned on the validation data. 

- The performance of each set of hyperparameters is determined by
Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.

-Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have 
a large enough training set, we can probably get away with just using a single separate validation set,
but cross validation is a safer method to avoid overfitting.

-# Cross validation with early stopping

cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, 
                    metrics = 'auc', nfold = N_FOLDS, seed = 42)
                    
  We can use this result as a baseline model to beat. To find out how well the model does on our "test" data,
  we will retrain it on all the training data with the best number of estimators
  found during cross validation with early stopping.                  


- Gradient Boosting Machine (GBM) 

"The basics you need to know about the GBM are that it is an ensemble method 
that works by training many individual learners, almost always decision trees. However, unlike in a random forest
where the trees are trained in parallel, in a GBM, the trees are trained sequentially 
with each tree learning from 
the mistakes of the previous ones. 
The hundreds or thousands of weak learners are combined to make a single strong 
ensemble learner with the contributions of each individual learned during training using Gradient Descent 
(the weights of the individual trees would therefore be a model parameter).

The GBM has many hyperparameters to tune 
that control both the overall ensemble (such as the learning rate) 
and the individual decision trees 
(such as the number of leaves in the tree or the maximum depth of the tree).


-One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators 
(the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, 
but there's a better method: early stopping. Early stopping means training until the validation error
does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, 
and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation 
error has not decreased for 100 rounds. Then, the number of estimators 
that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model.
This is one of many forms of regularization that aims to improve generalization performance on the testing set 
by not overfitting to the training data.



-Histogram-based Gradient Boosting Classification Tree.
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier

This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).

This estimator has --native support for missing values (NaNs--).
During training, the tree grower learns at each split point whether samples with missing values should go to the left 
or right child, based on the potential gain.
When predicting, samples with missing values are assigned to the left or right child consequently. 
If no missing values were encountered for a given feature during training, then samples with missing values
are mapped to whichever child has the most samples.

This implementation is inspired by LightGBM.



--1.16. Probability calibration

https://scikit-learn.org/stable/modules/calibration.html

When performing classification you often want not only to predict the class label,
but also obtain a probability of the respective label.
This probability gives you some kind of confidence on the prediction. 
Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. 
The calibration module allows you 
to better calibrate the probabilities of a given model, or to add support for probability prediction.


- Working with small data:

"In the manufacturing system described above, the absolute number of examples was small. 
But the problem of small data also arises when
the dataset in aggregate is large, but the frequency of specific important classes is low.

Say you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia
in the training set, then the algorithm can obtain high training- and test-set accuracy, 
but still do poorly on cases of hernia

Small data (also called low data) problems are hard because most learning algorithms optimize a cost function 
that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes
and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes 
does not work, as it introduces excessive variance.

We have huge datasets for self-driving, 
but getting good performance on important but rare cases continues to be challenging.


How do we address small data? We are still in the early days of building small data algorithms, but some approaches include:

    Transfer learning, in which we learn from a related task and transfer knowledge over.
    This includes variations on self-supervised learning, in which the related tasks
    can be “made up” from cheap unlabeled data.
    
    One- or few-shot learning, in which we (meta-)learn from many related tasks with small training sets in the hope
    of doing well on the problem of interest.
    
    Relying on hand-coded knowledge, for example through designing more complex ML pipelines. 
    An AI system has two major sources of knowledge: (i) data and (ii) prior knowledge encoded by the engineering team.
    If we have small data, then we may need to encode more prior knowledge.
    
    Data augmentation and data synthesis.
    
    --  how to create a confusion matrix for error analysis 
    
  
  You need to make predictions using the cross_val_predict() function,
  and then call the confusion_matrix() function
    
     y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)
    
 conf_mx = confusion_matrix(y_train, y_train_pred)

 conf_mx
 

Let’s focus the plot on the errors. First, you need to divide each value in the confusion
matrix by the number of images in the corresponding class, so you can compare error
rates instead of absolute number of errors (which would make abundant classes look
unfairly bad):

row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums

-"Many machine learning algorithms are known to produce better models by discretizing continuous attributes."

-"NAIVE BAYES is a probabilistic machine learning algorithm that can be used in a wide variety of classification tasks.
 It is based on the works of Thomas Bayes (1702–61).

But why is it called ‘Naive’?

The name naive is used because it assumes the features that go into the model 
are independent of each other. 
That is changing the value of one feature, does not directly influence or change the value 
of any of the other features 
used in the algorithm.

Since it is a probabilistic model, the algorithm can be coded up easily and
the predictions made real quick. Real-time quick. Because of this, it is easily scalable and is trditionally the 
algorithm of choice for real-world applications (apps) that are required to respond to user’s requests instantaneously.

We need to understand what ‘Conditional Probability’ is and what is the ‘Bayes Rule’.

-"Bayesian Optimization is often used in applied machine learning to tune the hyperparameters
of a given well-performing model on a validation dataset.

If training is really long, you  might prefer a bayesian optimization approach.

Bayesian Optimization provides a probabilistically principled method for global optimization.

Global optimization is a challenging problem that involves black box and often non-convex,
non-linear, noisy, and computationally expensive objective functions.

Global function optimization, or function optimization for short, 
involves finding the minimum or maximum of an objective function.

Objective Function == Function that takes a sample and returns a cost.


Samples are drawn from the domain and evaluated by the objective function to give a score or cost.

-
-"Through hyperparameter optimization, a practitioner identifies free parameters in the model 
that can be tuned to achieve better model performance. There are a few commonly used methods: 
hand-tuning, grid search, random search, evolutionary algorithms and Bayesian optimization.

Grid search cannot efficiently optimize models with more than 4 dimensions, due to the curse of dimensionality.
Although random search is more capable than grid search, its naive approach is still both time-consuming and expensive 
and is more likely to settle in a local optima. Furthermore, it is unscalable beyond 10 parameters. 
Evolutionary algorithms are great if you have access to nearly unlimited compute that can be run in parallel, 
but is often difficult to implement if you do not.  

Bayesian optimization democratizes access to these algorithmic superpowers by relaxing each of these constraints. 

-"Evolutionary algorithms for hyperparameter optmizaiton  use the comparison of “mutation-like” configurations
with the best performing configurations to iterate on the model parameters.

-A.G. "Once you are confident about your final model, measure its performance on the
test set to estimate the generalization error.

Don’t tweak your model after measuring the generalization error:
you would just start overfitting the test set.

- A.G. "Fine-Tune the System

• You will want to use as much data as possible for this step, especially as you move
toward the end of fine-tuning.
• As always automate what you can.

-A.G. "Monitoring

Write monitoring code to check your system’s live performance at regular intervals 
and trigger alerts when it drops.

• Beware of slow degradation too: models tend to “rot” as data evolves.

• Measuring performance may require a human pipeline (e.g., via a crowdsourc‐
ing service).

• Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐
dom values, or another team’s output becoming stale). This is particularly
important for online learning systems.

Retrain your models on a regular basis on fresh data (automate as much as possi‐
ble).

--bias (math)

An intercept or offset from an origin. Bias (also known as the bias term)
is referred to as b or w0 in machine learning models. 

--convergence (Google ML Glossary)

Informally, often refers to a state reached during training in which training loss and validation loss
change very little or not at all with each iteration after a certain number of iterations. 
In other words, a model reaches convergence when additional training on the current data will not improve the model.

--A.G. "training a model means
searching for a combination of model parameters 
that minimizes a cost function (over the training set). 

It is a search in the model's "parameter space"

-- batch

The set of examples used in one iteration (that is, one gradient update) of model training.

This is why the algorithm is
called Batch Gradient Descent: it uses the whole batch of training
data at every step. As a result it is terribly slow on very large train‐
ing sets (but we will see much faster Gradient Descent algorithms
shortly). However, Gradient Descent scales well with the number of
features; training a Linear Regression model when there are hun‐
dreds of thousands of features is much faster using Gradient
Descent than using the Normal Equation.

-- "The risk of using external data sources is that schema or pattern of the data 
may change over time without notification or the team’s awareness. 

For example, someone may vandalize the open data, the method of contents generation may change,
or suppliers may alter the schema of the data without telling downstream users. 
The reasons above can threaten the performance of the ML model and create critical issues if left unnoticed for too long.

In order to monitor the ETL process effectively and prevent critical data issue to affect 
the model and its performance; 
integrity, consistency, and availability of the data are three aspects
to consider when selecting metrics for ETL monitoring.

The integrity of the data can be monitored by creating comprehensive validation rules
to catch and report the exceptions. Validation rules are usually developed during the 
exploratory data analysis and training of the model. Some metrics that can be used are, for instance,
the number of missing or unseen features or ratio of data points that fail each validation rule.

The second principle for ETL monitoring metrics is the consistency of the data.
Consistency is important because a data point may pass the validation rules but the values in the features may
be vastly different from its history in which the model was trained from. The causes that impact the integrity 
of the data also affect the consistency of the data as well.
But in the case of user-generated data, 
the change in consistency can also happen organically
due to a shift in how users interact on the platform.

The availability of the data may not affect the precision of the ML model but it does affect the performance of the ML pipeline as a whole since the business objectives can’t be achieved without the data coming in (or coming late). Examples of KPIs that can be used to monitor the availability of the data are:

    Amount of delay time between each batch of data
    Total minutes, hours, or days, without data in the pipeline
    Number of times that data is delivered behind schedule
    

-"The most basic aspects of model performance  / KD Nuggets

is whether it is getting requests and how quickly it is producing results. 
No model will give good results if you don’t ask for them. 
And results are no good unless you actually get them in good time.

To monitor whether requests are arriving, you need to record the arrival each incoming request together with 
the name of the machine where the request has landed and an accurate timestamp. 
It is typically a bit better to record this data to a persistent stream on a distributed platform because 
log files can disappear if a machine goes down. It is also better to record
request arrival and completion as separate events 
so that you can distinguish failure to respond from lack of requests.

As a first cut, we can compute the distribution of the reported elapsed times. 
For latencies, the best way to do this is by using a non-uniform histogram.
We can compute such a histogram for each, say, five minute interval. To monitor performance, we can accumulate
a background over a fairly long period of time and plot the recent results against that background distribution.

These histograms can also be compared using more advanced, automated techniques based on comparing the counts 
in corresponding bins. One good way to do this is to use a statistical method known as the G-test. 

Are the results any good? Are they right? We have no real idea.
All we know so far is that requests are arriving and results are being produced at historically plausible levels and speeds. 

As nice as real-time ground truth feedback may be, there are lots of use cases where this just isn’t possible. 
For instance, we may not know even slightly whether a detected fraud is really just that for days or weeks.
There are still things that we can do to detect problems quickly, however.
Lots of models produce some sort of score or set of scores. Often these represent some kind of probability estimate. 
The method, then is to store digests of the score distribution every minute or so tagged with model version and such. 
There are two popular ways to compare distributions.

Nothing sings like a canary
In general, the more we know about what a model should be doing and the more specific we can be about comparing
against reference behavior, the quicker we can reliably detect changes. This is the motivation behind using a canary model.
The idea is that we send every request to the current production model as usually, 
but we also keep an older version of the model around and send every (or nearly every) request
to that older version as well. The older model is called a canary.
Because we are sending the exact same requests to both models and because the canary is a model that does nearly the same 
thing that we want the current model to do, we can compare the output of the two models request by request
to get a very specific idea about whether the new model is behaving as expected.

A canary model is also very handy when we are fielding a potential challenger to our current champion. 
If we are trying to quantify the risk of rolling out this new challenger to replace our current champion.
(you can analyze where the new model failed and the specifics of what is different)

This article has walked through a range of near real-time monitoring techniques for machine learning models
starting with monitors that look purely at the gross operational characteristics 
of request rate and response times.
Those are often very good for detecting systems level problems in evaluating requests.

For cases where we don’t know exactly what our model should output, 
we talked about methods for looking for changes in score distributions by comparing 
to the past performance of the model or by comparing to a canary model.


-"Tracking Experiments 
– Storing the model code, hyperparameters, and result metrics of every experiment is important 
to be able to discuss results, decide in which direction to go next or reproduce experiments. The simplest format 
for this could be a shared spreadsheet, but more sophisticated options are available
– though they often come with additional
requirements for the rest of the pipeline. 
As an example, here is a screenshot from MLflow`s experiment tracking server UI

-"Scalability 
– If you invest into building an ML pipeline, you may want to eliminate the need to rebuild it 
just because it can’t handle increasing volumes of data or the demands of a growing team of data scientists. 
Make sure that the pipeline is scalable by choosing building blocks that are scalable themselves.

-Automation – Even when a lot of steps are performed manually in the beginning, 
make sure that all steps can be automated later on via APIs or certain tools. 
Repeated manual tasks are boring and error-prone.

-Model and Code Versioning – Versioning the model code is essential for reproducing training runs. 
The pipeline should also be capable of managing versioned artifacts for at least every model 
that is supposed to be deployed in production. This is necessary for rollbacks and A/B tests in production.

---Sacred 
is a tool to help you configure, organize, log and reproduce experiments. 
It is designed to do all the tedious overhead work that you need to do around your actual experiment in order to:

    keep track of all the parameters of your experiment
    easily run your experiment for different settings
    save configurations for individual runs in a database
    reproduce your results
    
-Omniboard is a web dashboard for the Sacred machine learning experiment management tool.

It connects to the MongoDB database used by Sacred and helps in visualizing the experiments and metrics / logs 
collected in each experiment.     https://vivekratnavel.github.io/omniboard/#/

-One particular technical challenge often faced and highlighted is how to ensure that the same data transformations 
that are applied during training are also applied on prediction input. 
This includes constants that were computed during training (e. g. for normalization). Consider this early on in your
planning phase. If you are using TensorFlow, for example, take a look at TensorFlow Transform which addresses this challenge.

-What is Jenkins?

Jenkins is an open source automation tool written in Java with plugins built for Continuous Integration purpose.
Jenkins is used to build and test your software projects continuously making it easier for developers to integrate changes
to the project, and making it easier for users to obtain a fresh build. 
It also allows you to continuously deliver your software by integrating with a large number of testing and deployment 
technologies.

With Jenkins, organizations can accelerate the software development process through automation.
Jenkins integrates development life-cycle processes of all kinds, including build, document,
test, package, stage, deploy, static analysis and much more.

Jenkins achieves Continuous Integration with the help of plugins. 
Plugins allows the integration of Various DevOps stages.

-" What is an Objective Function?

Definition: The objective function is a mathematical term that describes --how different variables contribute--
to a certain value that is being sought to be optimized.

The objective function shows us how each of these inputs or variables contributes
to the value that is being optimized.


--Automated Machine Learning Hyperparameter Tuning in Python

A complete walk through using Bayesian optimization for automated hyperparameter tuning in Python

As a brief primer, Bayesian optimization finds the value that minimizes an objective function by 
building a surrogate function (probability model) based on past evaluation results of the objective. 
The surrogate is cheaper to optimize than the objective, 
so the next input values to evaluate are selected by applying a criterion to the surrogate (often Expected Improvement). 
Bayesian methods differ from random or grid search in that they use past evaluation results to choose the next values
to evaluate. The concept is: limit expensive evaluations 
of the objective function by choosing the next input values based on those that have done well in the past.

--In a pub/sub model, any message published to a topic is immediately received by all of the subscribers to the topic.
Pub/sub messaging can be used to enable 
event-driven architectures, or to decouple applications in order to increase performance, reliability and scalability.

--More than one models can be deployed at any time to enable safe transition between old and new models — i.e. when
deploying a new model, the services need to keep serving prediction requests.

-- model scoring and model serving es hacer inferencias / predicciones

--Depending on the use-case, scores can also be delivered to the client asynchronously i.e. independently of the request:
• Push: Once the scores are generated, they are pushed to the caller as a notification.
• Poll: Once the scores are generated, they are stored in a low read-latency database;
the caller periodically polls the database for available predictions.

In order to minimise the time the system takes to serve the scoring when it receives the request, two methods are employed:
• the input features are stored in a low-read latency in-memory data store,
• predictions precomputed in an offline batch-scoring job are cached for easy access 
[this is depending on the use-case, as offline predictions might not be relevant].

-- "Any ML solution requires a well-defined performance monitoring solution. 
An example of information that we might want to see for model serving applications includes:
• model identifier,
• deployment date/time,
• number of times the model has been served,
• average/min/max model serving times,
• distribution of features used.
• predicted vs. actual/observed results.

This metadata is calculated during the model scoring and then used for monitoring.

The basic implementation for monitoring can follow different approaches with the most popular being
logging analytics (Kibana, Grafana, Splunk etc).

Any sign of degradation can be dealt with by reverting to the previous model.
A chain of responsibility pattern can be used to chain the different versions together.


-"Cross cutting concerns. 
A ML application, 
like any other application, has some common functionality that spans across layers/pipelines.

The cross cutting concerns are normally centralised in one place, which increases the application’s modularity. 
They are often managed by other teams in the organisation or are off-the-shelf / thrid-party products. 
Dependency injection is the best way to inject these in the relevant places in the code.

The most important concerns to be addressed, in our use-case are:

• Notifications
• Scheduling
• Logging Framework (and Alert mechanism)
• Exception Management
• Configuration Service
• Data Service (to expose querying in a data store)
• Auditing
• Data Lineage
• Caching
• Instrumentation


--Data lineage is generally defined as a kind of data life cycle that includes the data's origins
and where it moves over time.
This term can also describe what happens to data as it goes through diverse processes. Data lineage can help
with efforts to analyze how information is used and to track key bits of information that serve a particular purpose.



--"Python tricks:

-Lambda- is a method to compose a function in one line for one-time use. 
Performance suffers if the functions are called multiple times.
On the other hand, -map- applies a function to all elements in a list, 
whereas -filter- gets a subset of elements in a set that meets a user-defined condition.


--A model has high VARIANCE if it is very sensitive to (small) changes in the training data.

-'Remember that since 'samples' is a NumPy array, 
you can't use the .loc[] or iloc[] accessors to select specific rows or columns.

-Cosine similarity
higher values means more similarity

Cosine similarity measures the similarity between two vectors of an inner product space.
It is measured by the cosine of the angle between two vectors

# Compute cosine similarities: similarities
similarities = df.dot(artist)      dot es cosine similarities

-The first step in the pipeline, MaxAbsScaler, transforms the data so that all
features have the same influence on the model, regardless of how many different artists they've listened to.

-The GENERALIZATION ERROR tells you how well the model generalizes to unseen data.
(our goal is to try to minimize it)

-2 types cross-validation
K-Fold CV
Hold-Out CV

CV is a great technique to get an estimate of a model's performance without affecting the test set.

CV can't comput normal MSE so you have to use its negative scoring

cross_val_score(dt, X_train, y_train, cv=10, 
                       scoring='neg_mean_squared_error',



-     n_jobs= -1  (minus 1/ to exploit all available CPUs for computation)


-If the train and test error are similar probably the model is underfitting.
If the test error is higher than the train error, the model is probably overfitting.


-" What is STRATIFY in Train_test_split?
In this context, stratification means that the train_test_split method returns 
training and test subsets that have the SAME PROPORTIONS OF CLASS LABELS as the input dataset.

-"  Given that this dataset is IMBALANCED, 
you'll be using the ROC AUC score as a metric instead of accuracy.


- Parameters= learn from data through training

Hyperparameters= defined by the engineer prior to training
for tuning these (GridSearch, Random Search, Bayesian optimization, Genetic algorithms, etc..)

-Hyperparameter tuning

for a decision tree     print(dt.get_params())  
to get a dictionarty with the hyperparameters names

To extract the best hyperparameters after using GridSearchCV or any other CV model
use the .best_params attribute 
(trained_model.best_params_)

The best cross-validationa accuracy can be found usingthe attribute  .best_score_  

And the best model can be extracted using the attibute .best_estimator_

(knowing the best model allows you to predict the test set labels
and evaluate the test set RMSE)

To finish, best_model.score(X_test,y_test)

To fit the model to the whole training set, refit=True in the GridSeachCV parameters (that is the default)

Hyperparameter tuning is computationally expensive (and sometimes it only gets you slim improvements)

For ensembles, you can tune the hyperparameters of the models AND of the ensemble

- Metrics of perfomance: 

Accuracy for classifiers

R-squared for regressors

A model generalization performance is evaluated using cross-validation

-Verbose = print information about the process during training

-Mean square error-- you square each error and take the average of that as a measure of model quality.



-LogLoss :
Logarithmic loss (related to cross-entropy)
measures the performance of a classification model where the prediction input is a probability value between 0 and 1. 

Scipy: 
a Python-based ecosystem of open-source software for mathematics, science, and engineering.
In particular, these are some of the core packages:

    NumPy
    Base N-dimensional array package
    
    pandas
    Data structures & analysis
    
    SciPy library
    Fundamental library for scientific computing
    
    Matplotlib
    Comprehensive 2-D plotting
    
    IPython
    Enhanced interactive console
    
    SymPy
    Symbolic mathematics
    
   


-----------> Kaggle competitions

"Before making any progress in the competition, you should get familiar with the expected output.

That is why, let's look at the columns of the test dataset and compare it to the train columns. 
Additionally, let's explore the format of the sample submission.

-Determine your problem: classification, regression, ranking etc..

 train.hist(['sales'])  --> sales being the target variable
 plt.show()
 then you an see the histogram showing that is a continuous variable == regression

-To select multiple columns, we need to use double BRACKETS [[]]

-Your goal is to read the test data, make predictions, and save these 
in the format specified in the "sample_submission.csv" file. 

-
# Read test and sample submission data
test = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')

# Show the head() of the sample_submission
print(sample_submission.head())

# Get predictions for the test set (and add them to the new colum 'sales' in the test data set)
test['sales'] = rf.predict(test[['store', 'item']])

# Write test predictions using the sample_submission format
test[['id', 'sales']].to_csv('kaggle_submission.csv', index=False)


-->Competition metrics

Clasification    ( Area Under the ROC curve (AUC),   F1 Score (F1),   LogLoss )

Regression  ( Mean Absolute Error (MAE), Mean Squared Error (MSE), 

Ranking  ( Mean Average Precision at K (MAPK, MAP@K)



-To measure the quality of the models you will use Mean Squared Error (MSE).
It's available in sklearn.metrics as mean_squared_error() 
and it takes two arguments: true values and predicted values.

- overfit is when the test error is higher thant the training error

--
1. Type of data-- are we dealing with: tabular data, time series, images, text?

data = pd.read_csv('cool_beans.csv')
print ('Cool beans shape:', data.shape)
print ( data.columns.tolist() )
data.target_variable.value_counts ()   if your target variable is divided into categories, you'd see them here
                                     if there is a class imbalance you'd see it here (there are class balancing methods)

data.describe ()   (count, mean, std, min, max, etc..)

import matplotlib.pyplot as plt


2. Problem type--  Classification, Regression, Ranking, etc..
3. Evaluation metric -- MAE,MSE, F1 score, LogLoss, ROC AUC etc..   most of them in sklearn.metrics
4. Exploratory Data Analysis


---- Exploratory Data Analysis

-Size of the dataset

-Properties of the target variable
(there might be a class imbalance in the classification problem 
or a skewed distribution in the regression problem)

-Properties of the features
(finding a pattern between features and target variables is intersting here)

-Generate ideas for feature engineering

# Calculate the ride distance
train['distance_km'] = haversine_distance(train)    hav dist is a customized function to calculate distance between
                                                         'pick_up' location and 'drop_off'
# Draw a scatterplot
plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)
plt.xlabel('Fare amount')
plt.ylabel('Distance, km')
plt.title('Fare amount based on the distance')

# Limit on the distance
plt.ylim(0, 50)    para no incluir outliers
plt.show()



------K-Fold and Stratified K-Fold validation

"As you've just noticed, you have a pretty different target variable distribution among the folds due to the random splits.
It's not crucial for this particular competition, but could be an issue for the classification competitions 
with the highly imbalanced target variable.

To overcome this, let's implement the stratified K-fold strategy with the stratification on the target variable.
Now you see that --both size and target distribution-- are the same among the folds.
The general rule is to prefer Stratified K-Fold over usual K-Fold in any classification problem. 
Next, more on other cross-validation strategies.

- DATA LEAKAGE
Leakage is when a model seems accurate until you test with real-world data/in a real-world setting

*Leak in --features--: using data that will not be available in the real setting.
*Leak in --validation strategy--: validation strategy differ from the real-world situation
(like a validarion for time series data where you are relying on future data you don't have yet).


- Model Validation 

from sklearn.model_selection import TimeSeriesSplit
import numpy as np

# Sort train data by date
train = train.sort_values('date')

# Initialize 3-fold time cross-validation
kf = TimeSeriesSplit(n_splits=3)

# Get MSE scores for each cross-validation split   ,  For simplicity, you're given get_fold_mse() function 
that for each cross-validation split fits a Random Forest model and returns a list of MSE scores by fold. 
mse_scores = get_fold_mse(train, kf)

print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))
print('MSE by fold: {}'.format(mse_scores))
print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores))) 
------>  este ^  standard deviation te da una mejor idea de el overall MSE






















